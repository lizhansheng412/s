#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step One - æ„å»ºå®Œæ•´çš„ corpusid è¡¨
åŠŸèƒ½ï¼š
1. è§£å‹ paper-ids æ•°æ®é›†çš„æ‰€æœ‰ gz æ–‡ä»¶
2. æå–æ¯è¡Œçš„ corpusid å­—æ®µ
3. ä½¿ç”¨ COPY æ‰¹é‡æ’å…¥åˆ° PostgreSQL æ•°æ®åº“
4. å®Œæˆåå¯¹ corpusid æ’åºå¹¶å»ºç«‹ä¸»é”®ç´¢å¼•
"""

import sys
import gzip
import time
import tempfile
from pathlib import Path
from io import StringIO
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import orjson
import psycopg2
from psycopg2 import sql
from tqdm import tqdm

from step_scripts.step_one.machine_db_config import get_db_config
from step_scripts.step_one.init_process_table import ProcessRecorder, DatasetType

# =============================================================================
# é…ç½®
# =============================================================================

TABLE_NAME = 'full_corpusid'
DATA_FOLDER = Path(r'D:\2025-09-30\paper-ids')
BATCH_SIZE = 500000  # æ¯æ‰¹æ¬¡å¤„ç†çš„è¡Œæ•°ï¼ˆé’ˆå¯¹2äº¿+æ•°æ®ä¼˜åŒ–ï¼‰

# =============================================================================
# æ•°æ®åº“æ“ä½œ
# =============================================================================

def create_table_if_not_exists(cursor):
    """åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰- æ— ç´¢å¼•æ¨¡å¼ç”¨äºå¿«é€Ÿæ’å…¥"""
    cursor.execute(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name = '{TABLE_NAME}'
        );
    """)
    
    if cursor.fetchone()[0]:
        print(f"âš ï¸  è¡¨ {TABLE_NAME} å·²å­˜åœ¨")
        response = input("æ˜¯å¦åˆ é™¤å¹¶é‡å»ºï¼Ÿ(yes/no): ").strip().lower()
        if response != 'yes':
            return False
        cursor.execute(f"DROP TABLE {TABLE_NAME} CASCADE;")
    
    cursor.execute(f"""
        CREATE TABLE {TABLE_NAME} (
            corpusid BIGINT NOT NULL
        ) WITH (
            fillfactor = 100,
            autovacuum_enabled = false
        );
    """)
    return True

def process_gz_file(gz_path, cursor, conn):
    """
    å¤„ç†å•ä¸ª gz æ–‡ä»¶ï¼Œæå– corpusid å¹¶æ‰¹é‡æ’å…¥
    
    Args:
        gz_path: gz æ–‡ä»¶è·¯å¾„
        cursor: æ•°æ®åº“æ¸¸æ ‡
        conn: æ•°æ®åº“è¿æ¥
    
    Returns:
        æ’å…¥çš„è®°å½•æ•°
    """
    total_inserted = 0
    batch_buffer = []
    
    try:
        with gzip.open(gz_path, 'rb') as f:
            for line in f:
                if not line.strip():
                    continue
                
                try:
                    data = orjson.loads(line)
                    corpusid = data.get('corpusid')
                    
                    if corpusid is not None:
                        batch_buffer.append(str(corpusid))
                        
                        # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶æ‰§è¡Œæ’å…¥
                        if len(batch_buffer) >= BATCH_SIZE:
                            insert_batch(cursor, batch_buffer)
                            total_inserted += len(batch_buffer)
                            batch_buffer = []
                            conn.commit()
                
                except Exception as e:
                    print(f"âš ï¸  è§£æè¡Œå¤±è´¥: {e}")
                    continue
        
        # æ’å…¥å‰©ä½™æ•°æ®
        if batch_buffer:
            insert_batch(cursor, batch_buffer)
            total_inserted += len(batch_buffer)
            conn.commit()
    
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {gz_path.name}: {e}")
        conn.rollback()
        raise
    
    return total_inserted

def insert_batch(cursor, corpusid_list):
    """ä½¿ç”¨ COPY æ‰¹é‡æ’å…¥æ•°æ®"""
    buffer = StringIO()
    for corpusid in corpusid_list:
        buffer.write(f"{corpusid}\n")
    buffer.seek(0)
    
    cursor.copy_from(buffer, TABLE_NAME, columns=('corpusid',))

def build_index_and_sort(cursor, conn):
    """æ’åºå¹¶å»ºç«‹ä¸»é”®ç´¢å¼•"""
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®ä¼˜åŒ–ä¸­...")
    
    # ç»Ÿè®¡åŸå§‹è®°å½•æ•°
    cursor.execute(f"SELECT COUNT(*) FROM {TABLE_NAME};")
    total_count = cursor.fetchone()[0]
    print(f"åŸå§‹è®°å½•æ•°: {total_count:,}")
    
    # åˆ›å»ºå»é‡æ’åºè¡¨å¹¶å»ºç«‹ä¸»é”®
    print("å»é‡ã€æ’åºã€å»ºç«‹ä¸»é”®...")
    start_time = time.time()
    temp_table = f"{TABLE_NAME}_new"
    
    cursor.execute(f"""
        CREATE TABLE {temp_table} (
            corpusid BIGINT PRIMARY KEY
        ) WITH (fillfactor = 100);
    """)
    
    cursor.execute(f"""
        INSERT INTO {temp_table} (corpusid)
        SELECT DISTINCT corpusid 
        FROM {TABLE_NAME}
        ON CONFLICT (corpusid) DO NOTHING;
    """)
    
    cursor.execute(f"DROP TABLE {TABLE_NAME};")
    cursor.execute(f"ALTER TABLE {temp_table} RENAME TO {TABLE_NAME};")
    
    cursor.execute(f"""
        ALTER TABLE {TABLE_NAME}
        SET (autovacuum_enabled = true);
    """)
    
    cursor.execute(f"ANALYZE {TABLE_NAME};")
    conn.commit()
    
    cursor.execute(f"SELECT COUNT(*) FROM {TABLE_NAME};")
    final_count = cursor.fetchone()[0]
    elapsed = time.time() - start_time
    
    print(f"æœ€ç»ˆè®°å½•æ•°: {final_count:,} (å»é‡: {total_count - final_count:,})")
    print(f"ä¼˜åŒ–è€—æ—¶: {elapsed:.1f}ç§’")
    print("="*70)

# =============================================================================
# ä¸»æµç¨‹
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Step One - æ„å»ºå®Œæ•´çš„ corpusid è¡¨")
    print(f"æ•°æ®ç›®å½•: {DATA_FOLDER}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE:,}")
    print("="*70)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not DATA_FOLDER.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_FOLDER}")
        return
    
    # åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ è®°å½•å™¨
    recorder = ProcessRecorder(machine='machine2')
    
    # è·å–æ‰€æœ‰ gz æ–‡ä»¶
    gz_files = sorted(DATA_FOLDER.glob("*.gz"))
    if not gz_files:
        print(f"âŒ æœªæ‰¾åˆ° gz æ–‡ä»¶")
        recorder.close()
        return
    
    # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
    pending_files = []
    skipped_count = 0
    for gz_file in gz_files:
        if recorder.is_processed(gz_file.name, DatasetType.PAPERS):  # paper-ids ä½¿ç”¨ PAPERS ç±»å‹
            skipped_count += 1
        else:
            pending_files.append(gz_file)
    
    print(f"æ‰¾åˆ° {len(gz_files)} ä¸ª gz æ–‡ä»¶")
    print(f"å·²å¤„ç†: {skipped_count} ä¸ª | å¾…å¤„ç†: {len(pending_files)} ä¸ª\n")
    
    if not pending_files:
        print("âœ“ æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
        recorder.close()
        return
    
    # è¿æ¥æ•°æ®åº“
    try:
        config = get_db_config('machine2')
        print(f"è¿æ¥æ•°æ®åº“: {config['database']}@{config['host']}:{config['port']}")
        conn = psycopg2.connect(**config)
        cursor = conn.cursor()
        
        # åˆ›å»ºè¡¨
        if not create_table_if_not_exists(cursor):
            cursor.close()
            conn.close()
            recorder.close()
            return
        conn.commit()
        
        # å¤„ç†å¾…å¤„ç†çš„ gz æ–‡ä»¶
        print("\n" + "="*70)
        print("å¼€å§‹å¤„ç† gz æ–‡ä»¶")
        print("="*70)
        
        total_records = 0
        start_time = time.time()
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦å’Œé¢„ä¼°æ—¶é—´
        with tqdm(total=len(pending_files), desc="å¤„ç†è¿›åº¦", unit="file", 
                  bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
            for idx, gz_file in enumerate(pending_files, 1):
                file_start = time.time()
                records = process_gz_file(gz_file, cursor, conn)
                file_elapsed = time.time() - file_start
                total_records += records
                
                # è®°å½•æ–‡ä»¶å·²å¤„ç†ï¼ˆåªæœ‰å½“å‰æ–‡ä»¶å®Œå…¨å¤„ç†å®Œæ‰è®°å½•ï¼‰
                recorder.add_record(gz_file.name, DatasetType.PAPERS)
                
                # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
                elapsed = time.time() - start_time
                avg_time_per_file = elapsed / idx
                remaining_files = len(pending_files) - idx
                eta_seconds = avg_time_per_file * remaining_files
                eta_str = time.strftime('%H:%M:%S', time.gmtime(eta_seconds))
                
                pbar.set_postfix({
                    'å½“å‰': f'{records:,}æ¡',
                    'æ€»è®¡': f'{total_records:,}æ¡',
                    'é€Ÿåº¦': f'{records/file_elapsed:.0f}æ¡/ç§’',
                    'é¢„è®¡å‰©ä½™': eta_str
                })
                pbar.update(1)
        
        elapsed = time.time() - start_time
        print(f"\næ€»è®°å½•æ•°: {total_records:,}")
        print(f"æ€»è€—æ—¶: {elapsed:.1f}ç§’ | å¹³å‡é€Ÿåº¦: {total_records/elapsed:.0f}æ¡/ç§’")
        
        # æ’åºå¹¶å»ºç«‹ç´¢å¼•
        build_index_and_sort(cursor, conn)
        
        # å…³é—­è¿æ¥
        cursor.close()
        conn.close()
        recorder.close()
        print("\nâœ… æ‰€æœ‰æ“ä½œå®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        if 'conn' in locals():
            conn.rollback()
            conn.close()
        if 'recorder' in locals():
            recorder.close()
        return

if __name__ == '__main__':
    main()