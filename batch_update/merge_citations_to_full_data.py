"""
å°† xxx_part2.jsonl å’Œæ•°æ®åº“ä¸­çš„æ•°æ®åˆå¹¶æ›´æ–°åˆ° xxx.jsonl
æ”¯æŒ: åŒæºåˆå¹¶ + æ–­ç‚¹ç»­ä¼  + è·¨æœºå™¨
"""

import os
import sys
import tempfile
import shutil
import time
import sqlite3
import psycopg2
from psycopg2 import OperationalError, InterfaceError
import argparse
import re
from pathlib import Path
from typing import Dict, Any, Set, Optional
from datetime import datetime
from tqdm import tqdm
import orjson

# orjson.dumps è¿”å› bytesï¼Œéœ€è¦è§£ç 
def json_dumps(obj):
    return orjson.dumps(obj).decode('utf-8')

json_loads = orjson.loads

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from db_config import get_db_config


# éœ€è¦æ›´æ–°çš„å­—æ®µ
CITATION_FIELDS = ["citations", "references", "detailsOfCitations", "detailsOfReference"]
# DB_FIELDS = ["specter_v1", "specter_v2", "content"]
DB_FIELDS = ["content"]
ALL_UPDATE_FIELDS = CITATION_FIELDS + DB_FIELDS

IGNORE_IS_DONE_FILTER = False
IS_DONE_FILTER_VALUE = False  # PostgreSQL boolean ç±»å‹

# é‡è¯•é…ç½®
MAX_RETRIES = 5              # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 2              # é‡è¯•é—´éš”(ç§’)
CONNECTION_TIMEOUT = 30      # è¿æ¥è¶…æ—¶(ç§’)


def log(log_file: Path, msg: str):
    """æ—¥å¿—åªå†™å…¥æ–‡ä»¶"""
    with open(log_file, 'a', encoding='utf-8') as f:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"[{timestamp}] {msg}\n")


def clean_json_line(line: str) -> str:
    """æ¸…ç† JSON å­—ç¬¦ä¸²ä¸­çš„éæ³•æ§åˆ¶å­—ç¬¦"""
    # ä½¿ç”¨å­—å…¸æ˜ å°„æé«˜æ•ˆç‡
    control_chars = {'\t': '\\t', '\n': '\\n', '\r': '\\r'}
    
    def replace_char(match):
        char = match.group(0)
        return control_chars.get(char, '')  # å…¶ä»–æ§åˆ¶å­—ç¬¦ç§»é™¤
    
    return re.sub(r'[\x00-\x1f]', replace_char, line)


def init_progress_db(progress_db: Path):
    """åˆå§‹åŒ–è¿›åº¦æ•°æ®åº“"""
    progress_db.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(progress_db)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS progress (
            filename TEXT PRIMARY KEY,
            is_done BOOLEAN NOT NULL DEFAULT 0,
            updated_at TEXT NOT NULL
        )
    ''')
    conn.commit()
    conn.close()


def get_completed_files(progress_db: Path) -> Set[str]:
    """è·å–å·²å®Œæˆçš„æ–‡ä»¶åˆ—è¡¨"""
    conn = sqlite3.connect(progress_db)
    cursor = conn.cursor()
    cursor.execute('SELECT filename FROM progress WHERE is_done = 1')
    completed = {row[0] for row in cursor.fetchall()}
    conn.close()
    return completed


def mark_file_done(progress_db: Path, filename: str):
    """æ ‡è®°æ–‡ä»¶ä¸ºå·²å®Œæˆ"""
    conn = sqlite3.connect(progress_db)
    cursor = conn.cursor()
    cursor.execute('''
        INSERT OR REPLACE INTO progress (filename, is_done, updated_at)
        VALUES (?, 1, ?)
    ''', (filename, datetime.now().isoformat()))
    conn.commit()
    conn.close()


def connect_pg_db(db_config: Dict[str, str], log_file: Optional[Path] = None):
    """è¿æ¥PostgreSQLæ•°æ®åº“(å¸¦é‡è¯•)"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            conn = psycopg2.connect(
                host=db_config['host'],
                port=db_config['port'],
                database=db_config['database'],
                user=db_config['user'],
                password=db_config['password'],
                connect_timeout=CONNECTION_TIMEOUT
            )
            return conn
        except (OperationalError, InterfaceError) as e:
            if attempt < MAX_RETRIES:
                if log_file:
                    log(log_file, f"æ•°æ®åº“è¿æ¥å¤±è´¥ (å°è¯• {attempt}/{MAX_RETRIES}): {e}, {RETRY_DELAY}ç§’åé‡è¯•...")
                time.sleep(RETRY_DELAY)
            else:
                raise e


def is_citation_fields_empty(record: Dict[str, Any]) -> bool:
    """æ£€æŸ¥å¼•ç”¨å­—æ®µæ˜¯å¦å…¨ä¸ºç©º"""
    for field in CITATION_FIELDS:
        value = record.get(field)
        if isinstance(value, list) and len(value) > 0:
            return False
        elif isinstance(value, dict):
            data = value.get("data", [])
            if isinstance(data, list) and len(data) > 0:
                return False
    return True


def is_db_fields_empty(record: Dict[str, Any]) -> bool:
    """æ£€æŸ¥æ•°æ®åº“å­—æ®µæ˜¯å¦å…¨ä¸ºç©º"""
    for field in DB_FIELDS:
        value = record.get(field)
        if value and (isinstance(value, str) and value.strip() or not isinstance(value, str)):
            return False
    return True


def update_record_fields(target_record: Dict[str, Any], source_record: Dict[str, Any], 
                        fields: list, skip_if_target_not_empty: bool = False) -> int:
    """
    æ›´æ–°è®°å½•å­—æ®µ
    
    Args:
        target_record: ç›®æ ‡è®°å½•
        source_record: æºè®°å½•
        fields: è¦æ›´æ–°çš„å­—æ®µåˆ—è¡¨
        skip_if_target_not_empty: å¦‚æœç›®æ ‡å­—æ®µä¸ä¸ºç©ºåˆ™è·³è¿‡æ›´æ–°
    
    Returns:
        æ›´æ–°çš„å­—æ®µæ•°
    """
    updated = 0
    for field in fields:
        source_value = source_record.get(field)
        
        # æ£€æŸ¥æºå€¼æ˜¯å¦ä¸ºç©º
        is_empty = False
        if source_value is None:
            is_empty = True
        elif isinstance(source_value, list):
            is_empty = len(source_value) == 0
        elif isinstance(source_value, dict):
            data = source_value.get("data", [])
            is_empty = not (isinstance(data, list) and len(data) > 0)
        elif isinstance(source_value, str):
            is_empty = not source_value.strip()
        
        if is_empty:
            continue
        
        # å¦‚æœéœ€è¦æ£€æŸ¥ç›®æ ‡å­—æ®µæ˜¯å¦ä¸ºç©º
        if skip_if_target_not_empty:
            target_value = target_record.get(field)
            target_not_empty = False
            
            if target_value is not None:
                if isinstance(target_value, list):
                    target_not_empty = len(target_value) > 0
                elif isinstance(target_value, dict):
                    data = target_value.get("data", [])
                    target_not_empty = isinstance(data, list) and len(data) > 0
                elif isinstance(target_value, str):
                    target_not_empty = bool(target_value.strip())
            
            if target_not_empty:
                continue
        
        # æ›´æ–°å­—æ®µ
        target_record[field] = source_value
        updated += 1
    
    return updated


def load_db_data(db_conn, corpusid_list: list, db_config: Dict[str, str], log_file: Path) -> Dict[int, Dict[str, Any]]:
    """
    ä»æ•°æ®åº“æ‰¹é‡åŠ è½½æ•°æ®(å¸¦é‡è¯•å’Œè¿æ¥æ¢å¤)
    
    Returns:
        {corpusid: {content}}
    """
    if not corpusid_list:
        return {}
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
            try:
                db_conn.isolation_level
            except (OperationalError, InterfaceError):
                log(log_file, f"æ£€æµ‹åˆ°æ•°æ®åº“è¿æ¥æ–­å¼€,æ­£åœ¨é‡æ–°è¿æ¥...")
                db_conn = connect_pg_db(db_config, log_file)
            
            cursor = db_conn.cursor()
            placeholders = ','.join(['%s'] * len(corpusid_list))
            
            if IGNORE_IS_DONE_FILTER:
                query = f"""
                    SELECT corpusid, content
                    FROM temp_import
                    WHERE corpusid IN ({placeholders})
                """
                params = corpusid_list
            else:
                query = f"""
                    SELECT corpusid, content
                    FROM temp_import
                    WHERE is_done = %s AND corpusid IN ({placeholders})
                """
                params = [IS_DONE_FILTER_VALUE, *corpusid_list]
            
            cursor.execute(query, params)
            
            db_data = {}
            for row in cursor.fetchall():
                corpusid, content = row
                
                # è·³è¿‡ content ä¸ºç©ºçš„è®°å½•
                if not content:
                    continue
                
                db_data[corpusid] = {
                    "content": content
                }
            
            cursor.close()
            return db_data
            
        except (OperationalError, InterfaceError) as e:
            # å›æ»šå½“å‰äº‹åŠ¡
            try:
                db_conn.rollback()
            except:
                pass
            
            if attempt < MAX_RETRIES:
                log(log_file, f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ (å°è¯• {attempt}/{MAX_RETRIES}): {e}, {RETRY_DELAY}ç§’åé‡è¯•...")
                time.sleep(RETRY_DELAY)
                try:
                    db_conn.close()
                except:
                    pass
                db_conn = connect_pg_db(db_config, log_file)
            else:
                log(log_file, f"æ•°æ®åº“æŸ¥è¯¢æœ€ç»ˆå¤±è´¥: {e}")
                raise e
        except Exception as e:
            # å…¶ä»–é”™è¯¯ä¹Ÿéœ€è¦å›æ»š
            try:
                db_conn.rollback()
            except:
                pass
            log(log_file, f"æ•°æ®åº“æŸ¥è¯¢é”™è¯¯: {e}")
            raise e


def process_file_pair(source_file: Path, target_file: Path, 
                     db_conn, db_config: Dict[str, str], log_file: Path) -> Dict[str, Any]:
    """å¤„ç†æ–‡ä»¶å¯¹"""
    file_start = time.time()
    
    stats = {
        "source_lines": 0,
        "target_lines": 0,
        "skipped_empty": 0,
        "updated_citation": 0,
        "updated_db": 0,
        "updated_total": 0
    }
    
    timings = {
        "init_temp": 0,          # åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶
        "load_part2": 0,         # åŠ è½½part2æ–‡ä»¶
        "load_db": 0,            # åŠ è½½æ•°æ®åº“æ•°æ®
        "process_target": 0,      # å¤„ç†ç›®æ ‡æ–‡ä»¶
        "write_temp": 0,          # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        "move_file": 0,           # æ›¿æ¢æ–‡ä»¶
        "total": 0                # æ€»è€—æ—¶
    }
    
    # åˆå§‹åŒ–å˜é‡(ç”¨äºæ—¥å¿—è®°å½•)
    part2_data = {}
    db_data = {}
    
    # é˜¶æ®µ1: åˆ›å»ºä¸´æ—¶æ–‡ä»¶ (ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„)
    t1 = time.time()
    temp_fd, temp_path_str = tempfile.mkstemp(
        suffix='.jsonl',
        dir=str(target_file.parent),
        prefix='.tmp_'
    )
    os.close(temp_fd)
    temp_path = Path(temp_path_str)
    timings["init_temp"] = time.time() - t1
    
    try:
        BUFFER_SIZE = 4 * 1024 * 1024  # 4MBç¼“å†²åŒº (ä»1MBå¢åŠ )
        
        # ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„é¿å… Windows è·¯å¾„é—®é¢˜
        with open(str(source_file), 'r', encoding='utf-8', buffering=BUFFER_SIZE) as f_source, \
             open(str(target_file), 'r', encoding='utf-8', buffering=BUFFER_SIZE) as f_target, \
             open(str(temp_path), 'w', encoding='utf-8', buffering=BUFFER_SIZE) as f_temp:
            
                # é˜¶æ®µ2: é¢„åŠ è½½part2æ–‡ä»¶æ•°æ®åˆ°å†…å­˜
                t2 = time.time()
                part2_data = {}
                for line in f_source:
                    line = line.strip()
                    if not line:
                        continue
                    stats["source_lines"] += 1
                    
                    # æ¸…ç†æ§åˆ¶å­—ç¬¦
                    try:
                        record = json_loads(line)
                    except (ValueError, Exception) as e:
                        # å°è¯•æ¸…ç†åå†è§£æ
                        try:
                            cleaned_line = clean_json_line(line)
                            record = json_loads(cleaned_line)
                            log(log_file, f"è­¦å‘Š: æ¸…ç†äº†éæ³•å­—ç¬¦ (part2æ–‡ä»¶ç¬¬{stats['source_lines']}è¡Œ)")
                        except (ValueError, Exception):
                            log(log_file, f"è·³è¿‡: part2æ–‡ä»¶ç¬¬{stats['source_lines']}è¡Œè§£æå¤±è´¥ - {str(e)}")
                            continue
                    
                    if not is_citation_fields_empty(record):
                        corpusid = record.get("corpusid")
                        if corpusid is not None:
                            part2_data[corpusid] = record
                    else:
                        stats["skipped_empty"] += 1
                timings["load_part2"] = time.time() - t2
                
                # é˜¶æ®µ3: ä»æ•°æ®åº“æ‰¹é‡åŠ è½½æ•°æ®(å¦‚æœæä¾›äº†è¿æ¥)
                t3 = time.time()
                db_data = {}
                if db_conn:
                    corpusid_list = list(part2_data.keys())
                    # åˆ†æ‰¹æŸ¥è¯¢(æ¯æ¬¡5000ä¸ª)
                    BATCH_SIZE = 5000
                    for i in range(0, len(corpusid_list), BATCH_SIZE):
                        batch = corpusid_list[i:i+BATCH_SIZE]
                        batch_data = load_db_data(db_conn, batch, db_config, log_file)
                        db_data.update(batch_data)
                timings["load_db"] = time.time() - t3
                
                # é˜¶æ®µ4: æµå¼å¤„ç†ç›®æ ‡æ–‡ä»¶å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶
                t4 = time.time()
                write_buffer = []
                WRITE_BATCH = 10000  # ä»5000å¢åŠ åˆ°10000
                write_time = 0
                
                for target_line in f_target:
                    target_line = target_line.strip()
                    if not target_line:
                        continue
                    
                    stats["target_lines"] += 1
                    
                    # æ¸…ç†æ§åˆ¶å­—ç¬¦
                    try:
                        target_record = json_loads(target_line)
                    except (ValueError, Exception) as e:
                        # å°è¯•æ¸…ç†åå†è§£æ
                        try:
                            cleaned_line = clean_json_line(target_line)
                            target_record = json_loads(cleaned_line)
                            log(log_file, f"è­¦å‘Š: æ¸…ç†äº†éæ³•å­—ç¬¦ (ç›®æ ‡æ–‡ä»¶ç¬¬{stats['target_lines']}è¡Œ)")
                        except (ValueError, Exception):
                            log(log_file, f"è·³è¿‡: ç›®æ ‡æ–‡ä»¶ç¬¬{stats['target_lines']}è¡Œè§£æå¤±è´¥ - {str(e)}")
                            # ç›®æ ‡æ–‡ä»¶è§£æå¤±è´¥ï¼Œä¿ç•™åŸå§‹è¡Œ
                            write_buffer.append(target_line + '\n')
                            if len(write_buffer) >= WRITE_BATCH:
                                tw = time.time()
                                f_temp.writelines(write_buffer)
                                write_time += time.time() - tw
                                write_buffer = []
                            continue
                    
                    target_corpusid = target_record.get("corpusid")
                    
                    record_updated = False
                    
                    # æ›´æ–°å¼•ç”¨å­—æ®µ(ä»part2æ–‡ä»¶)
                    if target_corpusid in part2_data:
                        cnt = update_record_fields(
                            target_record, 
                            part2_data[target_corpusid], 
                            CITATION_FIELDS,
                            skip_if_target_not_empty=False
                        )
                        if cnt > 0:
                            stats["updated_citation"] += 1
                            record_updated = True
                    
                    # æ›´æ–°æ•°æ®åº“å­—æ®µ(å¦‚æœç›®æ ‡å­—æ®µä¸ºç©º)
                    if target_corpusid in db_data:
                        cnt = update_record_fields(
                            target_record, 
                            db_data[target_corpusid], 
                            DB_FIELDS,
                            skip_if_target_not_empty=True  # ç›®æ ‡ä¸ä¸ºç©ºåˆ™è·³è¿‡
                        )
                        if cnt > 0:
                            stats["updated_db"] += 1
                            record_updated = True
                    
                    if record_updated:
                        stats["updated_total"] += 1
                    
                    # æ‰¹é‡å†™å…¥
                    write_buffer.append(json_dumps(target_record) + '\n')
                    if len(write_buffer) >= WRITE_BATCH:
                        tw = time.time()
                        f_temp.writelines(write_buffer)
                        write_time += time.time() - tw
                        write_buffer = []
                
                # å†™å…¥å‰©ä½™æ•°æ®
                if write_buffer:
                    tw = time.time()
                    f_temp.writelines(write_buffer)
                    write_time += time.time() - tw
                
                timings["process_target"] = time.time() - t4 - write_time
                timings["write_temp"] = write_time
        
        # é˜¶æ®µ5: æ›¿æ¢æ–‡ä»¶ (ä¼˜åŒ–ï¼šç›´æ¥æ›¿æ¢é¿å…åˆ é™¤)
        t5 = time.time()
        target_file_str = str(target_file)
        temp_path_str = str(temp_path)
        
        # Windowsä¸Šç›´æ¥æ›¿æ¢ï¼ˆos.replaceæ¯”åˆ é™¤å†é‡å‘½åæ›´å¿«ï¼‰
        os.replace(temp_path_str, target_file_str)
        timings["move_file"] = time.time() - t5
        
    except Exception as e:
        try:
            if os.path.exists(str(temp_path)):
                os.remove(str(temp_path))
        except:
            pass
        raise e
    
    timings["total"] = time.time() - file_start
    stats["time"] = timings["total"]
    stats["timings"] = timings
    
    # è®°å½•è¯¦ç»†æ—¥å¿—
    log(log_file, f"å®Œæˆ: {source_file.name}")
    log(log_file, f"  é˜¶æ®µè€—æ—¶:")
    log(log_file, f"    - åˆå§‹åŒ–ä¸´æ—¶æ–‡ä»¶: {timings['init_temp']:.3f}s")
    log(log_file, f"    - åŠ è½½part2æ–‡ä»¶: {timings['load_part2']:.3f}s ({stats['source_lines']}è¡Œ, {len(part2_data)}æ¡æœ‰æ•ˆ)")
    log(log_file, f"    - åŠ è½½æ•°æ®åº“æ•°æ®: {timings['load_db']:.3f}s ({len(db_data)}æ¡)")
    log(log_file, f"    - å¤„ç†ç›®æ ‡æ–‡ä»¶: {timings['process_target']:.3f}s ({stats['target_lines']}è¡Œ)")
    log(log_file, f"    - å†™å…¥ä¸´æ—¶æ–‡ä»¶: {timings['write_temp']:.3f}s")
    log(log_file, f"    - æ›¿æ¢æ–‡ä»¶: {timings['move_file']:.3f}s")
    log(log_file, f"  æ€»è€—æ—¶: {timings['total']:.3f}s")
    log(log_file, f"  æ›´æ–°ç»Ÿè®¡: {stats['updated_total']}/{stats['target_lines']} (å¼•ç”¨:{stats['updated_citation']}, DB:{stats['updated_db']})")
    
    return stats


def main():
    parser = argparse.ArgumentParser(description='åˆå¹¶part2å’Œæ•°æ®åº“æ•°æ®åˆ°ç›®æ ‡æ–‡ä»¶')
    parser.add_argument('--source-dir', required=True, help='æºæ–‡ä»¶ç›®å½•(xxx_part2.jsonl)')
    parser.add_argument('--target-dir', required=True, help='ç›®æ ‡æ–‡ä»¶ç›®å½•(xxx.jsonl)')
    parser.add_argument('--machine', required=True, 
                       choices=['machine0', 'machine2'],
                       help='æœºå™¨ID (machine0è‡ªåŠ¨è¿æ¥è¿œç¨‹machine2æ•°æ®åº“)')
    
    args = parser.parse_args()
    
    # é…ç½®è·¯å¾„
    SOURCE_DIR = args.source_dir
    TARGET_DIR = args.target_dir
    PROGRESS_DB = Path(__file__).parent.parent / "logs" / "merge_progress.db"
    LOG_FILE = Path(__file__).parent.parent / "logs" / f"merge_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    start_time = time.time()
    
    # åˆå§‹åŒ–
    init_progress_db(PROGRESS_DB)
    
    # åŠ è½½æ•°æ®åº“é…ç½®
    if args.machine == 'machine0':
        # machine0 è‡ªåŠ¨è¿æ¥è¿œç¨‹ machine2 æ•°æ®åº“
        db_config = get_db_config('machine2')
        print(f"ğŸ“¡ machine0: è‡ªåŠ¨è¿æ¥è¿œç¨‹ machine2 æ•°æ®åº“")
    else:
        # machine2 ä½¿ç”¨æœ¬åœ°æ•°æ®åº“é…ç½®
        db_config = get_db_config('machine2')
        print(f"ğŸ’¾ machine2: ä½¿ç”¨æœ¬åœ°æ•°æ®åº“é…ç½®")
    
    log(LOG_FILE, "=" * 80)
    log(LOG_FILE, "å¼•ç”¨æ•°æ®åˆå¹¶å·¥å…· - åŒæºåˆå¹¶+æ–­ç‚¹ç»­ä¼ ")
    log(LOG_FILE, "=" * 80)
    log(LOG_FILE, f"æœºå™¨ID: {args.machine}")
    if args.machine == 'machine0':
        log(LOG_FILE, f"æ•°æ®åº“æ¨¡å¼: è¿œç¨‹è¿æ¥ machine2")
    else:
        log(LOG_FILE, f"æ•°æ®åº“æ¨¡å¼: æœ¬åœ°")
    log(LOG_FILE, f"æºç›®å½•: {SOURCE_DIR}")
    log(LOG_FILE, f"ç›®æ ‡ç›®å½•: {TARGET_DIR}")
    log(LOG_FILE, f"æ•°æ®åº“: {db_config['host']}:{db_config['port']}/{db_config['database']}")
    log(LOG_FILE, f"è¿›åº¦æ•°æ®åº“: {PROGRESS_DB}")
    log(LOG_FILE, f"æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
    log(LOG_FILE, f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log(LOG_FILE, "=" * 80)
    
    # æ£€æŸ¥ç›®å½•
    source_path = Path(SOURCE_DIR)
    target_path = Path(TARGET_DIR)
    
    if not source_path.exists():
        print(f"âŒ é”™è¯¯: æºç›®å½•ä¸å­˜åœ¨: {SOURCE_DIR}")
        return
    
    if not target_path.exists():
        print(f"âŒ é”™è¯¯: ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {TARGET_DIR}")
        return
    
    # è¿æ¥æ•°æ®åº“
    db_conn = None
    try:
        db_conn = connect_pg_db(db_config, LOG_FILE)
        print(f"âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ: {db_config['host']}:{db_config['port']}/{db_config['database']}")
        log(LOG_FILE, f"æ•°æ®åº“è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥(å·²é‡è¯•{MAX_RETRIES}æ¬¡): {e}")
        log(LOG_FILE, f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return
    
    try:
        # è·å–æ‰€æœ‰æºæ–‡ä»¶
        source_files = sorted(source_path.glob("*_part2.jsonl"))
        
        if not source_files:
            print("âŒ é”™è¯¯: æºç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ° *_part2.jsonl æ–‡ä»¶")
            return
        
        # å¯¹äº machine2ï¼Œåªå¤„ç†ç›®æ ‡ç›®å½•ä¸­å­˜åœ¨å¯¹åº”æ–‡ä»¶çš„æºæ–‡ä»¶
        if args.machine == 'machine2':
            # è·å–ç›®æ ‡ç›®å½•æ‰€æœ‰æ–‡ä»¶åï¼ˆä¸å«åç¼€ï¼‰
            target_files_set = set()
            for target_file in target_path.glob("*.jsonl"):
                if not target_file.name.endswith("_part2.jsonl"):
                    target_files_set.add(target_file.stem)
            
            log(LOG_FILE, f"ç›®æ ‡ç›®å½•æ–‡ä»¶æ•°: {len(target_files_set)}")
            
            # è¿‡æ»¤æºæ–‡ä»¶ï¼šåªä¿ç•™ç›®æ ‡ç›®å½•ä¸­å­˜åœ¨å¯¹åº”æ–‡ä»¶çš„
            filtered_source_files = []
            for source_file in source_files:
                source_name = source_file.stem  # ä¾‹å¦‚: "4f694c82_part2"
                if source_name.endswith("_part2"):
                    base_name = source_name[:-6]  # å»æ‰ "_part2"
                    if base_name in target_files_set:
                        filtered_source_files.append(source_file)
            
            source_files = filtered_source_files
            log(LOG_FILE, f"åŒ¹é…çš„æºæ–‡ä»¶æ•°: {len(source_files)}")
            
            if not source_files:
                print("âŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä¸ç›®æ ‡ç›®å½•åŒ¹é…çš„æºæ–‡ä»¶")
                return
        
        # è·å–å·²å®Œæˆçš„æ–‡ä»¶
        completed_files = get_completed_files(PROGRESS_DB)
        
        # è¿‡æ»¤å‡ºæœªå®Œæˆçš„æ–‡ä»¶
        pending_files = [f for f in source_files if f.name not in completed_files]
        
        log(LOG_FILE, f"æ€»æ–‡ä»¶æ•°: {len(source_files)}")
        log(LOG_FILE, f"å·²å®Œæˆ: {len(completed_files)}")
        log(LOG_FILE, f"å¾…å¤„ç†: {len(pending_files)}")
        
        if len(completed_files) > 0:
            print(f"\nğŸ“Š æ–­ç‚¹ç»­ä¼ : å‘ç° {len(completed_files)} ä¸ªå·²å®Œæˆæ–‡ä»¶,ç»§ç»­å¤„ç†å‰©ä½™ {len(pending_files)} ä¸ªæ–‡ä»¶\n")
        else:
            print(f"\nğŸ“Š å¼€å§‹å¤„ç† {len(pending_files)} ä¸ªæ–‡ä»¶\n")
        
        if len(pending_files) == 0:
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ!")
            return
        
        # å…¨å±€ç»Ÿè®¡
        global_stats = {
            "total_files": len(source_files),
            "completed_before": len(completed_files),
            "processed_now": 0,
            "skipped": 0,
            "total_updated": 0,
            "total_citation": 0,
            "total_db": 0
        }
        
        # è¿›åº¦æ¡
        pbar = tqdm(
            pending_files,
            total=len(pending_files),
            desc="å¤„ç†è¿›åº¦",
            unit="file",
            bar_format='{percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, {postfix}]',
            ncols=120
        )
        
        # å¤„ç†æ–‡ä»¶
        for source_file in pbar:
            source_name = source_file.stem
            pbar.set_postfix_str(source_file.name, refresh=False)
            
            if not source_name.endswith("_part2"):
                global_stats["skipped"] += 1
                continue
            
            base_name = source_name[:-6]
            target_file = target_path / f"{base_name}.jsonl"
            
            # Machine2 å·²é¢„å…ˆè¿‡æ»¤ï¼Œæ— éœ€æ£€æŸ¥ï¼›Machine0 éœ€è¦æ£€æŸ¥
            if args.machine != 'machine2':
                # ä½¿ç”¨ os.path.isfile æ›´å¯é 
                if not os.path.isfile(str(target_file)):
                    log(LOG_FILE, f"è·³è¿‡: ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ - {target_file.name}")
                    global_stats["skipped"] += 1
                    continue
            
            try:
                stats = process_file_pair(source_file, target_file, db_conn, db_config, LOG_FILE)
                
                # æ ‡è®°ä¸ºå·²å®Œæˆ
                mark_file_done(PROGRESS_DB, source_file.name)
                
                global_stats["processed_now"] += 1
                global_stats["total_updated"] += stats["updated_total"]
                global_stats["total_citation"] += stats["updated_citation"]
                global_stats["total_db"] += stats["updated_db"]
                
            except Exception as e:
                log(LOG_FILE, f"é”™è¯¯: å¤„ç† {source_file.name} å¤±è´¥ - {str(e)}")
                print(f"âš ï¸  å¤„ç† {source_file.name} å¤±è´¥: {e}")
                global_stats["skipped"] += 1
                continue
        
        pbar.close()
        
        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        avg_time = total_time / global_stats["processed_now"] if global_stats["processed_now"] > 0 else 0
        
        print("\n" + "=" * 80)
        print("å¤„ç†å®Œæˆ!")
        print("=" * 80)
        print(f"æ€»æ–‡ä»¶æ•°: {global_stats['total_files']}")
        print(f"ä¹‹å‰å·²å®Œæˆ: {global_stats['completed_before']}")
        print(f"æœ¬æ¬¡å¤„ç†: {global_stats['processed_now']}")
        print(f"è·³è¿‡: {global_stats['skipped']}")
        print(f"æ€»æ›´æ–°è®°å½•: {global_stats['total_updated']}")
        print(f"  - å¼•ç”¨å­—æ®µæ›´æ–°: {global_stats['total_citation']}")
        print(f"  - æ•°æ®åº“å­—æ®µæ›´æ–°: {global_stats['total_db']}")
        print(f"å¹³å‡é€Ÿåº¦: {avg_time:.2f}s/file")
        print(f"æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"\næ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
        print(f"è¿›åº¦æ•°æ®åº“: {PROGRESS_DB}")
        print("=" * 80)
        
        log(LOG_FILE, "\n" + "=" * 80)
        log(LOG_FILE, "å¤„ç†å®Œæˆ - å…¨å±€ç»Ÿè®¡")
        log(LOG_FILE, "=" * 80)
        log(LOG_FILE, f"æ€»æ–‡ä»¶æ•°: {global_stats['total_files']}")
        log(LOG_FILE, f"ä¹‹å‰å·²å®Œæˆ: {global_stats['completed_before']}")
        log(LOG_FILE, f"æœ¬æ¬¡å¤„ç†: {global_stats['processed_now']}")
        log(LOG_FILE, f"è·³è¿‡: {global_stats['skipped']}")
        log(LOG_FILE, f"æ€»æ›´æ–°è®°å½•: {global_stats['total_updated']}")
        log(LOG_FILE, f"  - å¼•ç”¨å­—æ®µæ›´æ–°: {global_stats['total_citation']}")
        log(LOG_FILE, f"  - æ•°æ®åº“å­—æ®µæ›´æ–°: {global_stats['total_db']}")
        log(LOG_FILE, f"å¹³å‡é€Ÿåº¦: {avg_time:.2f}s/file")
        log(LOG_FILE, f"æ€»è€—æ—¶: {total_time:.2f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
        log(LOG_FILE, f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log(LOG_FILE, "=" * 80)
        
    finally:
        if db_conn:
            db_conn.close()
            print("\nâœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    main()
