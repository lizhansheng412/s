"""
é«˜æ€§èƒ½å¯¼å…¥citationsæ•°æ®å¹¶æ›´æ–°temp_importè¡¨çš„citationså’Œreferenceså­—æ®µ

å¤„ç†æµç¨‹ï¼š
1. æ‰«ææ‰€æœ‰citations gzæ–‡ä»¶ï¼Œå¯¼å…¥åˆ°citation_rawæ˜ å°„è¡¨
2. åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
3. èšåˆreferencesæ•°æ®å¹¶å†™å…¥ç¼“å­˜è¡¨ï¼ˆcitingcorpusid -> citedcorpusidåˆ—è¡¨ï¼‰
4. èšåˆcitationsæ•°æ®å¹¶å†™å…¥ç¼“å­˜è¡¨ï¼ˆcitedcorpusid -> citingcorpusidåˆ—è¡¨ï¼‰
5. æ™ºèƒ½å¡«å……temp_importè¡¨ï¼ˆè‡ªåŠ¨é€‰æ‹©INSERTæˆ–UPDATEç­–ç•¥ï¼‰
6. æ¸…ç†ä¸´æ—¶è¡¨

æ€§èƒ½ä¼˜åŒ–ï¼š
- ä½¿ç”¨COPYå‘½ä»¤æ‰¹é‡å¯¼å…¥
- å»¶è¿Ÿåˆ›å»ºç´¢å¼•ï¼ˆå…ˆæ’å…¥åå»ºç´¢å¼•ï¼‰
- SQLå±‚é¢èšåˆï¼ˆé¿å…Pythonå¾ªç¯ï¼‰
- æ™ºèƒ½ç­–ç•¥ï¼šç©ºè¡¨ç”¨INSERTæ‰¹é‡æ’å…¥ï¼Œæœ‰æ•°æ®ç”¨UPDATEæ›´æ–°
- INSERTæ¨¡å¼ï¼šåˆ†æ‰¹å†™å…¥å¹¶å®æ—¶è¾“å‡ºè¿›åº¦ï¼ˆå¯å¤ç”¨ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import gzip
import orjson
import psycopg2
from datetime import datetime
import time
from tqdm import tqdm
from db_config import get_db_config

TEMP_TABLE = "temp_import"
CITATION_RAW_TABLE = "citation_raw"
RUNNING_LOG = Path(__file__).parent.parent / "logs" / "running.log"


def log_performance(stage, **metrics):
    """è®°å½•æ€§èƒ½æ—¥å¿—"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    metrics_str = " | ".join([f"{k}={v}" for k, v in metrics.items()])
    log_line = f"[{timestamp}] {stage} | {metrics_str}\n"
    with open(RUNNING_LOG, 'a', encoding='utf-8') as f:
        f.write(log_line)
    print(f"  {stage}: {metrics_str}")


class CopyStream:
    """å°†è¡Œè¿­ä»£å™¨åŒ…è£…ä¸ºpsycopg2 copy_expertå¯æ¶ˆè´¹çš„æµ"""

    def __init__(self, iterator, chunk_size=65536):
        self.iterator = iterator
        self.chunk_size = chunk_size
        self._buffer = bytearray()
        self._exhausted = False

    def readable(self):
        return True

    def read(self, size=-1):
        if size == -1:
            chunks = [bytes(self._buffer)]
            self._buffer.clear()
            for chunk in self.iterator:
                chunks.append(chunk)
            self._exhausted = True
            return b''.join(chunks)

        while len(self._buffer) < max(size, self.chunk_size) and not self._exhausted:
            try:
                self._buffer.extend(next(self.iterator))
            except StopIteration:
                self._exhausted = True
                break

        if not self._buffer:
            return b''

        if size >= len(self._buffer):
            data = bytes(self._buffer)
            self._buffer.clear()
            return data

        data = bytes(self._buffer[:size])
        del self._buffer[:size]
        return data


def create_citation_raw_table(cursor, truncate=False, no_count=False):
    """åˆ›å»ºcitation_rawæ˜ å°„è¡¨ï¼ˆä¸åˆ›å»ºç´¢å¼•ï¼‰"""
    print("\nã€é˜¶æ®µ0ã€‘åˆ›å»ºæ˜ å°„è¡¨...")
    
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {CITATION_RAW_TABLE} (
            citingcorpusid BIGINT NOT NULL,
            citedcorpusid BIGINT NOT NULL
        ) WITH (autovacuum_enabled = false)
    """)
    
    # å¿«é€Ÿå­˜åœ¨æ€§æ£€æŸ¥ï¼Œé¿å…å¯¹è¶…å¤§è¡¨åšå…¨è¡¨COUNT
    cursor.execute("SELECT to_regclass(%s) IS NOT NULL", (CITATION_RAW_TABLE,))
    exists = cursor.fetchone()[0]
    if not exists:
        print(f"  âœ“ è¡¨åˆ›å»ºæˆåŠŸ")
        return

    if no_count:
        # è·³è¿‡COUNT(*)ï¼Œç›´æ¥ç»™å‡ºæç¤º
        if truncate:
            print(f"  æ­£åœ¨æ¸…ç©ºè¡¨ï¼ˆè·³è¿‡è®¡æ•°ï¼‰...")
            cursor.execute(f"TRUNCATE TABLE {CITATION_RAW_TABLE}")
            print("  âœ“ è¡¨å·²æ¸…ç©º")
        else:
            print(f"  â“˜ è¡¨å·²å­˜åœ¨ï¼ˆè·³è¿‡è®¡æ•°ï¼‰ï¼Œå°†ç»§ç»­åç»­é˜¶æ®µ")
        return

    # æ­£å¸¸è·¯å¾„ï¼šåšä¸€æ¬¡COUNTï¼ˆè¡¨å¾ˆå¤§æ—¶å¯èƒ½è¾ƒæ…¢ï¼‰
    cursor.execute(f"SELECT COUNT(*) FROM {CITATION_RAW_TABLE}")
    count = cursor.fetchone()[0]
    if count > 0:
        if truncate:
            print(f"  æ­£åœ¨æ¸…ç©ºè¡¨ï¼ˆ{count:,} æ¡è®°å½•ï¼‰...")
            cursor.execute(f"TRUNCATE TABLE {CITATION_RAW_TABLE}")
            print("  âœ“ è¡¨å·²æ¸…ç©º")
        else:
            print(f"  âš ï¸  è¡¨å·²å­˜åœ¨ä¸”æœ‰ {count:,} æ¡è®°å½•ï¼Œå°†ç»§ç»­è¿½åŠ æ•°æ®")
    else:
        print(f"  âœ“ è¡¨åˆ›å»ºæˆåŠŸ")


def import_citations_gz(gz_directory, cursor, conn):
    """é˜¶æ®µ1ï¼šå¯¼å…¥æ‰€æœ‰citations gzæ–‡ä»¶åˆ°citation_rawè¡¨"""
    print("\nã€é˜¶æ®µ1ã€‘å¯¼å…¥citationsæ•°æ®...")
    
    gz_dir = Path(gz_directory)
    all_gz_files = sorted(gz_dir.glob("*.gz"))
    
    if not all_gz_files:
        raise FileNotFoundError(f"åœ¨ {gz_directory} ä¸­æ²¡æœ‰æ‰¾åˆ°gzæ–‡ä»¶")
    
    print(f"  å‘ç° {len(all_gz_files)} ä¸ªgzæ–‡ä»¶")
    
    # ä¼˜åŒ–æ•°æ®åº“é…ç½®
    cursor.execute("SET synchronous_commit = OFF")
    cursor.execute("SET work_mem = '512MB'")
    
    copy_sql = f"COPY {CITATION_RAW_TABLE} (citingcorpusid, citedcorpusid) FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t')"
    
    total_records = 0
    start_time = time.time()
    
    with tqdm(all_gz_files, desc="  å¯¼å…¥è¿›åº¦", unit="file") as pbar:
        for gz_file in pbar:
            file_start = time.time()
            file_count = 0
            
            def row_iterator():
                nonlocal file_count
                with gzip.open(gz_file, 'rt', encoding='utf-8', errors='replace') as f:
                    for line in f:
                        try:
                            data = orjson.loads(line.strip())
                            citing = data.get('citingcorpusid')
                            cited = data.get('citedcorpusid')
                            
                            if citing is not None and cited is not None:
                                file_count += 1
                                yield f"{citing}\t{cited}\n".encode('utf-8')
                        except Exception:
                            continue
            
            try:
                cursor.copy_expert(copy_sql, CopyStream(row_iterator()))
                conn.commit()
                
                total_records += file_count
                file_time = time.time() - file_start
                
                pbar.set_postfix_str(
                    f"å½“å‰: {file_count:,}æ¡/{file_time:.1f}ç§’ | "
                    f"æ€»è®¡: {total_records:,}æ¡"
                )
                
            except Exception as e:
                print(f"\n  âœ— æ–‡ä»¶ {gz_file.name} å¯¼å…¥å¤±è´¥: {e}")
                conn.rollback()
                continue
    
    total_time = time.time() - start_time
    speed = total_records / total_time if total_time > 0 else 0
    
    log_performance(
        "é˜¶æ®µ1-å¯¼å…¥å®Œæˆ",
        files=len(all_gz_files),
        records=f"{total_records:,}",
        time_sec=f"{total_time:.2f}",
        speed_per_sec=f"{speed:.0f}"
    )


def create_indexes(cursor, conn):
    """é˜¶æ®µ2ï¼šåˆ›å»ºç´¢å¼•ï¼ˆæè‡´ä¼˜åŒ–ç‰ˆï¼‰"""
    print("\nã€é˜¶æ®µ2ã€‘åˆ›å»ºç´¢å¼•...")
    
    start_time = time.time()
    
    # æè‡´ä¼˜åŒ–å‚æ•°
    cursor.execute("SET maintenance_work_mem = '8GB'")  # æå‡åˆ°8GB
    cursor.execute("SET max_parallel_maintenance_workers = 8")  # å¹¶è¡Œæ„å»º
    cursor.execute("SET max_parallel_workers = 16")  # å…¨å±€å¹¶è¡Œä¸Šé™
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
    cursor.execute("""
        SELECT COUNT(*) FROM pg_indexes 
        WHERE tablename = %s AND indexname IN ('idx_citation_citing', 'idx_citation_cited')
    """, (CITATION_RAW_TABLE,))
    existing_count = cursor.fetchone()[0]
    
    if existing_count == 2:
        print("  âœ“ ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
    elif existing_count == 1:
        print("  âš ï¸  ä»…æœ‰ä¸€ä¸ªç´¢å¼•å­˜åœ¨ï¼Œè¡¥å»ºç¼ºå¤±ç´¢å¼•...")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_citation_citing ON {CITATION_RAW_TABLE} (citingcorpusid)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_citation_cited ON {CITATION_RAW_TABLE} (citedcorpusid)")
    else:
        print("  åˆ›å»ºcitingcorpusidç´¢å¼•ï¼ˆå¹¶è¡Œæ„å»ºä¸­ï¼‰...")
        cursor.execute(f"CREATE INDEX idx_citation_citing ON {CITATION_RAW_TABLE} (citingcorpusid)")
        
        print("  åˆ›å»ºcitedcorpusidç´¢å¼•ï¼ˆå¹¶è¡Œæ„å»ºä¸­ï¼‰...")
        cursor.execute(f"CREATE INDEX idx_citation_cited ON {CITATION_RAW_TABLE} (citedcorpusid)")
    
    print("  æ”¶é›†ç»Ÿè®¡ä¿¡æ¯...")
    cursor.execute(f"ANALYZE {CITATION_RAW_TABLE}")
    
    conn.commit()
    
    elapsed = time.time() - start_time
    log_performance("é˜¶æ®µ2-ç´¢å¼•åˆ›å»º", time_sec=f"{elapsed:.2f}")


def build_references(cursor, conn, force=False):
    """é˜¶æ®µ3ï¼šæ„é€ referencesæ•°æ®ï¼ˆå¯å¤ç”¨ç¼“å­˜ç‰ˆï¼‰"""
    print("\nã€é˜¶æ®µ3ã€‘æ„é€ referencesæ•°æ®...")
    
    cursor.execute("SELECT to_regclass('temp_references')")
    exists = cursor.fetchone()[0]
    if exists and not force:
        print("  â“˜ temp_references å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å»ºï¼ˆä½¿ç”¨ --force-stage3 å¯å¼ºåˆ¶é‡å»ºï¼‰")
        return
    
    start_time = time.time()
    
    # ğŸš€ ç»ˆææé€Ÿï¼šä½¿ç”¨array_aggä»£æ›¿json_aggï¼ˆå¿«3-5å€ï¼‰
    cursor.execute("SET max_parallel_workers_per_gather = 0")
    cursor.execute("SET work_mem = '8GB'")
    cursor.execute("SET temp_buffers = '2GB'")
    cursor.execute("SET hash_mem_multiplier = 2.0")
    cursor.execute("SET enable_hashagg = ON")
    print("  é‡å»ºç¼“å­˜è¡¨ temp_references ...")
    cursor.execute("DROP TABLE IF EXISTS temp_references")
    cursor.execute(f"""
        CREATE UNLOGGED TABLE temp_references AS
        SELECT 
            citingcorpusid AS corpusid,
            array_agg(citedcorpusid) AS ref_ids
        FROM {CITATION_RAW_TABLE}
        GROUP BY citingcorpusid
    """)
    
    print("  åˆ›å»ºç´¢å¼•...")
    cursor.execute("SET maintenance_work_mem = '4GB'")
    cursor.execute("DROP INDEX IF EXISTS idx_temp_references_corpusid")
    cursor.execute("CREATE INDEX idx_temp_references_corpusid ON temp_references (corpusid)")
    
    cursor.execute("""
        SELECT reltuples::bigint 
        FROM pg_class 
        WHERE relname = 'temp_references'
    """)
    count = cursor.fetchone()[0]
    
    conn.commit()
    
    elapsed = time.time() - start_time
    log_performance("é˜¶æ®µ3-referencesæ„é€ ", records=f"{count:,}", time_sec=f"{elapsed:.2f}")


def build_citations(cursor, conn, force=False):
    """é˜¶æ®µ4ï¼šæ„é€ citationsæ•°æ®ï¼ˆå¯å¤ç”¨ç¼“å­˜ç‰ˆï¼‰"""
    print("\nã€é˜¶æ®µ4ã€‘æ„é€ citationsæ•°æ®...")
    
    cursor.execute("SELECT to_regclass('temp_citations')")
    exists = cursor.fetchone()[0]
    if exists and not force:
        print("  â“˜ temp_citations å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å»ºï¼ˆä½¿ç”¨ --force-stage4 å¯å¼ºåˆ¶é‡å»ºï¼‰")
        return
    
    start_time = time.time()
    
    print("  é‡å»ºç¼“å­˜è¡¨ temp_citations ...")
    cursor.execute("DROP TABLE IF EXISTS temp_citations")
    cursor.execute(f"""
        CREATE UNLOGGED TABLE temp_citations AS
        SELECT 
            citedcorpusid AS corpusid,
            array_agg(citingcorpusid) AS cite_ids
        FROM {CITATION_RAW_TABLE}
        GROUP BY citedcorpusid
    """)
    
    print("  åˆ›å»ºç´¢å¼•...")
    cursor.execute("DROP INDEX IF EXISTS idx_temp_citations_corpusid")
    cursor.execute("CREATE INDEX idx_temp_citations_corpusid ON temp_citations (corpusid)")
    
    cursor.execute("""
        SELECT reltuples::bigint 
        FROM pg_class 
        WHERE relname = 'temp_citations'
    """)
    count = cursor.fetchone()[0]
    
    conn.commit()
    
    elapsed = time.time() - start_time
    log_performance("é˜¶æ®µ4-citationsæ„é€ ", records=f"{count:,}", time_sec=f"{elapsed:.2f}")


def update_temp_import(cursor, conn):
    """é˜¶æ®µ5ï¼šæ™ºèƒ½å¡«å……temp_importè¡¨ï¼ˆè‡ªåŠ¨é€‰æ‹©INSERTæˆ–UPDATEç­–ç•¥ï¼‰"""
    print("\nã€é˜¶æ®µ5ã€‘å¡«å……temp_importè¡¨...")
    
    start_time = time.time()
    
    # ç¡®ä¿å‰ç½®ç¼“å­˜å­˜åœ¨
    for table_name in ("temp_references", "temp_citations"):
        cursor.execute("SELECT to_regclass(%s)", (table_name,))
        if not cursor.fetchone()[0]:
            raise RuntimeError(
                f"{table_name} ä¸å­˜åœ¨ã€‚è¯·å…ˆæ‰§è¡Œé˜¶æ®µ{3 if table_name == 'temp_references' else 4} "
                "ï¼ˆæˆ–ä½¿ç”¨ --force-stage3/--force-stage4 é‡å»ºï¼‰å†æ‰§è¡Œé˜¶æ®µ5ã€‚"
            )
    
    # æ£€æŸ¥temp_importè¡¨æ˜¯å¦ä¸ºç©º
    cursor.execute(f"SELECT COUNT(*) FROM {TEMP_TABLE}")
    existing_count = cursor.fetchone()[0]
    
    cursor.execute("SET work_mem = '8GB'")
    cursor.execute(f"ALTER TABLE {TEMP_TABLE} SET (autovacuum_enabled = false)")
    
    # æ­¥éª¤1: åˆ›å»ºtitleæ˜ å°„ç¼“å­˜
    print("  æ­¥éª¤1: åˆ›å»ºtitleæ˜ å°„ç¼“å­˜...")
    step1_start = time.time()
    cursor.execute(f"""
        CREATE TEMP TABLE temp_title_cache AS
        SELECT 
            corpusid,
            COALESCE(title, '') as title
        FROM corpusid_mapping_title
        WHERE corpusid IN (
            SELECT DISTINCT citedcorpusid FROM {CITATION_RAW_TABLE}
            UNION
            SELECT DISTINCT citingcorpusid FROM {CITATION_RAW_TABLE}
        )
    """)
    cursor.execute("CREATE INDEX ON temp_title_cache (corpusid)")
    print(f"    âœ“ å®Œæˆï¼ˆ{time.time() - step1_start:.1f}ç§’ï¼‰")
    
    if existing_count == 0:
        # è¡¨ä¸ºç©ºï¼šä½¿ç”¨åˆ†æ‰¹INSERTæ‰¹é‡å¡«å……
        print(f"\n  â“˜ temp_importè¡¨ä¸ºç©ºï¼Œä½¿ç”¨åˆ†æ‰¹INSERTæ¨¡å¼ï¼ˆè¶…å¤§è§„æ¨¡å®‰å…¨ç­–ç•¥ï¼‰")
        
        # æ­¥éª¤2: åˆ†æ‰¹èšåˆå¹¶æ’å…¥citations/referencesæ•°æ®
        print("  æ­¥éª¤2: åˆ†æ‰¹èšåˆå¹¶æ’å…¥citations/referencesæ•°æ®...")
        step2_start = time.time()
        
        # ä¼˜åŒ–æ•°æ®åº“é…ç½®
        cursor.execute("SET synchronous_commit = OFF")
        cursor.execute("SET work_mem = '8GB'")
        
        # æ„å»ºå…¨é‡corpusidé›†åˆ
        print("    2.1: å‡†å¤‡corpusidé›†åˆï¼ˆå»é‡ï¼‰...")
        cursor.execute(f"""
            CREATE TEMP TABLE temp_all_ids AS
            SELECT DISTINCT citingcorpusid AS corpusid FROM {CITATION_RAW_TABLE}
            UNION
            SELECT DISTINCT citedcorpusid AS corpusid FROM {CITATION_RAW_TABLE}
        """)
        cursor.execute("CREATE INDEX ON temp_all_ids (corpusid)")
        cursor.execute("SELECT COUNT(*) FROM temp_all_ids")
        total_ids = cursor.fetchone()[0]
        print(f"        âœ“ corpusid æ€»æ•°: {total_ids:,}")
        
        batch_size = 50000
        batch_cursor = conn.cursor(name="temp_all_ids_cursor")
        batch_cursor.itersize = batch_size
        batch_cursor.execute("SELECT corpusid FROM temp_all_ids ORDER BY corpusid")
        
        total_inserted = 0
        batch_num = 0
        while True:
            rows = batch_cursor.fetchmany(batch_size)
            if not rows:
                break
            corpus_ids = [row[0] for row in rows]
            batch_num += 1
            batch_start = time.time()
            cursor.execute(f"""
                INSERT INTO {TEMP_TABLE} (corpusid, "references", "citations", is_done)
                SELECT 
                    ids.corpusid,
                    COALESCE(
                        (SELECT json_agg(
                            json_build_object(
                                'corpusid', ref_id,
                                'title', COALESCE(tc.title, '')
                            )
                        )::TEXT
                        FROM unnest(tr.ref_ids) AS ref_id
                        LEFT JOIN temp_title_cache tc ON tc.corpusid = ref_id),
                        '[]'
                    ) AS references,
                    COALESCE(
                        (SELECT json_agg(
                            json_build_object(
                                'corpusid', cite_id,
                                'title', COALESCE(tc2.title, '')
                            )
                        )::TEXT
                        FROM unnest(tcite.cite_ids) AS cite_id
                        LEFT JOIN temp_title_cache tc2 ON tc2.corpusid = cite_id),
                        '[]'
                    ) AS citations,
                    FALSE AS is_done
                FROM unnest(%s::bigint[]) AS ids(corpusid)
                LEFT JOIN temp_references tr ON tr.corpusid = ids.corpusid
                LEFT JOIN temp_citations tcite ON tcite.corpusid = ids.corpusid
            """, (corpus_ids,))
            inserted = len(corpus_ids)
            total_inserted += inserted
            batch_time = time.time() - batch_start
            speed = inserted / batch_time if batch_time > 0 else 0
            print(f"        æ‰¹æ¬¡{batch_num:>4}: æ’å…¥{inserted:,}æ¡ | ç´¯è®¡{total_inserted:,}æ¡ | {batch_time:.1f}ç§’ | {speed:,.0f}æ¡/ç§’")
        
        batch_cursor.close()
        cursor.execute("DROP TABLE temp_all_ids")
        print(f"    âœ“ æ­¥éª¤2å®Œæˆï¼ˆ{time.time() - step2_start:.1f}ç§’ï¼Œæ’å…¥{total_inserted:,}æ¡ï¼‰")
        
        # æ­¥éª¤3: ä¸€æ¬¡æ€§æ„å»ºcorpusidç´¢å¼•
        print("  æ­¥éª¤3: æ„å»ºcorpusidç´¢å¼•...")
        step3_start = time.time()
        cursor.execute("""
            SELECT COUNT(*) FROM pg_indexes 
            WHERE tablename = %s AND indexname = 'idx_temp_import_corpusid'
        """, (TEMP_TABLE,))
        
        if cursor.fetchone()[0] == 0:
            cursor.execute("SET maintenance_work_mem = '8GB'")
            cursor.execute("SET max_parallel_maintenance_workers = 8")
            cursor.execute(f"CREATE INDEX idx_temp_import_corpusid ON {TEMP_TABLE} (corpusid)")
            print(f"    âœ“ ç´¢å¼•åˆ›å»ºå®Œæˆï¼ˆ{time.time() - step3_start:.1f}ç§’ï¼‰")
        else:
            print("    âœ“ ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
        
        log_performance(
            "é˜¶æ®µ5-åˆ†æ‰¹æ’å…¥å®Œæˆ",
            mode="BATCH_INSERT",
            batches=f"{batch_num}",
            total_inserted=f"{total_inserted:,}",
            time_sec=f"{time.time() - start_time:.2f}"
        )
    else:
        # è¡¨æœ‰æ•°æ®ï¼šä½¿ç”¨UPDATEæ›´æ–°æ¨¡å¼
        print(f"\n  â“˜ temp_importè¡¨å·²æœ‰{existing_count:,}æ¡æ•°æ®ï¼Œä½¿ç”¨UPDATEæ›´æ–°æ¨¡å¼")
        
        # æ­¥éª¤2: æ›´æ–°referenceså­—æ®µ
        print("  æ­¥éª¤2: æ›´æ–°referenceså­—æ®µï¼ˆarrayâ†’JSON+titleï¼‰...")
        step2_start = time.time()
        cursor.execute(f"""
            UPDATE {TEMP_TABLE} ti
            SET "references" = (
                SELECT json_agg(
                    json_build_object(
                        'corpusid', ref_id,
                        'title', COALESCE(tc.title, '')
                    )
                )::TEXT
                FROM unnest(tr.ref_ids) AS ref_id
                LEFT JOIN temp_title_cache tc ON tc.corpusid = ref_id
            )
            FROM temp_references tr
            WHERE ti.corpusid = tr.corpusid
        """)
        ref_count = cursor.rowcount
        print(f"    âœ“ å®Œæˆï¼ˆ{time.time() - step2_start:.1f}ç§’ï¼Œæ›´æ–°{ref_count:,}æ¡ï¼‰")
        
        # æ­¥éª¤3: æ›´æ–°citationså­—æ®µ
        print("  æ­¥éª¤3: æ›´æ–°citationså­—æ®µï¼ˆarrayâ†’JSON+titleï¼‰...")
        step3_start = time.time()
        cursor.execute(f"""
            UPDATE {TEMP_TABLE} ti
            SET "citations" = (
                SELECT json_agg(
                    json_build_object(
                        'corpusid', cite_id,
                        'title', COALESCE(tc.title, '')
                    )
                )::TEXT
                FROM unnest(tcite.cite_ids) AS cite_id
                LEFT JOIN temp_title_cache tc ON tc.corpusid = cite_id
            )
            FROM temp_citations tcite
            WHERE ti.corpusid = tcite.corpusid
        """)
        cite_count = cursor.rowcount
        print(f"    âœ“ å®Œæˆï¼ˆ{time.time() - step3_start:.1f}ç§’ï¼Œæ›´æ–°{cite_count:,}æ¡ï¼‰")
        
        # å¡«å……ç©ºå€¼
        print("  å¡«å……ç©ºå€¼...")
        cursor.execute(f"""
            UPDATE {TEMP_TABLE}
            SET 
                "references" = COALESCE("references", '[]'),
                "citations" = COALESCE("citations", '[]')
            WHERE "references" IS NULL OR "citations" IS NULL
        """)
        
        log_performance(
            "é˜¶æ®µ5-æ›´æ–°å®Œæˆ",
            mode="UPDATE",
            references_updated=f"{ref_count:,}",
            citations_updated=f"{cite_count:,}",
            time_sec=f"{time.time() - start_time:.2f}"
        )
    
    # æ¸…ç†titleç¼“å­˜
    cursor.execute("DROP TABLE temp_title_cache")
    
    cursor.execute(f"ALTER TABLE {TEMP_TABLE} SET (autovacuum_enabled = true)")
    
    conn.commit()
    
    elapsed = time.time() - start_time
    print(f"\n  æ€»è€—æ—¶: {elapsed:.2f}ç§’")


def cleanup(cursor, conn, keep_citation_raw=True):
    """é˜¶æ®µ6ï¼šæ¸…ç†æç¤ºï¼ˆä¸è‡ªåŠ¨åˆ é™¤ä»»ä½•è¡¨ï¼Œç”±ç”¨æˆ·æ‰‹åŠ¨æ¸…ç†ï¼‰"""
    print("\nã€é˜¶æ®µ6ã€‘æ¸…ç†æç¤º...")
    
    # æ£€æŸ¥å„è¡¨æ˜¯å¦å­˜åœ¨åŠè®°å½•æ•°
    print("\n  å½“å‰è¡¨çŠ¶æ€ï¼š")
    
    # æ£€æŸ¥temp_references
    try:
        cursor.execute("SELECT COUNT(*) FROM temp_references")
        count = cursor.fetchone()[0]
        print(f"  - temp_references: {count:,} æ¡ï¼ˆç¼“å­˜è¡¨ï¼Œå¯å¤ç”¨ï¼‰")
    except:
        print(f"  - temp_references: ä¸å­˜åœ¨")
    
    # æ£€æŸ¥temp_citations
    try:
        cursor.execute("SELECT COUNT(*) FROM temp_citations")
        count = cursor.fetchone()[0]
        print(f"  - temp_citations: {count:,} æ¡ï¼ˆç¼“å­˜è¡¨ï¼Œå¯å¤ç”¨ï¼‰")
    except:
        print(f"  - temp_citations: ä¸å­˜åœ¨")
    
    # æ£€æŸ¥citation_raw
    try:
        cursor.execute(f"SELECT COUNT(*) FROM {CITATION_RAW_TABLE}")
        count = cursor.fetchone()[0]
        print(f"  - {CITATION_RAW_TABLE}: {count:,} æ¡ï¼ˆé‡è¦æ•°æ®ï¼‰")
    except:
        print(f"  - {CITATION_RAW_TABLE}: ä¸å­˜åœ¨")
    
    print("\n  â“˜ æ‰€æœ‰è¡¨å·²ä¿ç•™ï¼Œå¦‚éœ€æ‰‹åŠ¨æ¸…ç†ï¼Œè¯·æ‰§è¡Œï¼š")
    print(f"     -- æ¸…ç†ç¼“å­˜è¡¨ï¼ˆéœ€è¦æ—¶æ‰‹åŠ¨æ‰§è¡Œï¼‰")
    print(f"     DROP TABLE IF EXISTS temp_references;")
    print(f"     DROP TABLE IF EXISTS temp_citations;")
    print(f"     DROP TABLE IF EXISTS temp_title_cache;")
    print(f"")
    print(f"     -- æ¸…ç†citation_rawï¼ˆè°¨æ…ï¼14äº¿+æ¡æ•°æ®ï¼Œéœ€1-2å°æ—¶é‡å»ºï¼‰")
    print(f"     DROP TABLE IF EXISTS {CITATION_RAW_TABLE};")
    
    conn.commit()


def run_full_pipeline(
    gz_directory,
    machine_id='machine0',
    keep_citation_raw=True,
    truncate=False,
    skip_import=False,
    skip_index=False,
    skip_stage3=False,
    skip_stage4=False,
    skip_stage5=False,
    only_import=False,
    only_index=False,
    only_stage3=False,
    only_stage4=False,
    only_stage5=False,
    force_stage3=False,
    force_stage4=False,
    no_count=False,
    skip_cleanup=False,
):
    """æ‰§è¡Œå®Œæ•´çš„citationså¤„ç†æµç¨‹
    
    å‚æ•°:
        gz_directory: citations gzæ–‡ä»¶ç›®å½•
        machine_id: ç›®æ ‡æ•°æ®åº“ (é»˜è®¤: machine0)
        keep_citation_raw: ä¿ç•™citation_rawè¡¨ (é»˜è®¤: Trueï¼Œé˜²æ­¢è¯¯åˆ )
        truncate: æ¸…ç©ºcitation_rawé‡æ–°å¯¼å…¥ (é»˜è®¤: False)
        skip_import: è·³è¿‡å¯¼å…¥é˜¶æ®µ (é»˜è®¤: False)
        skip_index: è·³è¿‡å»ºç´¢å¼•é˜¶æ®µ (é»˜è®¤: False)
        skip_stage3: è·³è¿‡é˜¶æ®µ3ç¼“å­˜æ„å»º (é»˜è®¤: False)
        skip_stage4: è·³è¿‡é˜¶æ®µ4ç¼“å­˜æ„å»º (é»˜è®¤: False)
        skip_stage5: è·³è¿‡é˜¶æ®µ5æ›´æ–° (é»˜è®¤: False)
        only_import: ä»…æ‰§è¡Œé˜¶æ®µ0-1ï¼ˆå¯¼å…¥ï¼‰ï¼Œç„¶ååœæ­¢ (é»˜è®¤: False)
        only_index: ä»…æ‰§è¡Œé˜¶æ®µ2ï¼ˆå»ºç´¢å¼•ï¼‰ï¼Œç„¶ååœæ­¢ (é»˜è®¤: False)
        only_stage3: ä»…æ‰§è¡Œé˜¶æ®µ3ï¼ˆæ„é€ referencesç¼“å­˜ï¼‰ (é»˜è®¤: False)
        only_stage4: ä»…æ‰§è¡Œé˜¶æ®µ4ï¼ˆæ„é€ citationsç¼“å­˜ï¼‰ (é»˜è®¤: False)
        only_stage5: ä»…æ‰§è¡Œé˜¶æ®µ5ï¼ˆå¡«å……temp_importè¡¨ï¼‰ (é»˜è®¤: False)
        force_stage3: å¼ºåˆ¶é‡å»ºé˜¶æ®µ3ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰ (é»˜è®¤: False)
        force_stage4: å¼ºåˆ¶é‡å»ºé˜¶æ®µ4ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰ (é»˜è®¤: False)
        no_count: è·³è¿‡è¡¨è®¡æ•° (é»˜è®¤: False)
        skip_cleanup: è·³è¿‡é˜¶æ®µ6æ¸…ç†æç¤º (é»˜è®¤: False)
    """
    print("=" * 80)
    print("Citationsæ•°æ®é«˜æ€§èƒ½å¤„ç†æµç¨‹")
    print("=" * 80)
    print(f"  æ•°æ®ç›®å½•: {gz_directory}")
    print(f"  ç›®æ ‡æœºå™¨: {machine_id}")
    print(f"  ä¿ç•™citation_raw: {'æ˜¯' if keep_citation_raw else 'å¦ï¼ˆè°¨æ…ï¼ï¼‰'}")
    print("=" * 80)
    
    overall_start = time.time()
    
    # è¿æ¥æ•°æ®åº“
    db_config = get_db_config(machine_id)
    print(f"\nè¿æ¥åˆ°æ•°æ®åº“ [{machine_id}: {db_config['database']}:{db_config['port']}]...")
    conn = psycopg2.connect(**db_config)
    cursor = conn.cursor()
    print("  âœ“ è¿æ¥æˆåŠŸ")
    
    try:
        only_flags = [only_import, only_index, only_stage3, only_stage4, only_stage5]
        if sum(1 for flag in only_flags if flag) > 1:
            raise ValueError("ä»…æ‰§è¡Œæ¨¡å¼å‚æ•°ï¼ˆ--only-*ï¼‰åªèƒ½æŒ‡å®šä¸€ä¸ª")
        
        # å¦‚æœä»…æ‰§è¡Œå•ç‹¬é˜¶æ®µï¼Œä¼˜å…ˆå¤„ç†
        if only_stage3:
            print("\nã€ä»…é˜¶æ®µ3æ¨¡å¼ã€‘æ‰§è¡Œreferencesç¼“å­˜æ„å»º...")
            build_references(cursor, conn, force=force_stage3)
            conn.commit()
            print("\n" + "=" * 80)
            print("ã€ä»…é˜¶æ®µ3æ¨¡å¼ã€‘å®Œæˆ")
            print("=" * 80)
            return
        
        if only_stage4:
            print("\nã€ä»…é˜¶æ®µ4æ¨¡å¼ã€‘æ‰§è¡Œcitationsç¼“å­˜æ„å»º...")
            build_citations(cursor, conn, force=force_stage4)
            conn.commit()
            print("\n" + "=" * 80)
            print("ã€ä»…é˜¶æ®µ4æ¨¡å¼ã€‘å®Œæˆ")
            print("=" * 80)
            return
        
        if only_stage5:
            print("\nã€ä»…é˜¶æ®µ5æ¨¡å¼ã€‘ç›´æ¥æ›´æ–°temp_importè¡¨...")
            update_temp_import(cursor, conn)
            conn.commit()
            if not skip_cleanup:
                cleanup(cursor, conn, keep_citation_raw)
            print("\n" + "=" * 80)
            print("ã€ä»…é˜¶æ®µ5æ¨¡å¼ã€‘å®Œæˆ")
            print("=" * 80)
            return
        
        # å¦‚æœåªå»ºç´¢å¼•ï¼Œç›´æ¥è·³åˆ°é˜¶æ®µ2
        if only_index:
            print("\nã€ä»…å»ºç´¢å¼•æ¨¡å¼ã€‘è·³è¿‡é˜¶æ®µ0å’Œ1ï¼Œç›´æ¥æ‰§è¡Œé˜¶æ®µ2...")
            create_indexes(cursor, conn)
            print("\n" + "=" * 80)
            print("ã€ä»…å»ºç´¢å¼•æ¨¡å¼ã€‘é˜¶æ®µ2å®Œæˆï¼Œåœæ­¢æ‰§è¡Œ")
            print("=" * 80)
            print("  ä¸‹ä¸€æ­¥è¯·æ‰§è¡Œï¼š")
            print("  python batch_update/import_citations.py <ç›®å½•> --skip-import --skip-index")
            print("=" * 80)
            return
        
        # å¦‚æœè·³è¿‡å¯¼å…¥å’Œç´¢å¼•ï¼Œç›´æ¥è·³åˆ°é˜¶æ®µ3
        if skip_import and skip_index:
            print("\nã€èšåˆæ›´æ–°æ¨¡å¼ã€‘è·³è¿‡é˜¶æ®µ0ã€1ã€2ï¼Œç›´æ¥æ‰§è¡Œé˜¶æ®µ3-5...")
        else:
            # é˜¶æ®µ0ï¼šåˆ›å»ºè¡¨
            create_citation_raw_table(cursor, truncate, no_count)
            conn.commit()
            
            # é˜¶æ®µ1ï¼šå¯¼å…¥æ•°æ®ï¼ˆå¯è·³è¿‡ï¼‰
            if skip_import:
                print("\nã€é˜¶æ®µ1ã€‘å¯¼å…¥citationsæ•°æ®...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-import æŒ‡å®šï¼‰")
            else:
                import_citations_gz(gz_directory, cursor, conn)
            
            # å¦‚æœåªå¯¼å…¥ï¼Œåˆ°æ­¤ç»“æŸ
            if only_import:
                print("\n" + "=" * 80)
                print("ã€ä»…å¯¼å…¥æ¨¡å¼ã€‘é˜¶æ®µ1å®Œæˆï¼Œåœæ­¢æ‰§è¡Œ")
                print("=" * 80)
                print("  ä¸‹ä¸€æ­¥è¯·æ‰§è¡Œï¼š")
                print("  python batch_update/import_citations.py <ç›®å½•> --only-index")
                print("=" * 80)
                return
            
            # é˜¶æ®µ2ï¼šåˆ›å»ºç´¢å¼•ï¼ˆå¯è·³è¿‡ï¼‰
            if skip_index:
                print("\nã€é˜¶æ®µ2ã€‘åˆ›å»ºç´¢å¼•...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-index æŒ‡å®šï¼‰")
            else:
                create_indexes(cursor, conn)
        
        # é˜¶æ®µ3ï¼šæ„é€ references
        if skip_stage3:
            print("\nã€é˜¶æ®µ3ã€‘æ„é€ referencesæ•°æ®...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-stage3 æŒ‡å®šï¼‰")
        else:
            build_references(cursor, conn, force=force_stage3)
        
        # é˜¶æ®µ4ï¼šæ„é€ citations
        if skip_stage4:
            print("\nã€é˜¶æ®µ4ã€‘æ„é€ citationsæ•°æ®...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-stage4 æŒ‡å®šï¼‰")
        else:
            build_citations(cursor, conn, force=force_stage4)
        
        # é˜¶æ®µ5ï¼šæ›´æ–°temp_import
        if skip_stage5:
            print("\nã€é˜¶æ®µ5ã€‘å¡«å……temp_importè¡¨...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-stage5 æŒ‡å®šï¼‰")
        else:
            update_temp_import(cursor, conn)
        
        # é˜¶æ®µ6ï¼šæ¸…ç†
        if not skip_cleanup:
            cleanup(cursor, conn, keep_citation_raw)
        else:
            print("\nã€é˜¶æ®µ6ã€‘æ¸…ç†æç¤º...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-cleanup æŒ‡å®šï¼‰")
        
        # æ€»ç»“
        total_time = time.time() - overall_start
        print("\n" + "=" * 80)
        print("ã€å¤„ç†å®Œæˆã€‘")
        print("=" * 80)
        print(f"  æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time:.2f}ç§’)")
        print("=" * 80)
        
        log_performance("å®Œæ•´æµç¨‹å®Œæˆ", total_time_min=f"{total_time/60:.1f}")
        
    except Exception as e:
        print(f"\nâœ— å¤„ç†å¤±è´¥: {e}")
        conn.rollback()
        raise
    
    finally:
        cursor.close()
        conn.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="é«˜æ€§èƒ½å¯¼å…¥citationsæ•°æ®å¹¶æ›´æ–°temp_importè¡¨ï¼ˆé»˜è®¤ä¿ç•™citation_rawï¼Œé˜²æ­¢è¯¯åˆ ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´æµç¨‹ï¼ˆé»˜è®¤machine0ï¼Œè‡ªåŠ¨ä¿ç•™citation_rawï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations
  
  # åˆ†æ­¥æ‰§è¡Œï¼ˆæ¨èï¼‰ï¼š
  # æ­¥éª¤1ï¼šä»…å¯¼å…¥æ•°æ®
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-import
  
  # æ­¥éª¤2ï¼šä»…å»ºç´¢å¼•
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-index
  
  # æ­¥éª¤3ï¼šæ„é€ ç¼“å­˜
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-stage3
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-stage4
  
  # æ­¥éª¤4ï¼šæ‰§è¡Œèšåˆå’Œæ›´æ–°ï¼ˆé˜¶æ®µ3-5ï¼Œå¯è·³è¿‡å·²å®Œæˆçš„é˜¶æ®µï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --skip-import --skip-index --skip-stage3 --skip-stage4
  
  # è·³è¿‡å¯¼å…¥ï¼Œç›´æ¥æ›´æ–°ï¼ˆç´¢å¼•å·²å­˜åœ¨æ—¶ä½¿ç”¨ï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --skip-import --no-count
  
  # æŒ‡å®šå…¶ä»–æœºå™¨
  python batch_update/import_citations.py D:\\gz_temp\\citations --machine machine2
  
  # æ¸…ç©ºè¡¨é‡æ–°å¯¼å…¥
  python batch_update/import_citations.py D:\\gz_temp\\citations --truncate

æ³¨æ„ï¼š
  - é»˜è®¤ä½¿ç”¨machine0æ•°æ®åº“ï¼Œè¯·ç”¨--machineæŒ‡å®šæ­£ç¡®çš„æ•°æ®åº“
  - citation_rawè¡¨é»˜è®¤ä¿ç•™ï¼ˆé˜²æ­¢è¯¯åˆ ï¼‰ï¼Œä¸å†æ”¯æŒè‡ªåŠ¨åˆ é™¤
  - æ•´ä¸ªæµç¨‹é¢„è®¡è€—æ—¶3-4å°æ—¶ï¼ˆ240GBæ•°æ®ï¼‰
  - ç¡®ä¿corpusid_mapping_titleè¡¨å·²å­˜åœ¨
  - ç¡®ä¿temp_importè¡¨æœ‰æ•°æ®ï¼ˆå¦åˆ™æ›´æ–°0æ¡ï¼‰
  - å»ºè®®åˆ†æ­¥æ‰§è¡Œï¼šå…ˆå¯¼å…¥ã€å†å»ºç´¢å¼•ã€æ„å»ºç¼“å­˜ï¼Œæœ€åèšåˆæ›´æ–°
        """
    )
    
    parser.add_argument("gz_directory", help="åŒ…å«citations gzæ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--machine", default="machine0", help="ç›®æ ‡æœºå™¨ (é»˜è®¤: machine0)")
    parser.add_argument("--truncate", action="store_true", help="æ¸…ç©ºcitation_rawè¡¨é‡æ–°å¯¼å…¥")
    parser.add_argument("--skip-import", action="store_true", help="è·³è¿‡é˜¶æ®µ1å¯¼å…¥")
    parser.add_argument("--skip-index", action="store_true", help="è·³è¿‡é˜¶æ®µ2å»ºç´¢å¼•")
    parser.add_argument("--skip-stage3", action="store_true", help="è·³è¿‡é˜¶æ®µ3ï¼ˆæ„é€ referencesç¼“å­˜ï¼‰")
    parser.add_argument("--skip-stage4", action="store_true", help="è·³è¿‡é˜¶æ®µ4ï¼ˆæ„é€ citationsç¼“å­˜ï¼‰")
    parser.add_argument("--skip-stage5", action="store_true", help="è·³è¿‡é˜¶æ®µ5ï¼ˆå¡«å……temp_importï¼‰")
    parser.add_argument("--only-import", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ0-1ï¼ˆå¯¼å…¥ï¼‰ï¼Œç„¶ååœæ­¢")
    parser.add_argument("--only-index", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ2ï¼ˆå»ºç´¢å¼•ï¼‰ï¼Œç„¶ååœæ­¢")
    parser.add_argument("--only-stage3", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ3ï¼ˆæ„é€ referencesç¼“å­˜ï¼‰")
    parser.add_argument("--only-stage4", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ4ï¼ˆæ„é€ citationsç¼“å­˜ï¼‰")
    parser.add_argument("--only-stage5", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ5ï¼ˆå¡«å……temp_importè¡¨ï¼‰")
    parser.add_argument("--force-stage3", action="store_true", help="å¼ºåˆ¶é‡å»ºé˜¶æ®µ3ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰")
    parser.add_argument("--force-stage4", action="store_true", help="å¼ºåˆ¶é‡å»ºé˜¶æ®µ4ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰")
    parser.add_argument("--no-count", action="store_true", help="é˜¶æ®µ0è·³è¿‡COUNT(*)ï¼Œé¿å…åœ¨è¶…å¤§è¡¨ä¸Šå¡ä½")
    parser.add_argument("--skip-cleanup", action="store_true", help="è·³è¿‡é˜¶æ®µ6æ¸…ç†æç¤ºè¾“å‡º")
    parser.add_argument("--keep-raw", action="store_true", help="ï¼ˆå·²åºŸå¼ƒï¼šç°åœ¨é»˜è®¤ä¿ç•™citation_rawï¼‰")
    
    args = parser.parse_args()
    
    gz_dir = Path(args.gz_directory)
    if not gz_dir.is_dir():
        print(f"é”™è¯¯: {args.gz_directory} ä¸æ˜¯æœ‰æ•ˆçš„ç›®å½•")
        sys.exit(1)
    
    run_full_pipeline(
        args.gz_directory,
        args.machine,
        args.keep_raw,
        args.truncate,
        args.skip_import,
        args.skip_index,
        args.skip_stage3,
        args.skip_stage4,
        args.skip_stage5,
        args.only_import,
        args.only_index,
        args.only_stage3,
        args.only_stage4,
        args.only_stage5,
        args.force_stage3,
        args.force_stage4,
        args.no_count,
        args.skip_cleanup,
    )

