"""
é«˜æ€§èƒ½å¯¼å…¥citationsæ•°æ®å¹¶æ›´æ–°temp_importè¡¨

æ ¸å¿ƒæµç¨‹ï¼ˆ6ä¸ªç‹¬ç«‹é˜¶æ®µï¼‰ï¼š
ã€é˜¶æ®µ0ã€‘åˆ›å»ºcitation_rawè¡¨
ã€é˜¶æ®µ1ã€‘å¯¼å…¥gzæ–‡ä»¶åˆ°citation_rawè¡¨ï¼ˆcitingcorpusid, citedcorpusidï¼‰
ã€é˜¶æ®µ2ã€‘åˆ›å»ºç´¢å¼•ï¼ˆidx_citation_citing, idx_citation_citedï¼‰
ã€é˜¶æ®µ3ã€‘æ„é€ referencesç¼“å­˜ï¼ˆtemp_references: corpusid -> array[citedcorpusid]ï¼‰
ã€é˜¶æ®µ4ã€‘æ„é€ citationsç¼“å­˜ï¼ˆtemp_citations: corpusid -> array[citingcorpusid]ï¼‰
ã€é˜¶æ®µ5ã€‘å¡«å……temp_importè¡¨ï¼ˆä»ç¼“å­˜è¡¨è¯»å–æ•°æ®ï¼Œæ›´æ–°citations/referenceså­—æ®µï¼‰
ã€é˜¶æ®µ6ã€‘æ¸…ç†æç¤º

æ¯ä¸ªé˜¶æ®µéƒ½å¯ç‹¬ç«‹æ‰§è¡Œï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ï¼š
- --only-import:  ä»…æ‰§è¡Œé˜¶æ®µ0-1
- --only-index:   ä»…æ‰§è¡Œé˜¶æ®µ2
- --only-stage3:  ä»…æ‰§è¡Œé˜¶æ®µ3
- --only-stage4:  ä»…æ‰§è¡Œé˜¶æ®µ4
- --only-stage5:  ä»…æ‰§è¡Œé˜¶æ®µ5
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import gzip
import orjson
import psycopg2
from datetime import datetime
import time
from tqdm import tqdm
from db_config import get_db_config

TEMP_TABLE = "temp_import"
CITATION_RAW_TABLE = "citation_raw"
RUNNING_LOG = Path(__file__).parent.parent / "logs" / "running.log"


def log_performance(stage, **metrics):
    """è®°å½•æ€§èƒ½æ—¥å¿—"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    metrics_str = " | ".join([f"{k}={v}" for k, v in metrics.items()])
    log_line = f"[{timestamp}] {stage} | {metrics_str}\n"
    with open(RUNNING_LOG, 'a', encoding='utf-8') as f:
        f.write(log_line)
    print(f"  {stage}: {metrics_str}")


class CopyStream:
    """å°†è¡Œè¿­ä»£å™¨åŒ…è£…ä¸ºpsycopg2 copy_expertå¯æ¶ˆè´¹çš„æµ"""

    def __init__(self, iterator, chunk_size=65536):
        self.iterator = iterator
        self.chunk_size = chunk_size
        self._buffer = bytearray()
        self._exhausted = False

    def readable(self):
        return True

    def read(self, size=-1):
        if size == -1:
            chunks = [bytes(self._buffer)]
            self._buffer.clear()
            for chunk in self.iterator:
                chunks.append(chunk)
            self._exhausted = True
            return b''.join(chunks)

        while len(self._buffer) < max(size, self.chunk_size) and not self._exhausted:
            try:
                self._buffer.extend(next(self.iterator))
            except StopIteration:
                self._exhausted = True
                break

        if not self._buffer:
            return b''

        if size >= len(self._buffer):
            data = bytes(self._buffer)
            self._buffer.clear()
            return data

        data = bytes(self._buffer[:size])
        del self._buffer[:size]
        return data


def create_citation_raw_table(cursor, truncate=False, no_count=False):
    """åˆ›å»ºcitation_rawæ˜ å°„è¡¨ï¼ˆä¸åˆ›å»ºç´¢å¼•ï¼‰"""
    print("\nã€é˜¶æ®µ0ã€‘åˆ›å»ºæ˜ å°„è¡¨...")
    
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {CITATION_RAW_TABLE} (
            citingcorpusid BIGINT NOT NULL,
            citedcorpusid BIGINT NOT NULL
        ) WITH (autovacuum_enabled = false)
    """)
    
    # å¿«é€Ÿå­˜åœ¨æ€§æ£€æŸ¥ï¼Œé¿å…å¯¹è¶…å¤§è¡¨åšå…¨è¡¨COUNT
    cursor.execute("SELECT to_regclass(%s) IS NOT NULL", (CITATION_RAW_TABLE,))
    exists = cursor.fetchone()[0]
    if not exists:
        print(f"  âœ“ è¡¨åˆ›å»ºæˆåŠŸ")
        return

    if no_count:
        # è·³è¿‡COUNT(*)ï¼Œç›´æ¥ç»™å‡ºæç¤º
        if truncate:
            print(f"  æ­£åœ¨æ¸…ç©ºè¡¨ï¼ˆè·³è¿‡è®¡æ•°ï¼‰...")
            cursor.execute(f"TRUNCATE TABLE {CITATION_RAW_TABLE}")
            print("  âœ“ è¡¨å·²æ¸…ç©º")
        else:
            print(f"  â“˜ è¡¨å·²å­˜åœ¨ï¼ˆè·³è¿‡è®¡æ•°ï¼‰ï¼Œå°†ç»§ç»­åç»­é˜¶æ®µ")
        return

    # æ­£å¸¸è·¯å¾„ï¼šåšä¸€æ¬¡COUNTï¼ˆè¡¨å¾ˆå¤§æ—¶å¯èƒ½è¾ƒæ…¢ï¼‰
    cursor.execute(f"SELECT COUNT(*) FROM {CITATION_RAW_TABLE}")
    count = cursor.fetchone()[0]
    if count > 0:
        if truncate:
            print(f"  æ­£åœ¨æ¸…ç©ºè¡¨ï¼ˆ{count:,} æ¡è®°å½•ï¼‰...")
            cursor.execute(f"TRUNCATE TABLE {CITATION_RAW_TABLE}")
            print("  âœ“ è¡¨å·²æ¸…ç©º")
        else:
            print(f"  âš ï¸  è¡¨å·²å­˜åœ¨ä¸”æœ‰ {count:,} æ¡è®°å½•ï¼Œå°†ç»§ç»­è¿½åŠ æ•°æ®")
    else:
        print(f"  âœ“ è¡¨åˆ›å»ºæˆåŠŸ")


def import_citations_gz(gz_directory, cursor, conn):
    """é˜¶æ®µ1ï¼šå¯¼å…¥æ‰€æœ‰citations gzæ–‡ä»¶åˆ°citation_rawè¡¨"""
    print("\nã€é˜¶æ®µ1ã€‘å¯¼å…¥citationsæ•°æ®...")
    
    gz_dir = Path(gz_directory)
    all_gz_files = sorted(gz_dir.glob("*.gz"))
    
    if not all_gz_files:
        raise FileNotFoundError(f"åœ¨ {gz_directory} ä¸­æ²¡æœ‰æ‰¾åˆ°gzæ–‡ä»¶")
    
    print(f"  å‘ç° {len(all_gz_files)} ä¸ªgzæ–‡ä»¶")
    
    # ä¼˜åŒ–æ•°æ®åº“é…ç½®
    cursor.execute("SET synchronous_commit = OFF")
    cursor.execute("SET work_mem = '512MB'")
    
    copy_sql = f"COPY {CITATION_RAW_TABLE} (citingcorpusid, citedcorpusid) FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t')"
    
    total_records = 0
    start_time = time.time()
    
    with tqdm(all_gz_files, desc="  å¯¼å…¥è¿›åº¦", unit="file") as pbar:
        for gz_file in pbar:
            file_start = time.time()
            file_count = 0
            
            def row_iterator():
                nonlocal file_count
                with gzip.open(gz_file, 'rt', encoding='utf-8', errors='replace') as f:
                    for line in f:
                        try:
                            data = orjson.loads(line.strip())
                            citing = data.get('citingcorpusid')
                            cited = data.get('citedcorpusid')
                            
                            if citing is not None and cited is not None:
                                file_count += 1
                                yield f"{citing}\t{cited}\n".encode('utf-8')
                        except Exception:
                            continue
            
            try:
                cursor.copy_expert(copy_sql, CopyStream(row_iterator()))
                conn.commit()
                
                total_records += file_count
                file_time = time.time() - file_start
                
                pbar.set_postfix_str(
                    f"å½“å‰: {file_count:,}æ¡/{file_time:.1f}ç§’ | "
                    f"æ€»è®¡: {total_records:,}æ¡"
                )
                
            except Exception as e:
                print(f"\n  âœ— æ–‡ä»¶ {gz_file.name} å¯¼å…¥å¤±è´¥: {e}")
                conn.rollback()
                continue
    
    total_time = time.time() - start_time
    speed = total_records / total_time if total_time > 0 else 0
    
    log_performance(
        "é˜¶æ®µ1-å¯¼å…¥å®Œæˆ",
        files=len(all_gz_files),
        records=f"{total_records:,}",
        time_sec=f"{total_time:.2f}",
        speed_per_sec=f"{speed:.0f}"
    )


def create_indexes(cursor, conn):
    """é˜¶æ®µ2ï¼šåˆ›å»ºç´¢å¼•ï¼ˆæè‡´ä¼˜åŒ–ç‰ˆï¼‰"""
    print("\nã€é˜¶æ®µ2ã€‘åˆ›å»ºç´¢å¼•...")
    
    start_time = time.time()
    
    # æè‡´ä¼˜åŒ–å‚æ•°
    cursor.execute("SET maintenance_work_mem = '8GB'")  # æå‡åˆ°8GB
    cursor.execute("SET max_parallel_maintenance_workers = 8")  # å¹¶è¡Œæ„å»º
    cursor.execute("SET max_parallel_workers = 16")  # å…¨å±€å¹¶è¡Œä¸Šé™
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²å­˜åœ¨
    cursor.execute("""
        SELECT COUNT(*) FROM pg_indexes 
        WHERE tablename = %s AND indexname IN ('idx_citation_citing', 'idx_citation_cited')
    """, (CITATION_RAW_TABLE,))
    existing_count = cursor.fetchone()[0]
    
    if existing_count == 2:
        print("  âœ“ ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
    elif existing_count == 1:
        print("  âš ï¸  ä»…æœ‰ä¸€ä¸ªç´¢å¼•å­˜åœ¨ï¼Œè¡¥å»ºç¼ºå¤±ç´¢å¼•...")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_citation_citing ON {CITATION_RAW_TABLE} (citingcorpusid)")
        cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_citation_cited ON {CITATION_RAW_TABLE} (citedcorpusid)")
    else:
        print("  åˆ›å»ºcitingcorpusidç´¢å¼•ï¼ˆå¹¶è¡Œæ„å»ºä¸­ï¼‰...")
        cursor.execute(f"CREATE INDEX idx_citation_citing ON {CITATION_RAW_TABLE} (citingcorpusid)")
        
        print("  åˆ›å»ºcitedcorpusidç´¢å¼•ï¼ˆå¹¶è¡Œæ„å»ºä¸­ï¼‰...")
        cursor.execute(f"CREATE INDEX idx_citation_cited ON {CITATION_RAW_TABLE} (citedcorpusid)")
    
    print("  æ”¶é›†ç»Ÿè®¡ä¿¡æ¯...")
    cursor.execute(f"ANALYZE {CITATION_RAW_TABLE}")
    
    conn.commit()
    
    elapsed = time.time() - start_time
    log_performance("é˜¶æ®µ2-ç´¢å¼•åˆ›å»º", time_sec=f"{elapsed:.2f}")


def build_references(cursor, conn, force=False):
    """é˜¶æ®µ3ï¼šæ„é€ referencesç¼“å­˜ï¼ˆcitingcorpusid -> array[citedcorpusid]ï¼‰"""
    print("\nã€é˜¶æ®µ3ã€‘æ„é€ referencesç¼“å­˜...")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    cursor.execute("SELECT to_regclass('temp_references')")
    exists = cursor.fetchone()[0]
    if exists and not force:
        cursor.execute("SELECT COUNT(*) FROM temp_references")
        count = cursor.fetchone()[0]
        print(f"  â“˜ temp_references å·²å­˜åœ¨ï¼ˆ{count:,}æ¡ï¼‰ï¼Œè·³è¿‡é‡å»º")
        print(f"     ä½¿ç”¨ --force-stage3 å¯å¼ºåˆ¶é‡å»º")
        return
    
    start_time = time.time()
    
    # ä¼˜åŒ–æ•°æ®åº“é…ç½®
    print("  ä¼˜åŒ–æ•°æ®åº“é…ç½®...")
    cursor.execute("SET work_mem = '8GB'")
    cursor.execute("SET temp_buffers = '2GB'")
    cursor.execute("SET hash_mem_multiplier = 2.0")
    
    # æ„å»ºç¼“å­˜è¡¨ï¼ˆä½¿ç”¨array_aggï¼Œæ¯”json_aggå¿«3-5å€ï¼‰
    print("  èšåˆæ•°æ®ï¼ˆcitingcorpusid -> array[citedcorpusid]ï¼‰...")
    cursor.execute("DROP TABLE IF EXISTS temp_references")
    cursor.execute(f"""
        CREATE UNLOGGED TABLE temp_references AS
        SELECT 
            citingcorpusid AS corpusid,
            array_agg(citedcorpusid) AS ref_ids
        FROM {CITATION_RAW_TABLE}
        GROUP BY citingcorpusid
    """)
    
    # åˆ›å»ºç´¢å¼•
    print("  åˆ›å»ºç´¢å¼•...")
    cursor.execute("SET maintenance_work_mem = '4GB'")
    cursor.execute("SET max_parallel_maintenance_workers = 8")
    cursor.execute("CREATE INDEX idx_temp_references_corpusid ON temp_references (corpusid)")
    
    # ç»Ÿè®¡ç»“æœ
    cursor.execute("SELECT COUNT(*) FROM temp_references")
    count = cursor.fetchone()[0]
    conn.commit()
    
    elapsed = time.time() - start_time
    print(f"\n  âœ“ å®Œæˆï¼š{count:,} æ¡è®°å½•ï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
    log_performance("é˜¶æ®µ3-referencesæ„é€ ", records=f"{count:,}", time_sec=f"{elapsed:.2f}")


def build_citations(cursor, conn, force=False):
    """é˜¶æ®µ4ï¼šæ„é€ citationsç¼“å­˜ï¼ˆcitedcorpusid -> array[citingcorpusid]ï¼‰"""
    print("\nã€é˜¶æ®µ4ã€‘æ„é€ citationsç¼“å­˜...")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    cursor.execute("SELECT to_regclass('temp_citations')")
    exists = cursor.fetchone()[0]
    if exists and not force:
        cursor.execute("SELECT COUNT(*) FROM temp_citations")
        count = cursor.fetchone()[0]
        print(f"  â“˜ temp_citations å·²å­˜åœ¨ï¼ˆ{count:,}æ¡ï¼‰ï¼Œè·³è¿‡é‡å»º")
        print(f"     ä½¿ç”¨ --force-stage4 å¯å¼ºåˆ¶é‡å»º")
        return
    
    start_time = time.time()
    
    # ä¼˜åŒ–æ•°æ®åº“é…ç½®
    print("  ä¼˜åŒ–æ•°æ®åº“é…ç½®...")
    cursor.execute("SET work_mem = '8GB'")
    cursor.execute("SET temp_buffers = '2GB'")
    cursor.execute("SET hash_mem_multiplier = 2.0")
    
    # æ„å»ºç¼“å­˜è¡¨
    print("  èšåˆæ•°æ®ï¼ˆcitedcorpusid -> array[citingcorpusid]ï¼‰...")
    cursor.execute("DROP TABLE IF EXISTS temp_citations")
    cursor.execute(f"""
        CREATE UNLOGGED TABLE temp_citations AS
        SELECT 
            citedcorpusid AS corpusid,
            array_agg(citingcorpusid) AS cite_ids
        FROM {CITATION_RAW_TABLE}
        GROUP BY citedcorpusid
    """)
    
    # åˆ›å»ºç´¢å¼•
    print("  åˆ›å»ºç´¢å¼•...")
    cursor.execute("SET maintenance_work_mem = '4GB'")
    cursor.execute("SET max_parallel_maintenance_workers = 8")
    cursor.execute("CREATE INDEX idx_temp_citations_corpusid ON temp_citations (corpusid)")
    
    # ç»Ÿè®¡ç»“æœ
    cursor.execute("SELECT COUNT(*) FROM temp_citations")
    count = cursor.fetchone()[0]
    conn.commit()
    
    elapsed = time.time() - start_time
    print(f"\n  âœ“ å®Œæˆï¼š{count:,} æ¡è®°å½•ï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
    log_performance("é˜¶æ®µ4-citationsæ„é€ ", records=f"{count:,}", time_sec=f"{elapsed:.2f}")


def update_temp_import(cursor, conn):
    """é˜¶æ®µ5ï¼šå¡«å……temp_importè¡¨ï¼ˆä¸´æ—¶è¡¨èšåˆååˆ†æ‰¹INSERTï¼‰
    
    ç­–ç•¥ï¼š
    1. åœ¨ä¸´æ—¶è¡¨ä¸­èšåˆæ‰€æœ‰æ•°æ®ï¼ˆå¸¦titleçš„JSONæ ¼å¼ï¼ŒSQLå±‚é¢å®Œæˆï¼‰
    2. åˆ†æ‰¹INSERTåˆ°temp_importè¡¨ï¼ˆå®æ—¶æ˜¾ç¤ºè¿›åº¦ï¼‰
    
    ä¼˜ç‚¹ï¼š
    - æé€Ÿï¼šSQLå±‚é¢èšåˆï¼Œé¿å…Pythonå¾ªç¯
    - è¿›åº¦å¯è§ï¼šåˆ†æ‰¹æ’å…¥ï¼Œå®æ—¶æ˜¾ç¤ºè¿›åº¦
    - ç®€æ´ï¼šä¸åšå»é‡æ£€æŸ¥ï¼Œç›´æ¥æ’å…¥
    """
    print("\nã€é˜¶æ®µ5ã€‘å¡«å……temp_importè¡¨...")
    start_time = time.time()
    
    # æ­¥éª¤1: æ£€æŸ¥å‰ç½®æ¡ä»¶
    print("  æ­¥éª¤1: æ£€æŸ¥å‰ç½®æ¡ä»¶...")
    
    # æ£€æŸ¥ç¼“å­˜è¡¨æ˜¯å¦å­˜åœ¨
    for table_name in ("temp_references", "temp_citations"):
        cursor.execute("SELECT to_regclass(%s)", (table_name,))
        if not cursor.fetchone()[0]:
            raise RuntimeError(
                f"âŒ {table_name} ä¸å­˜åœ¨ã€‚\n"
                f"   è¯·å…ˆæ‰§è¡Œ: python batch_update/import_citations.py <ç›®å½•> "
                f"--only-stage{3 if table_name == 'temp_references' else 4}"
            )
    
    # ä½¿ç”¨ pg_class çš„ç»Ÿè®¡ä¿¡æ¯ä¼°ç®—è¡Œæ•°ï¼Œé¿å…åœ¨è¶…å¤§è¡¨ä¸Šæ‰§è¡Œ COUNT(*)
    cursor.execute(
        "SELECT reltuples::bigint FROM pg_class WHERE oid = 'temp_references'::regclass"
    )
    ref_estimate = cursor.fetchone()[0]
    cursor.execute(
        "SELECT reltuples::bigint FROM pg_class WHERE oid = 'temp_citations'::regclass"
    )
    cite_estimate = cursor.fetchone()[0]
    
    ref_msg = f"çº¦ {int(ref_estimate):,} æ¡" if ref_estimate is not None else "æœªçŸ¥"
    cite_msg = f"çº¦ {int(cite_estimate):,} æ¡" if cite_estimate is not None else "æœªçŸ¥"
    
    print(f"    âœ“ temp_references: {ref_msg}ï¼ˆä¼°ç®—å€¼ï¼‰")
    print(f"    âœ“ temp_citations: {cite_msg}ï¼ˆä¼°ç®—å€¼ï¼‰")
    
    # æ­¥éª¤2: ä¼˜åŒ–æ•°æ®åº“é…ç½®ï¼ˆå……åˆ†åˆ©ç”¨32GBå†…å­˜å’Œ8æ ¸CPUï¼‰
    print("\n  æ­¥éª¤2: ä¼˜åŒ–æ•°æ®åº“é…ç½®ï¼ˆå……åˆ†åˆ©ç”¨32GBå†…å­˜å’Œ8æ ¸CPUï¼‰...")
    cursor.execute("SET work_mem = '2GB'")  # å•ä¸ªæ“ä½œ2GBï¼ˆå¤§è§„æ¨¡UNIONå»é‡éœ€è¦ï¼‰
    cursor.execute("SET temp_buffers = '2GB'")  # ä¸´æ—¶ç¼“å†²åŒº2GB
    cursor.execute("SET maintenance_work_mem = '4GB'")  # ç»´æŠ¤æ“ä½œ4GB
    cursor.execute("SET synchronous_commit = OFF")  # å¼‚æ­¥æäº¤ï¼Œæå‡10å€å†™å…¥é€Ÿåº¦
    cursor.execute("SET statement_timeout = 0")  # ç¦ç”¨è¶…æ—¶ï¼ˆå¤§æ•°æ®UNIONæ“ä½œéœ€è¦é•¿æ—¶é—´ï¼‰
    cursor.execute("SET max_parallel_workers_per_gather = 4")  # å¹¶è¡ŒæŸ¥è¯¢ï¼š4ä¸ªworker
    cursor.execute("SET parallel_tuple_cost = 0.01")  # é™ä½å¹¶è¡Œæˆæœ¬ä¼°ç®—ï¼Œé¼“åŠ±å¹¶è¡Œ
    cursor.execute("SET enable_hashagg = ON")  # å¯ç”¨HashAgg
    cursor.execute("SET hash_mem_multiplier = 3.0")  # å“ˆå¸Œèšåˆå¯ç”¨3å€work_memï¼ˆ6GBï¼‰
    cursor.execute("SET effective_cache_size = '16GB'")  # å‘Šè¯‰ä¼˜åŒ–å™¨å¯ç”¨ç¼“å­˜å¤§å°
    print("    âœ“ å®Œæˆï¼ˆé«˜æ€§èƒ½æ¨¡å¼ï¼š2GB work_mem + 6GB hash_mem + ç¦ç”¨è¶…æ—¶ï¼‰")
    
    # æ­¥éª¤3: åˆ›å»ºtitleæ˜ å°„ç¼“å­˜ï¼ˆç›´æ¥JOINï¼Œæ— éœ€é¢„å…ˆæå–ï¼‰
    print("\n  æ­¥éª¤3: åˆ›å»ºtitleæ˜ å°„ç¼“å­˜...")
    step3_start = time.time()
    
    # ç›´æ¥ä»corpusid_mapping_titleå¤åˆ¶æ‰€æœ‰æ•°æ®ï¼ˆæ— è¿‡æ»¤ï¼Œæé€Ÿï¼‰
    cursor.execute("DROP TABLE IF EXISTS temp_title_cache")
    cursor.execute("""
        CREATE TEMP TABLE temp_title_cache AS
        SELECT corpusid, COALESCE(title, '') as title
        FROM corpusid_mapping_title
    """)
    
    # åˆ›å»ºç´¢å¼•ç”¨äºåç»­JOIN
    cursor.execute("CREATE INDEX ON temp_title_cache (corpusid)")
    
    cursor.execute("SELECT reltuples::bigint FROM pg_class WHERE oid = 'temp_title_cache'::regclass")
    cache_count = cursor.fetchone()[0]
    print(f"    âœ“ ç¼“å­˜çº¦{int(cache_count):,}æ¡titleï¼ˆ{time.time() - step3_start:.1f}ç§’ï¼‰")
    
    # æ­¥éª¤4: åˆ›å»ºå»é‡çš„corpusidåˆ—è¡¨ï¼ˆä½¿ç”¨UNIONè‡ªåŠ¨å»é‡ï¼Œæ¯”UNION ALL+GROUP BYå¿«ï¼‰
    print("\n  æ­¥éª¤4: åˆ›å»ºcorpusidå»é‡åˆ—è¡¨ï¼ˆä½¿ç”¨UNIONè‡ªåŠ¨å»é‡ï¼‰...")
    step4_start = time.time()
    
    cursor.execute("DROP TABLE IF EXISTS temp_unique_corpusids")
    cursor.execute("""
        CREATE UNLOGGED TABLE temp_unique_corpusids AS
        SELECT corpusid FROM temp_references
        UNION
        SELECT corpusid FROM temp_citations
    """)
    
    # åˆ›å»ºç´¢å¼•åŠ é€Ÿåç»­æ‰¹é‡è¯»å–
    cursor.execute("CREATE INDEX idx_unique_corpusids ON temp_unique_corpusids (corpusid)")
    cursor.execute("ANALYZE temp_unique_corpusids")
    
    cursor.execute("SELECT COUNT(*) FROM temp_unique_corpusids")
    unique_count = cursor.fetchone()[0]
    print(f"    âœ“ å»é‡å®Œæˆ: {unique_count:,} ä¸ªä¸é‡å¤corpusidï¼ˆ{time.time() - step4_start:.1f}ç§’ï¼‰")
    
    # æ­¥éª¤5: ä¸´æ—¶è¡¨JOIN + COPYæ‰¹é‡æ’å…¥ï¼ˆæé€Ÿä¼˜åŒ–ï¼Œå……åˆ†åˆ©ç”¨32GBå†…å­˜ï¼‰
    print("\n  æ­¥éª¤5: åˆ†æ‰¹COPYåˆ°temp_importè¡¨ï¼ˆä¸´æ—¶è¡¨JOINä¼˜åŒ–ï¼‰...")
    step5_start = time.time()
    
    batch_size = 50000  # å¤§æ‰¹æ¬¡ï¼š5ä¸‡æ¡/æ‰¹ï¼ˆå……åˆ†åˆ©ç”¨å†…å­˜ï¼‰
    total_inserted = 0
    batch_num = 0
    commit_interval = 5  # æ¯5æ‰¹æäº¤ä¸€æ¬¡
    failed_batches = []
    
    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰corpusidåˆ°å†…å­˜ï¼ˆ6700ä¸‡*8å­—èŠ‚=536MBï¼Œå†…å­˜å……è¶³ï¼‰
    print("    åŠ è½½corpusidåˆ—è¡¨åˆ°å†…å­˜...")
    load_start = time.time()
    cursor.execute("SELECT corpusid FROM temp_unique_corpusids ORDER BY corpusid")
    all_corpusids = cursor.fetchall()
    print(f"    âœ“ å·²åŠ è½½ {len(all_corpusids):,} æ¡corpusidï¼ˆ{time.time() - load_start:.1f}ç§’ï¼‰")
    
    print(f"\n    é…ç½®: {batch_size:,}æ¡/æ‰¹ | æ¯{commit_interval}æ‰¹æäº¤ | INæŸ¥è¯¢+åˆ†æ‰¹title | å†…å­˜å®‰å…¨")
    
    # å‡†å¤‡COPYå‘½ä»¤
    copy_sql = f"COPY {TEMP_TABLE} (corpusid, \"references\", \"citations\", is_done) FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t')"
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(all_corpusids), batch_size):
        corpusids = all_corpusids[i:i+batch_size]
        batch_num += 1
        batch_start = time.time()
        
        try:
            query_start = time.time()
            
            # 1. æ„é€ INå­å¥ï¼ˆå¯¹5ä¸‡æ¡IDï¼ŒINæ¯”ä¸´æ—¶è¡¨JOINæ›´å¿«ï¼‰
            id_list = ','.join(str(cid) for cid, in corpusids)
            
            # 2. æŸ¥è¯¢referencesï¼ˆä½¿ç”¨INï¼Œé¿å…ä¸´æ—¶è¡¨overheadï¼‰
            cursor.execute(f"""
                SELECT corpusid, ref_ids
                FROM temp_references
                WHERE corpusid IN ({id_list})
            """)
            ref_batch = {row[0]: row[1] for row in cursor.fetchall()}
            
            # 3. æŸ¥è¯¢citations
            cursor.execute(f"""
                SELECT corpusid, cite_ids
                FROM temp_citations
                WHERE corpusid IN ({id_list})
            """)
            cite_batch = {row[0]: row[1] for row in cursor.fetchall()}
            
            # 4. æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢titleçš„corpusid
            all_ids = set()
            for ref_ids in ref_batch.values():
                all_ids.update(ref_ids)
            for cite_ids in cite_batch.values():
                all_ids.update(cite_ids)
            
            # 5. æ‰¹é‡æŸ¥è¯¢titleï¼ˆä½¿ç”¨INï¼‰
            if all_ids:
                # é™åˆ¶titleæŸ¥è¯¢æ‰¹æ¬¡å¤§å°ï¼ˆé¿å…INå­å¥è¿‡å¤§ï¼‰
                title_map = {}
                title_ids_list = list(all_ids)
                title_batch_size = 100000  # æ¯æ¬¡æŸ¥è¯¢10ä¸‡ä¸ªtitle
                
                for j in range(0, len(title_ids_list), title_batch_size):
                    title_ids_batch = title_ids_list[j:j+title_batch_size]
                    title_id_str = ','.join(str(tid) for tid in title_ids_batch)
                    cursor.execute(f"""
                        SELECT corpusid, title
                        FROM temp_title_cache
                        WHERE corpusid IN ({title_id_str})
                    """)
                    title_map.update({row[0]: row[1] for row in cursor.fetchall()})
            else:
                title_map = {}
            
            query_time = time.time() - query_start
            
            # 6. Pythonæ„é€ JSONå¹¶COPYæ’å…¥
            def data_stream():
                for cid, in corpusids:
                    # æ„é€ references JSON
                    ref_ids = ref_batch.get(cid, [])
                    if ref_ids:
                        ref_list = [{"corpusid": rid, "title": title_map.get(rid, "")} for rid in ref_ids]
                        ref_json = orjson.dumps(ref_list).decode('utf-8')
                    else:
                        ref_json = '[]'
                    
                    # æ„é€ citations JSON
                    cite_ids = cite_batch.get(cid, [])
                    if cite_ids:
                        cite_list = [{"corpusid": cid2, "title": title_map.get(cid2, "")} for cid2 in cite_ids]
                        cite_json = orjson.dumps(cite_list).decode('utf-8')
                    else:
                        cite_json = '[]'
                    
                    yield f"{cid}\t{ref_json}\t{cite_json}\tf\n".encode('utf-8')
            
            copy_start = time.time()
            cursor.copy_expert(copy_sql, CopyStream(data_stream()))
            copy_time = time.time() - copy_start
            
            # 7. æ¸…ç†æ‰¹æ¬¡æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
            ref_batch.clear()
            cite_batch.clear()
            title_map.clear()
            all_ids.clear()
            
            inserted = len(corpusids)
            total_inserted += inserted
            batch_time = time.time() - batch_start
            rate = inserted / batch_time if batch_time > 0 else 0
            
            # æ‰¹é‡æäº¤
            commit_start = time.time()
            if batch_num % commit_interval == 0 or i + batch_size >= len(all_corpusids):
                conn.commit()
                commit_marker = "âœ“"
            else:
                commit_marker = " "
            commit_time = time.time() - commit_start
            
            # è®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
            elapsed_step5 = time.time() - step5_start
            progress = (total_inserted / unique_count * 100) if unique_count > 0 else 0
            remaining_count = unique_count - total_inserted
            avg_rate = total_inserted / elapsed_step5 if elapsed_step5 > 0 else 0
            eta_seconds = remaining_count / avg_rate if avg_rate > 0 else 0
            
            # æ ¼å¼åŒ–ETA
            if eta_seconds > 3600:
                eta_str = f"{eta_seconds/3600:.1f}å°æ—¶"
            elif eta_seconds > 60:
                eta_str = f"{eta_seconds/60:.1f}åˆ†é’Ÿ"
            else:
                eta_str = f"{eta_seconds:.0f}ç§’"
            
            # æ¯æ‰¹æ˜¾ç¤ºè¿›åº¦
            if batch_num % 5 == 0 or batch_num == 1:
                print(f"    æ‰¹æ¬¡ #{batch_num}{commit_marker}: {inserted:,}æ¡ | {batch_time:.2f}ç§’ | {rate:.0f}æ¡/ç§’ | è¿›åº¦{progress:.1f}% | ETA: {eta_str}")
                print(f"        è¯¦ç»†: INæŸ¥è¯¢({query_time:.2f}s) + Python+COPY({copy_time:.2f}s) + æäº¤({commit_time:.3f}s)")
                
                # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                log_performance(
                    "é˜¶æ®µ5-æ‰¹æ¬¡è¿›åº¦",
                    batch=batch_num,
                    inserted=f"{total_inserted:,}/{unique_count:,}",
                    progress=f"{progress:.1f}%",
                    rate=f"{avg_rate:.0f}æ¡/ç§’",
                    eta=eta_str
                )
        
        except Exception as e:
            error_msg = str(e)[:200]
            print(f"    âœ— æ‰¹æ¬¡ #{batch_num} å¤±è´¥: {error_msg}")
            failed_batches.append((batch_num, error_msg))
            conn.rollback()
            
            if len(failed_batches) >= 3:
                recent_failures = [b for b in failed_batches if b[0] > batch_num - 5]
                if len(recent_failures) >= 3:
                    print(f"\n    âŒ è¿ç»­å¤±è´¥è¶…è¿‡3æ‰¹ï¼Œç»ˆæ­¢å¤„ç†")
                    break
            continue
    
    conn.commit()
    
    print(f"\n    âœ“ æ€»è®¡æ’å…¥: {total_inserted:,} æ¡ï¼ˆ{time.time() - step5_start:.1f}ç§’ï¼‰")
    
    # æ˜¾ç¤ºå¤±è´¥æ‰¹æ¬¡æ±‡æ€»ï¼ˆå¦‚æœæœ‰ï¼‰
    if failed_batches:
        print(f"    âš ï¸  å¤±è´¥æ‰¹æ¬¡: {len(failed_batches)}/{batch_num} ({len(failed_batches)/batch_num*100:.1f}%)")
        for batch_id, error in failed_batches[:5]:  # ä»…æ˜¾ç¤ºå‰5ä¸ª
            print(f"      #{batch_id}: {error}")
    
    # æ­¥éª¤6: æ¸…ç†ä¸´æ—¶è¡¨
    print("\n  æ­¥éª¤6: æ¸…ç†ä¸´æ—¶è¡¨...")
    cursor.execute("DROP TABLE IF EXISTS temp_title_cache")
    cursor.execute("DROP TABLE IF EXISTS temp_unique_corpusids")
    cursor.execute(f"ALTER TABLE {TEMP_TABLE} SET (autovacuum_enabled = true)")
    conn.commit()
    print("    âœ“ å®Œæˆ")
    
    # ç»Ÿè®¡ç»“æœ
    elapsed = time.time() - start_time
    avg_speed = total_inserted/elapsed if elapsed > 0 else 0
    print(f"\n  ã€é˜¶æ®µ5å®Œæˆã€‘{unique_count:,}æ¡ | {batch_num}æ‰¹ | {avg_speed:.0f}æ¡/ç§’ | {elapsed:.1f}ç§’")
    
    log_performance(
        "é˜¶æ®µ5-INSERTå®Œæˆ",
        unique_corpusids=f"{unique_count:,}",
        inserted=f"{total_inserted:,}",
        batches=batch_num,
        failed=len(failed_batches),
        success_rate=f"{(batch_num-len(failed_batches))/batch_num*100:.1f}%" if batch_num > 0 else "100%",
        time_sec=f"{elapsed:.2f}",
        avg_speed=f"{total_inserted/elapsed:.0f}" if elapsed > 0 else "0"
    )


def cleanup(cursor, conn, keep_citation_raw=True):
    """é˜¶æ®µ6ï¼šæ¸…ç†æç¤ºï¼ˆä¸è‡ªåŠ¨åˆ é™¤ä»»ä½•è¡¨ï¼Œç”±ç”¨æˆ·æ‰‹åŠ¨æ¸…ç†ï¼‰"""
    print("\nã€é˜¶æ®µ6ã€‘æ¸…ç†æç¤º...")
    
    # æ£€æŸ¥å„è¡¨æ˜¯å¦å­˜åœ¨åŠè®°å½•æ•°
    print("\n  å½“å‰è¡¨çŠ¶æ€ï¼š")
    
    # æ£€æŸ¥temp_references
    try:
        cursor.execute("SELECT COUNT(*) FROM temp_references")
        count = cursor.fetchone()[0]
        print(f"  - temp_references: {count:,} æ¡ï¼ˆç¼“å­˜è¡¨ï¼Œå¯å¤ç”¨ï¼‰")
    except:
        print(f"  - temp_references: ä¸å­˜åœ¨")
    
    # æ£€æŸ¥temp_citations
    try:
        cursor.execute("SELECT COUNT(*) FROM temp_citations")
        count = cursor.fetchone()[0]
        print(f"  - temp_citations: {count:,} æ¡ï¼ˆç¼“å­˜è¡¨ï¼Œå¯å¤ç”¨ï¼‰")
    except:
        print(f"  - temp_citations: ä¸å­˜åœ¨")
    
    # æ£€æŸ¥citation_raw
    try:
        cursor.execute(f"SELECT COUNT(*) FROM {CITATION_RAW_TABLE}")
        count = cursor.fetchone()[0]
        print(f"  - {CITATION_RAW_TABLE}: {count:,} æ¡ï¼ˆé‡è¦æ•°æ®ï¼‰")
    except:
        print(f"  - {CITATION_RAW_TABLE}: ä¸å­˜åœ¨")
    
    print("\n  â“˜ æ‰€æœ‰è¡¨å·²ä¿ç•™ï¼Œå¦‚éœ€æ‰‹åŠ¨æ¸…ç†ï¼Œè¯·æ‰§è¡Œï¼š")
    print(f"     -- æ¸…ç†ç¼“å­˜è¡¨ï¼ˆéœ€è¦æ—¶æ‰‹åŠ¨æ‰§è¡Œï¼‰")
    print(f"     DROP TABLE IF EXISTS temp_references;")
    print(f"     DROP TABLE IF EXISTS temp_citations;")
    print(f"     DROP TABLE IF EXISTS temp_title_cache;")
    print(f"")
    print(f"     -- æ¸…ç†citation_rawï¼ˆè°¨æ…ï¼14äº¿+æ¡æ•°æ®ï¼Œéœ€1-2å°æ—¶é‡å»ºï¼‰")
    print(f"     DROP TABLE IF EXISTS {CITATION_RAW_TABLE};")
    
    conn.commit()


def run_full_pipeline(
    gz_directory,
    machine_id='machine0',
    keep_citation_raw=True,
    truncate=False,
    skip_import=False,
    skip_index=False,
    skip_stage3=False,
    skip_stage4=False,
    skip_stage5=False,
    only_import=False,
    only_index=False,
    only_stage3=False,
    only_stage4=False,
    only_stage5=False,
    force_stage3=False,
    force_stage4=False,
    no_count=False,
    skip_cleanup=False,
):
    """æ‰§è¡Œå®Œæ•´çš„citationså¤„ç†æµç¨‹
    
    å‚æ•°:
        gz_directory: citations gzæ–‡ä»¶ç›®å½•
        machine_id: ç›®æ ‡æ•°æ®åº“ (é»˜è®¤: machine0)
        keep_citation_raw: ä¿ç•™citation_rawè¡¨ (é»˜è®¤: Trueï¼Œé˜²æ­¢è¯¯åˆ )
        truncate: æ¸…ç©ºcitation_rawé‡æ–°å¯¼å…¥ (é»˜è®¤: False)
        skip_import: è·³è¿‡å¯¼å…¥é˜¶æ®µ (é»˜è®¤: False)
        skip_index: è·³è¿‡å»ºç´¢å¼•é˜¶æ®µ (é»˜è®¤: False)
        skip_stage3: è·³è¿‡é˜¶æ®µ3ç¼“å­˜æ„å»º (é»˜è®¤: False)
        skip_stage4: è·³è¿‡é˜¶æ®µ4ç¼“å­˜æ„å»º (é»˜è®¤: False)
        skip_stage5: è·³è¿‡é˜¶æ®µ5æ›´æ–° (é»˜è®¤: False)
        only_import: ä»…æ‰§è¡Œé˜¶æ®µ0-1ï¼ˆå¯¼å…¥ï¼‰ï¼Œç„¶ååœæ­¢ (é»˜è®¤: False)
        only_index: ä»…æ‰§è¡Œé˜¶æ®µ2ï¼ˆå»ºç´¢å¼•ï¼‰ï¼Œç„¶ååœæ­¢ (é»˜è®¤: False)
        only_stage3: ä»…æ‰§è¡Œé˜¶æ®µ3ï¼ˆæ„é€ referencesç¼“å­˜ï¼‰ (é»˜è®¤: False)
        only_stage4: ä»…æ‰§è¡Œé˜¶æ®µ4ï¼ˆæ„é€ citationsç¼“å­˜ï¼‰ (é»˜è®¤: False)
        only_stage5: ä»…æ‰§è¡Œé˜¶æ®µ5ï¼ˆå¡«å……temp_importè¡¨ï¼‰ (é»˜è®¤: False)
        force_stage3: å¼ºåˆ¶é‡å»ºé˜¶æ®µ3ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰ (é»˜è®¤: False)
        force_stage4: å¼ºåˆ¶é‡å»ºé˜¶æ®µ4ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼‰ (é»˜è®¤: False)
        no_count: è·³è¿‡è¡¨è®¡æ•° (é»˜è®¤: False)
        skip_cleanup: è·³è¿‡é˜¶æ®µ6æ¸…ç†æç¤º (é»˜è®¤: False)
    """
    print("=" * 80)
    print("Citationsæ•°æ®é«˜æ€§èƒ½å¤„ç†æµç¨‹")
    print("=" * 80)
    print(f"  æ•°æ®ç›®å½•: {gz_directory}")
    print(f"  ç›®æ ‡æœºå™¨: {machine_id}")
    print(f"  ä¿ç•™citation_raw: {'æ˜¯' if keep_citation_raw else 'å¦ï¼ˆè°¨æ…ï¼ï¼‰'}")
    print("=" * 80)
    
    overall_start = time.time()
    
    # è¿æ¥æ•°æ®åº“
    db_config = get_db_config(machine_id)
    print(f"\nè¿æ¥åˆ°æ•°æ®åº“ [{machine_id}: {db_config['database']}:{db_config['port']}]...")
    conn = psycopg2.connect(**db_config)
    cursor = conn.cursor()
    print("  âœ“ è¿æ¥æˆåŠŸ")
    
    try:
        only_flags = [only_import, only_index, only_stage3, only_stage4, only_stage5]
        if sum(1 for flag in only_flags if flag) > 1:
            raise ValueError("ä»…æ‰§è¡Œæ¨¡å¼å‚æ•°ï¼ˆ--only-*ï¼‰åªèƒ½æŒ‡å®šä¸€ä¸ª")
        
        # å¦‚æœä»…æ‰§è¡Œå•ç‹¬é˜¶æ®µï¼Œä¼˜å…ˆå¤„ç†
        if only_stage3:
            print("\nã€ä»…é˜¶æ®µ3æ¨¡å¼ã€‘æ‰§è¡Œreferencesç¼“å­˜æ„å»º...")
            build_references(cursor, conn, force=force_stage3)
            conn.commit()
            print("\n" + "=" * 80)
            print("ã€ä»…é˜¶æ®µ3æ¨¡å¼ã€‘å®Œæˆ")
            print("=" * 80)
            return
        
        if only_stage4:
            print("\nã€ä»…é˜¶æ®µ4æ¨¡å¼ã€‘æ‰§è¡Œcitationsç¼“å­˜æ„å»º...")
            build_citations(cursor, conn, force=force_stage4)
            conn.commit()
            print("\n" + "=" * 80)
            print("ã€ä»…é˜¶æ®µ4æ¨¡å¼ã€‘å®Œæˆ")
            print("=" * 80)
            return
        
        if only_stage5:
            print("\nã€ä»…é˜¶æ®µ5æ¨¡å¼ã€‘ç›´æ¥æ›´æ–°temp_importè¡¨...")
            update_temp_import(cursor, conn)
            conn.commit()
            if not skip_cleanup:
                cleanup(cursor, conn, keep_citation_raw)
            print("\n" + "=" * 80)
            print("ã€ä»…é˜¶æ®µ5æ¨¡å¼ã€‘å®Œæˆ")
            print("=" * 80)
            return
        
        # å¦‚æœåªå»ºç´¢å¼•ï¼Œç›´æ¥è·³åˆ°é˜¶æ®µ2
        if only_index:
            print("\nã€ä»…å»ºç´¢å¼•æ¨¡å¼ã€‘è·³è¿‡é˜¶æ®µ0å’Œ1ï¼Œç›´æ¥æ‰§è¡Œé˜¶æ®µ2...")
            create_indexes(cursor, conn)
            print("\n" + "=" * 80)
            print("ã€ä»…å»ºç´¢å¼•æ¨¡å¼ã€‘é˜¶æ®µ2å®Œæˆï¼Œåœæ­¢æ‰§è¡Œ")
            print("=" * 80)
            print("  ä¸‹ä¸€æ­¥è¯·æ‰§è¡Œï¼š")
            print("  python batch_update/import_citations.py <ç›®å½•> --skip-import --skip-index")
            print("=" * 80)
            return
        
        # å¦‚æœä½¿ç”¨ --force-stage3 æˆ– --force-stage4ï¼Œè‡ªåŠ¨è·³è¿‡é˜¶æ®µ0ã€1ã€2
        # å› ä¸ºè¿™äº›å‚æ•°æ„å‘³ç€ç”¨æˆ·æƒ³ç›´æ¥åˆ©ç”¨å·²æœ‰çš„ citation_raw è¡¨
        if force_stage3 or force_stage4:
            print("\nã€å¿«é€Ÿç¼“å­˜é‡å»ºæ¨¡å¼ã€‘æ£€æµ‹åˆ° --force-stage3/4ï¼Œè‡ªåŠ¨è·³è¿‡é˜¶æ®µ0ã€1ã€2...")
            print("  å‡è®¾ citation_raw è¡¨å·²å­˜åœ¨å¹¶åŒ…å«æ•°æ®")
        
        # å¦‚æœè·³è¿‡å¯¼å…¥å’Œç´¢å¼•ï¼Œç›´æ¥è·³åˆ°é˜¶æ®µ3
        elif skip_import and skip_index:
            print("\nã€èšåˆæ›´æ–°æ¨¡å¼ã€‘è·³è¿‡é˜¶æ®µ0ã€1ã€2ï¼Œç›´æ¥æ‰§è¡Œé˜¶æ®µ3-5...")
        else:
            # é˜¶æ®µ0ï¼šåˆ›å»ºè¡¨
            create_citation_raw_table(cursor, truncate, no_count)
            conn.commit()
            
            # é˜¶æ®µ1ï¼šå¯¼å…¥æ•°æ®ï¼ˆå¯è·³è¿‡ï¼‰
            if skip_import:
                print("\nã€é˜¶æ®µ1ã€‘å¯¼å…¥citationsæ•°æ®...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-import æŒ‡å®šï¼‰")
            else:
                import_citations_gz(gz_directory, cursor, conn)
            
            # å¦‚æœåªå¯¼å…¥ï¼Œåˆ°æ­¤ç»“æŸ
            if only_import:
                print("\n" + "=" * 80)
                print("ã€ä»…å¯¼å…¥æ¨¡å¼ã€‘é˜¶æ®µ1å®Œæˆï¼Œåœæ­¢æ‰§è¡Œ")
                print("=" * 80)
                print("  ä¸‹ä¸€æ­¥è¯·æ‰§è¡Œï¼š")
                print("  python batch_update/import_citations.py <ç›®å½•> --only-index")
                print("=" * 80)
                return
            
            # é˜¶æ®µ2ï¼šåˆ›å»ºç´¢å¼•ï¼ˆå¯è·³è¿‡ï¼‰
            if skip_index:
                print("\nã€é˜¶æ®µ2ã€‘åˆ›å»ºç´¢å¼•...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-index æŒ‡å®šï¼‰")
            else:
                create_indexes(cursor, conn)
        
        # é˜¶æ®µ3ï¼šæ„é€ references
        if skip_stage3:
            print("\nã€é˜¶æ®µ3ã€‘æ„é€ referencesæ•°æ®...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-stage3 æŒ‡å®šï¼‰")
        else:
            build_references(cursor, conn, force=force_stage3)
        
        # é˜¶æ®µ4ï¼šæ„é€ citations
        if skip_stage4:
            print("\nã€é˜¶æ®µ4ã€‘æ„é€ citationsæ•°æ®...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-stage4 æŒ‡å®šï¼‰")
        else:
            build_citations(cursor, conn, force=force_stage4)
        
        # é˜¶æ®µ5ï¼šæ›´æ–°temp_import
        if skip_stage5:
            print("\nã€é˜¶æ®µ5ã€‘å¡«å……temp_importè¡¨...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-stage5 æŒ‡å®šï¼‰")
        else:
            update_temp_import(cursor, conn)
        
        # é˜¶æ®µ6ï¼šæ¸…ç†
        if not skip_cleanup:
            cleanup(cursor, conn, keep_citation_raw)
        else:
            print("\nã€é˜¶æ®µ6ã€‘æ¸…ç†æç¤º...ï¼ˆå·²è·³è¿‡ï¼ŒæŒ‰ --skip-cleanup æŒ‡å®šï¼‰")
        
        # æ€»ç»“
        total_time = time.time() - overall_start
        print("\n" + "=" * 80)
        print("ã€å¤„ç†å®Œæˆã€‘")
        print("=" * 80)
        print(f"  æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ ({total_time:.2f}ç§’)")
        print("=" * 80)
        
        log_performance("å®Œæ•´æµç¨‹å®Œæˆ", total_time_min=f"{total_time/60:.1f}")
        
    except Exception as e:
        print(f"\nâœ— å¤„ç†å¤±è´¥: {e}")
        conn.rollback()
        raise
    
    finally:
        cursor.close()
        conn.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="é«˜æ€§èƒ½å¯¼å…¥citationsæ•°æ®å¹¶æ›´æ–°temp_importè¡¨ï¼ˆé»˜è®¤ä¿ç•™citation_rawï¼Œé˜²æ­¢è¯¯åˆ ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸ”¥ æ¨èåˆ†æ­¥æ‰§è¡Œï¼ˆæ¯ä¸ªé˜¶æ®µç‹¬ç«‹è¿è¡Œï¼Œå¯ä¸­æ–­æ¢å¤ï¼‰ï¼š

  é˜¶æ®µ1ï¼šå¯¼å…¥gzæ–‡ä»¶åˆ°citation_rawè¡¨ï¼ˆ14äº¿+æ¡è®°å½•ï¼Œçº¦1-2å°æ—¶ï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-import
  
  é˜¶æ®µ2ï¼šåˆ›å»ºç´¢å¼•ï¼ˆçº¦30-60åˆ†é’Ÿï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-index
  
  é˜¶æ®µ3ï¼šæ„é€ referencesç¼“å­˜ï¼ˆçº¦20-30åˆ†é’Ÿï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-stage3
  
  é˜¶æ®µ4ï¼šæ„é€ citationsç¼“å­˜ï¼ˆçº¦20-30åˆ†é’Ÿï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-stage4
  
  é˜¶æ®µ5ï¼šæ›´æ–°temp_importè¡¨ï¼ˆçº¦10-20åˆ†é’Ÿï¼Œéœ€è¦temp_importè¡¨å·²æœ‰æ•°æ®ï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --only-stage5

ğŸ“‹ ä¸€æ¬¡æ€§æ‰§è¡Œï¼ˆä¸æ¨èï¼Œä¸­æ–­åéœ€é‡å¤´å¼€å§‹ï¼‰ï¼š
  python batch_update/import_citations.py D:\\gz_temp\\citations

âš™ï¸ å…¶ä»–å¸¸ç”¨é€‰é¡¹ï¼š
  # æ¸…ç©ºcitation_rawè¡¨é‡æ–°å¯¼å…¥
  python batch_update/import_citations.py D:\\gz_temp\\citations --truncate --only-import
  
  # å¼ºåˆ¶é‡å»ºç¼“å­˜ï¼ˆå·²æœ‰ç¼“å­˜æ—¶ï¼Œè‡ªåŠ¨è·³è¿‡é˜¶æ®µ0-2ï¼‰
  python batch_update/import_citations.py D:\\gz_temp\\citations --force-stage3
  python batch_update/import_citations.py D:\\gz_temp\\citations --force-stage4
  
  # æŒ‡å®šå…¶ä»–æœºå™¨
  python batch_update/import_citations.py D:\\gz_temp\\citations --machine machine2

âš ï¸ é‡è¦æç¤ºï¼š
  1. é˜¶æ®µ5ï¼šä¸´æ—¶è¡¨èšåˆï¼ˆSQLå±‚é¢ï¼‰+ åˆ†æ‰¹INSERTï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
  2. é˜¶æ®µ5ä¼šæ’å…¥çº¦6700ä¸‡æ¡è®°å½•ï¼ˆæ¯æ‰¹10ä¸‡æ¡ï¼Œå®æ—¶æ˜¾ç¤ºè¿›åº¦ï¼‰
  3. ä¸åšå»é‡ï¼šé‡å¤æ‰§è¡Œä¼šäº§ç”Ÿé‡å¤æ•°æ®ï¼ˆå»ºè®®æ‰§è¡Œå‰æ¸…ç©ºtemp_importï¼‰
  4. citation_rawè¡¨é»˜è®¤ä¿ç•™ï¼ˆ14äº¿+è®°å½•ï¼Œé‡å»ºéœ€1-2å°æ—¶ï¼‰
  5. ç¼“å­˜è¡¨temp_referenceså’Œtemp_citationså¯å¤ç”¨ï¼Œé™¤éæ•°æ®å˜åŒ–
  6. ç¡®ä¿corpusid_mapping_titleè¡¨å·²å­˜åœ¨ï¼ˆç”¨äºå¡«å……titleå­—æ®µï¼‰
        """
    )
    
    parser.add_argument("gz_directory", help="åŒ…å«citations gzæ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--machine", default="machine0", help="ç›®æ ‡æœºå™¨ (é»˜è®¤: machine0)")
    parser.add_argument("--truncate", action="store_true", help="æ¸…ç©ºcitation_rawè¡¨é‡æ–°å¯¼å…¥")
    parser.add_argument("--skip-import", action="store_true", help="è·³è¿‡é˜¶æ®µ1å¯¼å…¥")
    parser.add_argument("--skip-index", action="store_true", help="è·³è¿‡é˜¶æ®µ2å»ºç´¢å¼•")
    parser.add_argument("--skip-stage3", action="store_true", help="è·³è¿‡é˜¶æ®µ3ï¼ˆæ„é€ referencesç¼“å­˜ï¼‰")
    parser.add_argument("--skip-stage4", action="store_true", help="è·³è¿‡é˜¶æ®µ4ï¼ˆæ„é€ citationsç¼“å­˜ï¼‰")
    parser.add_argument("--skip-stage5", action="store_true", help="è·³è¿‡é˜¶æ®µ5ï¼ˆå¡«å……temp_importï¼‰")
    parser.add_argument("--only-import", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ0-1ï¼ˆå¯¼å…¥ï¼‰ï¼Œç„¶ååœæ­¢")
    parser.add_argument("--only-index", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ2ï¼ˆå»ºç´¢å¼•ï¼‰ï¼Œç„¶ååœæ­¢")
    parser.add_argument("--only-stage3", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ3ï¼ˆæ„é€ referencesç¼“å­˜ï¼‰")
    parser.add_argument("--only-stage4", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ4ï¼ˆæ„é€ citationsç¼“å­˜ï¼‰")
    parser.add_argument("--only-stage5", action="store_true", help="ä»…æ‰§è¡Œé˜¶æ®µ5ï¼ˆå¡«å……temp_importè¡¨ï¼‰")
    parser.add_argument("--force-stage3", action="store_true", help="å¼ºåˆ¶é‡å»ºé˜¶æ®µ3ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼Œè‡ªåŠ¨è·³è¿‡é˜¶æ®µ0-2ï¼‰")
    parser.add_argument("--force-stage4", action="store_true", help="å¼ºåˆ¶é‡å»ºé˜¶æ®µ4ç¼“å­˜ï¼ˆå¿½ç•¥å·²æœ‰ç¼“å­˜ï¼Œè‡ªåŠ¨è·³è¿‡é˜¶æ®µ0-2ï¼‰")
    parser.add_argument("--no-count", action="store_true", help="é˜¶æ®µ0è·³è¿‡COUNT(*)ï¼Œé¿å…åœ¨è¶…å¤§è¡¨ä¸Šå¡ä½")
    parser.add_argument("--skip-cleanup", action="store_true", help="è·³è¿‡é˜¶æ®µ6æ¸…ç†æç¤ºè¾“å‡º")
    parser.add_argument("--keep-raw", action="store_true", help="ï¼ˆå·²åºŸå¼ƒï¼šç°åœ¨é»˜è®¤ä¿ç•™citation_rawï¼‰")
    
    args = parser.parse_args()
    
    gz_dir = Path(args.gz_directory)
    if not gz_dir.is_dir():
        print(f"é”™è¯¯: {args.gz_directory} ä¸æ˜¯æœ‰æ•ˆçš„ç›®å½•")
        sys.exit(1)
    
    run_full_pipeline(
        args.gz_directory,
        args.machine,
        args.keep_raw,
        args.truncate,
        args.skip_import,
        args.skip_index,
        args.skip_stage3,
        args.skip_stage4,
        args.skip_stage5,
        args.only_import,
        args.only_index,
        args.only_stage3,
        args.only_stage4,
        args.only_stage5,
        args.force_stage3,
        args.force_stage4,
        args.no_count,
        args.skip_cleanup,
    )

