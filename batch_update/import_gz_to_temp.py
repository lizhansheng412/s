"""
ä»gzæ–‡ä»¶å¿«é€Ÿå¯¼å…¥æ•°æ®åˆ°ä¸´æ—¶è¡¨
ä½¿ç”¨COPYå‘½ä»¤å’Œä¼˜åŒ–çš„å¯¼å…¥ç­–ç•¥
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import gzip
import orjson  # æ¯”jsonå¿«2-3å€
import psycopg2
from datetime import datetime
import time
import threading
from db_config import get_db_config, MACHINE_DB_MAP
from init_temp_table import GZ_LOG_TABLE, DATASET_TYPES
from cleanup_imported_gz import DISK_THRESHOLD_GB

TEMP_TABLE = "temp_import"
CHUNK_SIZE = 60000  # å•æ–‡ä»¶ä¸€æ¬¡COPYï¼ˆ54000è¡Œ+ä½™é‡ï¼Œç¡®ä¿ä»»ä½•æ–‡ä»¶éƒ½ä¸€æ¬¡å®Œæˆï¼‰
RUNNING_LOG = Path(__file__).parent.parent / "logs" / "running.log"
FAILED_LOG_BASE = Path(__file__).parent.parent / "logs" / "batch_update"


def get_copy_sql(data_type):
    """æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆå¯¹åº”çš„COPY SQLå‘½ä»¤"""
    if data_type == 'embeddings_specter_v1':
        field = 'specter_v1'
    elif data_type == 'embeddings_specter_v2':
        field = 'specter_v2'
    elif data_type in ('s2orc', 's2orc_v2'):
        field = 'content'
    elif data_type == 'citations':
        # citations ä½¿ç”¨ä¸¤ä¸ªå­—æ®µï¼šcitations å’Œ referencesï¼ˆreferencesæ˜¯ä¿ç•™å­—ï¼Œéœ€è¦åŒå¼•å·ï¼‰
        return (
            f'COPY {TEMP_TABLE} (corpusid, citations, "references", is_done) '
            "FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t', NULL '')"
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {data_type}")
    
    return (
        f"COPY {TEMP_TABLE} (corpusid, {field}, is_done) "
        "FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t', NULL '')"
    )


def get_failed_log_path(data_type):
    """è·å–æŒ‡å®šæ•°æ®é›†çš„å¤±è´¥æ—¥å¿—è·¯å¾„"""
    return FAILED_LOG_BASE / data_type / "gz_import_failed.txt"


def log_performance(stage, **metrics):
    """è®°å½•æ€§èƒ½æ—¥å¿—åˆ°running.log"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    metrics_str = " | ".join([f"{k}={v}" for k, v in metrics.items()])
    log_line = f"[{timestamp}] {stage} | {metrics_str}\n"
    with open(RUNNING_LOG, 'a', encoding='utf-8') as f:
        f.write(log_line)


def log_failed_file(filename, data_type, error):
    """è®°å½•å¤±è´¥çš„gzæ–‡ä»¶"""
    failed_log = get_failed_log_path(data_type)
    failed_log.parent.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    log_line = f"[{timestamp}] {filename} | error={error}\n"
    with open(failed_log, 'a', encoding='utf-8') as f:
        f.write(log_line)


class CopyStream:
    """å°†è¡Œè¿­ä»£å™¨åŒ…è£…ä¸ºpsycopg2 copy_expertå¯æ¶ˆè´¹çš„æµ"""

    def __init__(self, iterator, chunk_size=65536):
        self.iterator = iterator
        self.chunk_size = chunk_size
        self._buffer = bytearray()
        self._exhausted = False

    def readable(self):  # ä¸ioæ¥å£å…¼å®¹
        return True

    def read(self, size=-1):
        if size == -1:
            chunks = [bytes(self._buffer)]
            self._buffer.clear()
            for chunk in self.iterator:
                chunks.append(chunk)
            self._exhausted = True
            return b''.join(chunks)

        while len(self._buffer) < max(size, self.chunk_size) and not self._exhausted:
            try:
                self._buffer.extend(next(self.iterator))
            except StopIteration:
                self._exhausted = True
                break

        if not self._buffer:
            return b''

        if size >= len(self._buffer):
            data = bytes(self._buffer)
            self._buffer.clear()
            return data

        data = bytes(self._buffer[:size])
        del self._buffer[:size]
        return data


def load_failed_files(data_type):
    """ä»å¤±è´¥æ—¥å¿—ä¸­è¯»å–æŒ‡å®šdatasetçš„å¤±è´¥æ–‡ä»¶åˆ—è¡¨"""
    failed_log = get_failed_log_path(data_type)
    if not failed_log.exists():
        return set()
    
    failed_files = set()
    try:
        with open(failed_log, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # è§£æ: [timestamp] filename | error=xxx
                if line.startswith('['):
                    try:
                        parts = line.split('|')
                        if len(parts) >= 1:
                            filename = parts[0].split(']', 1)[1].strip()
                            failed_files.add(filename)
                    except:
                        continue
    except Exception as e:
        print(f"WARNING: Failed to read failed log - {e}")
        return set()
    
    return failed_files


def delete_gz_file(gz_path):
    """åˆ é™¤gzæ–‡ä»¶ä»¥é‡Šæ”¾å­˜å‚¨ç©ºé—´"""
    try:
        gz_path.unlink()
        print(f"  ğŸ—‘ï¸  å·²åˆ é™¤æ–‡ä»¶ä»¥é‡Šæ”¾ç©ºé—´")
        return True
    except Exception as e:
        print(f"  âš ï¸  åˆ é™¤æ–‡ä»¶å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")
        return False


def start_cleanup_monitor(gz_directory, data_type, machine_id):
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨æ¸…ç†ç›‘æ§
    
    æ³¨æ„ï¼šä½¿ç”¨ daemon=True ç¡®ä¿ä¸»è¿›ç¨‹ç»“æŸæ—¶ç›‘æ§çº¿ç¨‹ä¹Ÿä¼šè‡ªåŠ¨ç»ˆæ­¢
    """
    try:
        # å¯¼å…¥ç›‘æ§å‡½æ•°
        from cleanup_imported_gz import monitor_and_cleanup
        
        # å¯åŠ¨å®ˆæŠ¤çº¿ç¨‹ï¼šä¸»è¿›ç¨‹ç»“æŸæ—¶ä¼šè‡ªåŠ¨ç»ˆæ­¢
        monitor_thread = threading.Thread(
            target=monitor_and_cleanup,
            args=(gz_directory, data_type, machine_id),
            daemon=True,  # å®ˆæŠ¤çº¿ç¨‹ï¼šä¸»è¿›ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
            name="DiskSpaceMonitor"
        )
        monitor_thread.start()
        
        print(f"å·²å¯åŠ¨ç£ç›˜ç©ºé—´ç›‘æ§ (é˜ˆå€¼: {DISK_THRESHOLD_GB}GB, é—´éš”: 15åˆ†é’Ÿ)")
    except Exception as e:
        print(f"WARNING: Failed to start cleanup monitor - {e}")


def is_file_imported(cursor, filename, data_type):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¯¼å…¥"""
    cursor.execute(
        f"SELECT 1 FROM {GZ_LOG_TABLE} WHERE filename = %s AND data_type = %s LIMIT 1",
        (filename, data_type)
    )
    return cursor.fetchone() is not None


def log_imported_file(cursor, filename, data_type):
    """è®°å½•å·²æˆåŠŸå¯¼å…¥çš„æ–‡ä»¶"""
    cursor.execute(
        f"INSERT INTO {GZ_LOG_TABLE} (filename, data_type) VALUES (%s, %s) ON CONFLICT (filename, data_type) DO NOTHING",
        (filename, data_type)
    )


def import_gz_to_temp_fast(gz_file_path, data_type=None, machine_id='machine0'):
    """
    ä»å•ä¸ªgzæ–‡ä»¶å¿«é€Ÿå¯¼å…¥æ•°æ®åˆ°ä¸´æ—¶è¡¨
    ä½¿ç”¨COPYå‘½ä»¤ï¼Œåˆ†å—å†™å…¥ï¼Œå‡å°‘å†…å­˜å³°å€¼
    
    ä¼˜åŒ–ç‚¹ï¼š
    - ä½¿ç”¨orjsonï¼ˆæ¯”jsonå¿«2-3å€ï¼‰
    - åˆ†å—COPYï¼ˆæ¯3ä¸‡è¡Œï¼Œ5ä¸‡è¡Œæ–‡ä»¶åˆ†2æ¬¡ï¼‰
    - å…³é—­åŒæ­¥æäº¤ï¼ˆæé€Ÿ20-30%ï¼‰
    - gzipå†…éƒ¨ç¼“å†²ä¼˜åŒ–
    
    Args:
        gz_file_path: gzæ–‡ä»¶è·¯å¾„
        data_type: æ•°æ®é›†ç±»å‹ï¼ˆå¿…éœ€ï¼Œç”¨äºè®°å½•å’Œè·³è¿‡å·²å¤„ç†æ–‡ä»¶ï¼‰
        machine_id: ç›®æ ‡æœºå™¨ID
    """
    if data_type is None:
        raise ValueError(f"å¿…é¡»æŒ‡å®š--datasetå‚æ•°ï¼Œå¯é€‰å€¼: {', '.join(DATASET_TYPES)}")
    
    if data_type not in DATASET_TYPES:
        raise ValueError(f"æ— æ•ˆçš„æ•°æ®é›†ç±»å‹: {data_type}ï¼Œå¯é€‰å€¼: {', '.join(DATASET_TYPES)}")
    
    conn = None
    cursor = None
    start_time = time.time()
    gz_path = Path(gz_file_path)
    filename = gz_path.name
    
    try:
        # è¿æ¥æ•°æ®åº“
        db_config = get_db_config(machine_id)
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¯¼å…¥
        if is_file_imported(cursor, filename, data_type):
            print(f"âŠ™ è·³è¿‡ï¼ˆå·²å¯¼å…¥ï¼‰: {filename} [{data_type}]")
            # åˆ é™¤å·²å¯¼å…¥çš„æ–‡ä»¶ä»¥é‡Šæ”¾ç©ºé—´
            delete_gz_file(gz_path)
            return
        
        print(f"å¤„ç†æ–‡ä»¶: {filename} [{data_type}]")
        
        # å…³é—­åŒæ­¥æäº¤ï¼Œæé€Ÿ20-30%ï¼ˆä¸´æ—¶æ•°æ®å¯æ¥å—é£é™©ï¼‰
        cursor.execute("SET LOCAL synchronous_commit = OFF;")
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹è·å–COPY SQL
        copy_sql = get_copy_sql(data_type)
        
        total_count = 0

        def row_iterator():
            nonlocal total_count

            try:
                with gzip.open(gz_file_path, 'rt', encoding='utf-8', errors='replace') as f:
                    for line in f:
                        try:
                            data = orjson.loads(line.strip())
                        except Exception:
                            continue

                        corpusid = data.get('corpusid')
                        if corpusid is None:
                            continue

                        # æ ¹æ®æ•°æ®é›†ç±»å‹æå–ä¸åŒçš„å­—æ®µ
                        data_to_store = None
                        
                        if data_type in ('s2orc', 's2orc_v2'):
                            # s2orc / s2orc_v2ï¼šä¼˜å…ˆä½¿ç”¨ contentï¼Œå¦åˆ™ç»„åˆ body + bibliography
                            data_to_store = data.get('content')
                            if data_to_store is None:
                                data_to_store = {}
                                if 'body' in data:
                                    data_to_store['body'] = data['body']
                                if 'bibliography' in data:
                                    data_to_store['bibliography'] = data['bibliography']
                                
                                # å¦‚æœæ—¢æ²¡æœ‰ body ä¹Ÿæ²¡æœ‰ bibliographyï¼Œè·³è¿‡è¯¥è®°å½•
                                if not data_to_store:
                                    continue
                        
                        elif data_type in ('embeddings_specter_v1', 'embeddings_specter_v2'):
                            # embeddingsï¼šç›´æ¥æå– vector å­—æ®µ
                            if 'vector' in data:
                                data_to_store = data['vector']
                            else:
                                continue
                        
                        elif data_type == 'citations':
                            # citationsï¼šæš‚ä¸å¤„ç†
                            continue
                        
                        else:
                            # æœªçŸ¥æ•°æ®é›†ç±»å‹
                            continue

                        content_json = orjson.dumps(data_to_store).decode('utf-8')
                        # TEXTæ ¼å¼ï¼šæ‰‹åŠ¨è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
                        escaped = content_json.replace('\\', '\\\\').replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                        row = f"{corpusid}\t{escaped}\tf\n"
                        total_count += 1
                        yield row.encode('utf-8')

            except Exception as e:
                raise RuntimeError(f"æ–‡ä»¶è¯»å–å¤±è´¥: {e}") from e

        parse_start = time.time()
        print("è¯»å–å¹¶è§£ægzæ–‡ä»¶...")

        try:
            cursor.copy_expert(copy_sql, CopyStream(row_iterator()))
        except RuntimeError as e:
            print(f"âœ— æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            print(f"  è·³è¿‡è¯¥æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
            log_failed_file(filename, data_type, str(e))
            delete_gz_file(gz_path)
            return
 
        # è®°å½•å·²æˆåŠŸå¯¼å…¥çš„æ–‡ä»¶
        log_imported_file(cursor, filename, data_type)
 
        conn.commit()
        
        # æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - start_time
        speed = total_count / total_time if total_time > 0 else 0
        
        print(f"âœ“ æˆåŠŸå¯¼å…¥ {total_count} æ¡è®°å½• [{data_type}]")
        print(f"  è€—æ—¶: {total_time:.2f}ç§’ | é€Ÿåº¦: {speed:.0f} æ¡/ç§’")
        
        # åˆ é™¤æˆåŠŸå¯¼å…¥çš„æ–‡ä»¶ä»¥é‡Šæ”¾ç©ºé—´
        delete_gz_file(gz_path)
        
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        # åˆ é™¤å¤±è´¥çš„æ–‡ä»¶ä»¥é‡Šæ”¾ç©ºé—´
        delete_gz_file(gz_path)
        raise
    
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def import_multiple_gz_fast(gz_directory, data_type=None, delete_gz=False, machine_id='machine0'):
    """
    é¡ºåºå¯¼å…¥ç›®å½•ä¸‹çš„æ‰€æœ‰gzæ–‡ä»¶
    ä½¿ç”¨åˆ†å—COPYå’Œä¼˜åŒ–çš„è§£æç­–ç•¥
    
    Args:
        gz_directory: åŒ…å«gzæ–‡ä»¶çš„ç›®å½•
        data_type: æ•°æ®é›†ç±»å‹ï¼ˆå¿…éœ€ï¼Œç”¨äºè®°å½•å’Œè·³è¿‡å·²å¤„ç†æ–‡ä»¶ï¼‰
        delete_gz: æ˜¯å¦åœ¨å¤„ç†å®Œæˆååˆ é™¤æ‰€æœ‰gzæ–‡ä»¶ï¼ˆé»˜è®¤Falseï¼‰
        machine_id: ç›®æ ‡æœºå™¨ID
    """
    if data_type is None:
        raise ValueError(f"å¿…é¡»æŒ‡å®š--datasetå‚æ•°ï¼Œå¯é€‰å€¼: {', '.join(DATASET_TYPES)}")
    
    if data_type not in DATASET_TYPES:
        raise ValueError(f"æ— æ•ˆçš„æ•°æ®é›†ç±»å‹: {data_type}ï¼Œå¯é€‰å€¼: {', '.join(DATASET_TYPES)}")
    
    gz_dir = Path(gz_directory)
    all_gz_files = sorted(gz_dir.glob("*.gz"))
    
    if not all_gz_files:
        print(f"åœ¨ {gz_directory} ä¸­æ²¡æœ‰æ‰¾åˆ°gzæ–‡ä»¶")
        return
    
    conn = None
    cursor = None
    overall_start = time.time()
    
    # å¯åŠ¨ç£ç›˜ç©ºé—´ç›‘æ§
    start_cleanup_monitor(gz_directory, data_type, machine_id)
    
    try:
        # è¿æ¥æ•°æ®åº“
        db_config = get_db_config(machine_id)
        print(f"è¿æ¥åˆ°æ•°æ®åº“ [{machine_id}: {db_config['database']}:{db_config['port']}]...")
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # æ£€æŸ¥å·²å¯¼å…¥çš„æ–‡ä»¶
        cursor.execute(
            f"SELECT filename FROM {GZ_LOG_TABLE} WHERE data_type = %s",
            (data_type,)
        )
        imported_files = set(row[0] for row in cursor.fetchall())
        
        # å¦‚æœå¯ç”¨äº†delete_gzï¼Œåˆ é™¤å·²æˆåŠŸå¯¼å…¥çš„æ–‡ä»¶ï¼ˆé‡Šæ”¾ç©ºé—´ï¼‰
        if delete_gz:
            skipped_imported = [f for f in all_gz_files if f.name in imported_files]
            if skipped_imported:
                print(f"æ­£åœ¨åˆ é™¤ {len(skipped_imported)} ä¸ªå·²å¯¼å…¥æ–‡ä»¶ä»¥é‡Šæ”¾ç©ºé—´...")
                deleted_imported = 0
                for gz_file in skipped_imported:
                    try:
                        if gz_file.exists():
                            gz_file.unlink()
                            deleted_imported += 1
                    except Exception:
                        # é™é»˜å¿½ç•¥åˆ é™¤é”™è¯¯
                        pass
                if deleted_imported > 0:
                    print(f"âœ“ å·²åˆ é™¤ {deleted_imported} ä¸ªå·²å¯¼å…¥æ–‡ä»¶")
        
        # åŠ è½½å·²çŸ¥å¤±è´¥çš„æ–‡ä»¶
        failed_files = load_failed_files(data_type)
        if failed_files:
            print(f"ä»å¤±è´¥æ—¥å¿—ä¸­åŠ è½½äº† {len(failed_files)} ä¸ªå·²çŸ¥å¤±è´¥æ–‡ä»¶")
        
        # å¦‚æœå¯ç”¨äº†delete_gzï¼Œåˆ é™¤å·²çŸ¥å¤±è´¥çš„æ–‡ä»¶ï¼ˆé‡Šæ”¾ç©ºé—´ï¼‰
        if delete_gz:
            skipped_failed = [f for f in all_gz_files if f.name in failed_files]
            if skipped_failed:
                print(f"æ­£åœ¨åˆ é™¤ {len(skipped_failed)} ä¸ªå·²çŸ¥å¤±è´¥æ–‡ä»¶ä»¥é‡Šæ”¾ç©ºé—´...")
                deleted_failed = 0
                for gz_file in skipped_failed:
                    try:
                        if gz_file.exists():
                            gz_file.unlink()
                            deleted_failed += 1
                    except Exception:
                        # é™é»˜å¿½ç•¥åˆ é™¤é”™è¯¯
                        pass
                if deleted_failed > 0:
                    print(f"âœ“ å·²åˆ é™¤ {deleted_failed} ä¸ªå·²çŸ¥å¤±è´¥æ–‡ä»¶")
        
        # è¿‡æ»¤å¾…å¤„ç†æ–‡ä»¶ï¼ˆæ’é™¤å·²å¯¼å…¥å’Œå·²çŸ¥å¤±è´¥çš„ï¼‰
        gz_files = [f for f in all_gz_files if f.name not in imported_files and f.name not in failed_files]
        
        print("=" * 80)
        print(f"æ•°æ®é›†: {data_type}")
        print(f"æ€»æ–‡ä»¶æ•°: {len(all_gz_files)}")
        if delete_gz:
            print(f"å·²å¯¼å…¥ï¼ˆå·²åˆ é™¤ï¼‰: {len(imported_files)}")
            print(f"å·²çŸ¥å¤±è´¥ï¼ˆå·²åˆ é™¤ï¼‰: {len(failed_files)}")
        else:
            print(f"å·²å¯¼å…¥ï¼ˆè·³è¿‡ï¼‰: {len(imported_files)}")
            print(f"å·²çŸ¥å¤±è´¥ï¼ˆè·³è¿‡ï¼‰: {len(failed_files)}")
        print(f"å¾…å¤„ç†: {len(gz_files)}")
        print("=" * 80)
        
        if not gz_files:
            print("æ²¡æœ‰å¾…å¤„ç†æ–‡ä»¶")
            return
        
        # å…³é—­åŒæ­¥æäº¤ï¼Œæé€Ÿ20-30%
        cursor.execute("SET synchronous_commit = OFF;")
        
        # æ ¹æ®æ•°æ®é›†ç±»å‹è·å–COPY SQL
        copy_sql = get_copy_sql(data_type)
        
        total_imported = 0
        skipped_count = 0
        failed_count = 0
        import_times = []
        
        for i, gz_file in enumerate(gz_files, 1):
            filename = gz_file.name
            
            file_start = time.time()
            print(f"\n[{i}/{len(gz_files)}] å¤„ç†æ–‡ä»¶: {filename}")
            
            file_count = 0

            try:
                print("  è¯»å–å¹¶è§£ægzæ–‡ä»¶...")
                def row_iterator():
                    nonlocal file_count

                    with gzip.open(gz_file, 'rt', encoding='utf-8', errors='replace') as f:
                        for line in f:
                            try:
                                data = orjson.loads(line.strip())
                            except Exception:
                                continue

                            corpusid = data.get('corpusid')
                            if corpusid is None:
                                continue

                            # æ ¹æ®æ•°æ®é›†ç±»å‹æå–ä¸åŒçš„å­—æ®µ
                            data_to_store = None
                            
                            if data_type in ('s2orc', 's2orc_v2'):
                                # s2orc / s2orc_v2ï¼šä¼˜å…ˆä½¿ç”¨ contentï¼Œå¦åˆ™ç»„åˆ body + bibliography
                                data_to_store = data.get('content')
                                if data_to_store is None:
                                    data_to_store = {}
                                    if 'body' in data:
                                        data_to_store['body'] = data['body']
                                    if 'bibliography' in data:
                                        data_to_store['bibliography'] = data['bibliography']
                                    
                                    # å¦‚æœæ—¢æ²¡æœ‰ body ä¹Ÿæ²¡æœ‰ bibliographyï¼Œè·³è¿‡è¯¥è®°å½•
                                    if not data_to_store:
                                        continue
                            
                            elif data_type in ('embeddings_specter_v1', 'embeddings_specter_v2'):
                                # embeddingsï¼šç›´æ¥æå– vector å­—æ®µ
                                if 'vector' in data:
                                    data_to_store = data['vector']
                                else:
                                    continue
                            
                            elif data_type == 'citations':
                                # citationsï¼šæš‚ä¸å¤„ç†
                                continue
                            
                            else:
                                # æœªçŸ¥æ•°æ®é›†ç±»å‹
                                continue

                            content_json = orjson.dumps(data_to_store).decode('utf-8')
                            # TEXTæ ¼å¼ï¼šæ‰‹åŠ¨è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
                            escaped = content_json.replace('\\', '\\\\').replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                            row = f"{corpusid}\t{escaped}\tf\n"
                            file_count += 1
                            yield row.encode('utf-8')

                cursor.copy_expert(copy_sql, CopyStream(row_iterator()))

                # è®°å½•å·²æˆåŠŸå¯¼å…¥çš„æ–‡ä»¶
                log_imported_file(cursor, filename, data_type)

                conn.commit()

                file_time = time.time() - file_start
                import_times.append(file_time)
                total_imported += file_count
                file_speed = file_count / file_time if file_time > 0 else 0

                print(f"  âœ“ {filename} å®Œæˆ ({file_count} æ¡ | {file_time:.2f}ç§’ | {file_speed:.0f} æ¡/ç§’)")
 
            except Exception as e:
                print(f"  âœ— {filename} å¤±è´¥: {e}")
                print(f"  è·³è¿‡è¯¥æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª...")
                log_failed_file(filename, data_type, str(e))
                failed_count += 1
                # å›æ»šå½“å‰äº‹åŠ¡ï¼Œä»¥ä¾¿ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶
                if conn:
                    conn.rollback()
                continue
        
        import_total_time = time.time() - overall_start
        success_count = len(gz_files) - failed_count
        avg_speed = total_imported / import_total_time if import_total_time > 0 else 0
        
        print("\n" + "=" * 80)
        print(f"ã€å¯¼å…¥å®Œæˆã€‘")
        print(f"  æ•°æ®é›†ç±»å‹: {data_type}")
        print(f"  å¾…å¤„ç†æ–‡ä»¶æ•°: {len(gz_files)} ä¸ª")
        print(f"  æˆåŠŸå¯¼å…¥: {success_count} ä¸ª")
        print(f"  å¤±è´¥: {failed_count} ä¸ª")
        print(f"  è®°å½•æ•°: {total_imported} æ¡")
        print(f"  æ€»è€—æ—¶: {import_total_time:.2f} ç§’")
        if total_imported > 0:
            print(f"  å¹³å‡é€Ÿåº¦: {avg_speed:.0f} æ¡/ç§’")
        print("=" * 80)
        
        if failed_count > 0:
            print(f"\nâš ï¸  æœ‰ {failed_count} ä¸ªæ–‡ä»¶å¯¼å…¥å¤±è´¥ï¼Œè¯¦ç»†ä¿¡æ¯å·²è®°å½•åˆ°:")
            print(f"   {get_failed_log_path(data_type)}")
        
        # æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦æ¸…ç©ºæ•´ä¸ªç›®å½•çš„æ‰€æœ‰ .gz æ–‡ä»¶
        if delete_gz:
            print("\n" + "=" * 80)
            print(f"æ­£åœ¨æ¸…ç©ºç›®å½• {gz_directory} ä¸­çš„æ‰€æœ‰gzæ–‡ä»¶...")
            remaining_gz_files = list(gz_dir.glob("*.gz"))
            deleted_count = 0
            
            for gz_file in remaining_gz_files:
                try:
                    if gz_file.exists():
                        gz_file.unlink()
                        deleted_count += 1
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤ {gz_file.name} å¤±è´¥: {e}")
            
            print(f"âœ“ å·²åˆ é™¤ {deleted_count} ä¸ªgzæ–‡ä»¶")
            print("=" * 80)
        else:
            remaining_count = len(list(gz_dir.glob("*.gz")))
            if remaining_count > 0:
                print(f"\nğŸ’¡ ç›®å½•ä¸­è¿˜æœ‰ {remaining_count} ä¸ªgzæ–‡ä»¶æœªåˆ é™¤")
                print(f"   å¦‚éœ€æ¸…ç©ºï¼Œè¯·æ·»åŠ  --delete-gz å‚æ•°")
        
        print("\næç¤º: æ•°æ®å·²å¯¼å…¥ï¼Œæ— éœ€æ’åºï¼ˆæ›´æ–°é˜¶æ®µä¼šåœ¨Pythonå±‚é¢æ’åºï¼‰")
        
        # è®°å½•æ—¥å¿—
        log_performance(
            "GZå¯¼å…¥å®Œæˆ",
            dataset=data_type,
            success=success_count,
            skipped=skipped_count,
            failed=failed_count,
            records=total_imported,
            time_sec=f"{import_total_time:.2f}",
            speed_per_sec=f"{avg_speed:.0f}"
        )
        
    except Exception as e:
        print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        raise
    
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ä»gzæ–‡ä»¶å¿«é€Ÿå¯¼å…¥æ•°æ®åˆ°ä¸´æ—¶è¡¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
æ•°æ®é›†ç±»å‹:
  {', '.join(DATASET_TYPES)}

ä½¿ç”¨ç¤ºä¾‹:
  # å¯¼å…¥åˆ° machine0 (é»˜è®¤)
  python batch_update/import_gz_to_temp.py D:\\gz_temp\\s2orc --dataset s2orc
  
  # å¯¼å…¥åˆ° machine1
  python batch_update/import_gz_to_temp.py D:\\gz_temp\\s2orc --dataset s2orc --machine machine1
  
  # å¯¼å…¥å¹¶åˆ é™¤æ‰€æœ‰gzæ–‡ä»¶
  python batch_update/import_gz_to_temp.py D:\\gz_temp\\s2orc --dataset s2orc --delete-gz
  
  # è‡ªåŠ¨æµæ°´çº¿æ¨¡å¼ï¼ˆå¯¼å…¥â†’å»ºç´¢å¼•â†’æ›´æ–°JSONLï¼‰
  python batch_update/import_gz_to_temp.py D:\\gz_temp\\s2orc --dataset s2orc --auto-pipeline
        """
    )
    parser.add_argument("path", help="gzæ–‡ä»¶è·¯å¾„æˆ–åŒ…å«gzæ–‡ä»¶çš„ç›®å½•")
    parser.add_argument(
        "--dataset",
        required=True,
        choices=DATASET_TYPES,
        help="æ•°æ®é›†ç±»å‹ï¼ˆå¿…éœ€ï¼‰"
    )
    parser.add_argument("--machine", default="machine0", choices=list(MACHINE_DB_MAP.keys()), 
                        help="ç›®æ ‡æœºå™¨ (é»˜è®¤: machine0)")
    parser.add_argument(
        "--delete-gz",
        action="store_true",
        help="å¤„ç†å®Œæˆååˆ é™¤æ‰€æœ‰gzæ–‡ä»¶ï¼ˆé»˜è®¤ä¸åˆ é™¤ï¼Œéœ€è¦æ˜ç¡®æŒ‡å®šï¼‰"
    )
    parser.add_argument(
        "--auto-pipeline",
        action="store_true",
        help="è‡ªåŠ¨æµæ°´çº¿æ¨¡å¼ï¼šå¯¼å…¥å®Œæˆåè‡ªåŠ¨æ‰§è¡Œå»ºç´¢å¼•å’ŒJSONLæ›´æ–°"
    )
    args = parser.parse_args()
    
    path = Path(args.path)
    
    # æ‰§è¡Œå¯¼å…¥
    if path.is_file():
        import_gz_to_temp_fast(path, data_type=args.dataset, machine_id=args.machine)
    elif path.is_dir():
        import_multiple_gz_fast(path, data_type=args.dataset, delete_gz=args.delete_gz, machine_id=args.machine)
    else:
        print(f"é”™è¯¯: {path} ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶æˆ–ç›®å½•")
        sys.exit(1)
    
    # å¦‚æœå¯ç”¨è‡ªåŠ¨æµæ°´çº¿ï¼Œç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤
    if args.auto_pipeline:
        print("\n" + "=" * 80)
        print("ã€è‡ªåŠ¨æµæ°´çº¿ã€‘æ­¥éª¤ 2/3: åˆ›å»ºç´¢å¼•")
        print("=" * 80)
        
        try:
            from init_temp_table import create_indexes
            create_indexes(machine_id=args.machine)
        except Exception as e:
            print(f"âœ— åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
            sys.exit(1)
        
        print("\n" + "=" * 80)
        print("ã€è‡ªåŠ¨æµæ°´çº¿ã€‘æ­¥éª¤ 3/3: æ›´æ–°JSONLæ–‡ä»¶")
        print("=" * 80)
        
        try:
            from jsonl_batch_updater import JSONLBatchUpdater
            updater = JSONLBatchUpdater(machine_id=args.machine)
            updater.run()
        except Exception as e:
            print(f"âœ— æ›´æ–°JSONLå¤±è´¥: {e}")
            sys.exit(1)
        
        print("\n" + "=" * 80)
        print("ã€è‡ªåŠ¨æµæ°´çº¿å®Œæˆã€‘æ‰€æœ‰æ­¥éª¤æ‰§è¡ŒæˆåŠŸï¼")
        print("=" * 80)

