#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æå–æ‰€æœ‰ gz æ–‡ä»¶ä¸­çš„ corpusid å¹¶æ’å…¥åˆ° final_delivery è¡¨
æé€Ÿ COPY æ’å…¥æ¨¡å¼ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
"""

import gzip
import sys
import time
import logging
from pathlib import Path
from typing import Set
from multiprocessing import Process, Queue, Manager
from queue import Empty
from io import StringIO, BufferedReader, TextIOWrapper

import psycopg2

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
import db_config

# æ€§èƒ½å‚æ•°ï¼ˆæ•°æ®åº“å†™å…¥ä¼˜åŒ– - å°æ‰¹æ¬¡å¿«é€Ÿæäº¤ï¼‰
BATCH_SIZE = 1000000         # 20ä¸‡æ¡/æ‰¹ï¼ˆå°æ‰¹æ¬¡ï¼Œé¿å…å•æ¬¡COPYè¿‡æ…¢ï¼‰
COMMIT_BATCHES = 1        # æ¯3æ‰¹æäº¤ï¼ˆ5ä¸‡æ¡/äº‹åŠ¡ï¼Œå¿«é€Ÿé‡Šæ”¾é”ï¼‰
NUM_EXTRACTORS = 1         # æå–è¿›ç¨‹ï¼ˆUSBç¡¬ç›˜ç“¶é¢ˆï¼Œå¿…é¡»ä¸º1é¿å…éšæœºè®¿é—®ï¼‰
NUM_INSERTERS = 1          # 3ä¸ªæ’å…¥è¿›ç¨‹ï¼ˆå¹³è¡¡å¹¶è¡Œå’Œé”ç«äº‰ï¼Œå¯ç”¨--insertersè°ƒæ•´ï¼‰
QUEUE_SIZE = 80            # å°é˜Ÿåˆ—ï¼ˆå¿«é€Ÿæµè½¬ï¼Œé¿å…å†…å­˜å †ç§¯ï¼‰

# USBç¡¬ç›˜ä¼˜åŒ–
USB_BUFFER_SIZE = 512 * 1024 * 1024  # 512MBç¼“å†²ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
SORT_BY_SIZE = True                  # æŒ‰æ–‡ä»¶å¤§å°æ’åº
SMALL_FILE_THRESHOLD = 500 * 1024 * 1024  # 500MBé˜ˆå€¼ï¼ˆå¿«é€Ÿè¯»å–ä¸­å°æ–‡ä»¶ï¼‰
SKIP_BATCH_DEDUP = True              # è·³è¿‡æ‰¹å†…å»é‡ï¼ˆæ•°æ®åº“å±‚é¢å»é‡æ›´å¿«ï¼‰

TABLE_NAME = 'final_delivery'

# æ—¥å¿—ç›®å½•
PROGRESS_DIR = 'logs/final_delivery_progress'
FAILED_DIR = 'logs/final_delivery_failed'

logging.basicConfig(level=logging.ERROR, format='%(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ª"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f if line.strip())
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


class FailedFilesLogger:
    """å¤±è´¥æ–‡ä»¶è®°å½•"""
    
    def __init__(self, failed_file: str):
        self.failed_file = Path(failed_file)
        self.failed_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_failed(self) -> Set[str]:
        if not self.failed_file.exists():
            return set()
        failed = set()
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('|')
                    if len(parts) >= 2:
                        failed.add(parts[1].strip())
        return failed
    
    def log_failed(self, file_name: str, error: str):
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(self.failed_file, 'a', encoding='utf-8') as f:
            f.write(f"{timestamp} | {file_name} | {error}\n")
            f.flush()
    
    def reset(self):
        if self.failed_file.exists():
            self.failed_file.unlink()


def fast_extract_corpusid(line: str) -> int:
    """å¿«é€Ÿæå– corpusid"""
    try:
        idx = line.lower().find('"corpusid"')
        if idx == -1:
            return None
        
        idx = line.find(':', idx)
        if idx == -1:
            return None
        
        idx += 1
        while idx < len(line) and line[idx] in ' \t':
            idx += 1
        
        start = idx
        while idx < len(line) and line[idx].isdigit():
            idx += 1
        
        if idx > start:
            return int(line[start:idx])
        return None
    except (ValueError, IndexError):
        return None


def extractor_worker(file_queue: Queue, data_queue: Queue, progress_queue: Queue,
                    stats_dict: dict, batch_size: int = BATCH_SIZE):
    """ç”Ÿäº§è€…ï¼šè§£å‹å¹¶æå– corpusid"""
    import logging
    logging.getLogger().setLevel(logging.CRITICAL)
    
    while True:
        try:
            task = file_queue.get(timeout=1)
            if task is None:
                break
            
            gz_file_path, file_name = task
            
            try:
                # ç»ˆæä¼˜åŒ–ï¼šè·³è¿‡æ‰¹å†…å»é‡ï¼Œç›´æ¥ç”¨listï¼ˆæ•°æ®åº“å±‚é¢å»é‡æ›´å¿«ï¼‰
                batch_list = [] if SKIP_BATCH_DEDUP else set()
                valid_count = 0
                
                try:
                    # USBç¡¬ç›˜ä¼˜åŒ–ï¼šæ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé€‰æ‹©æœ€ä¼˜è¯»å–æ–¹å¼
                    import os
                    file_size = os.path.getsize(gz_file_path)
                    
                    # å°æ–‡ä»¶ï¼ˆ<1.5GBï¼‰ï¼šä¸€æ¬¡æ€§è¯»å…¥å†…å­˜ï¼Œé¿å…å¤šæ¬¡ç£ç›˜è®¿é—®
                    if file_size < SMALL_FILE_THRESHOLD:
                        with gzip.open(gz_file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                            for line in content.splitlines():
                                line = line.strip()
                                if not line or len(line) < 15:
                                    continue
                                
                                corpusid = fast_extract_corpusid(line)
                                if corpusid is None:
                                    continue
                                
                                valid_count += 1
                                if SKIP_BATCH_DEDUP:
                                    batch_list.append(corpusid)
                                    if len(batch_list) >= batch_size:
                                        data_queue.put(('data', batch_list))
                                        batch_list = []
                                else:
                                    batch_list.add(corpusid)
                                    if len(batch_list) >= batch_size:
                                        data_queue.put(('data', list(batch_list)))
                                        batch_list.clear()
                    else:
                        # å¤§æ–‡ä»¶ï¼šæµå¼è¯»å–ï¼Œä½¿ç”¨è¶…å¤§ç¼“å†²åŒºï¼ˆ512MBï¼‰
                        with gzip.open(gz_file_path, 'rb') as f_binary:
                            f = TextIOWrapper(BufferedReader(f_binary, buffer_size=USB_BUFFER_SIZE), 
                                            encoding='utf-8', errors='ignore')
                            
                            for line in f:
                                line = line.strip()
                                if not line or len(line) < 15:
                                    continue
                                
                                corpusid = fast_extract_corpusid(line)
                                if corpusid is None:
                                    continue
                                
                                valid_count += 1
                                if SKIP_BATCH_DEDUP:
                                    batch_list.append(corpusid)
                                    if len(batch_list) >= batch_size:
                                        data_queue.put(('data', batch_list))
                                        batch_list = []
                                else:
                                    batch_list.add(corpusid)
                                    if len(batch_list) >= batch_size:
                                        data_queue.put(('data', list(batch_list)))
                                        batch_list.clear()
                    
                    if batch_list:
                        if SKIP_BATCH_DEDUP:
                            data_queue.put(('data', batch_list))
                        else:
                            data_queue.put(('data', list(batch_list)))
                    
                    progress_queue.put(('done', file_name, valid_count))
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError, gzip.BadGzipFile):
                    progress_queue.put(('error', file_name, "Corrupted"))
                    continue
                
            except Exception as e:
                progress_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


def inserter_worker(worker_id: int, data_queue: Queue, stats_dict: dict, 
                   commit_batches: int = COMMIT_BATCHES):
    """æ¶ˆè´¹è€…ï¼šæ‰¹é‡COPYæ’å…¥"""
    conn = None
    cursor = None
    buffer_pool = StringIO()
    
    try:
        config = db_config.DB_CONFIG
        conn = psycopg2.connect(**config)
        conn.autocommit = False
        cursor = conn.cursor()
        
        try:
            cursor.execute("SET synchronous_commit = OFF")
            cursor.execute("SET work_mem = '1GB'")
        except Exception:
            pass
        
        total_inserted = 0
        batch_count = 0
        
        while True:
            try:
                item = data_queue.get(timeout=5)
                item_type = item[0]
                
                if item_type == 'stop':
                    break
                
                elif item_type == 'data':
                    _, corpusids = item
                    
                    try:
                        inserted = batch_copy_insert(cursor, corpusids, buffer_pool)
                        batch_count += 1
                        
                        if batch_count >= commit_batches:
                            conn.commit()
                            batch_count = 0
                        
                        total_inserted += inserted
                    
                    except psycopg2.DatabaseError as e:
                        conn.rollback()
                        batch_count = 0
                        try:
                            if cursor:
                                cursor.close()
                            if conn:
                                conn.close()
                            conn = psycopg2.connect(**db_config)
                            conn.autocommit = False
                            cursor = conn.cursor()
                            cursor.execute("SET synchronous_commit = OFF")
                        except Exception:
                            pass
            
            except Empty:
                continue
            except Exception:
                if conn:
                    conn.rollback()
                continue
        
        if batch_count > 0:
            conn.commit()
        
        stats_dict[f'inserted_{worker_id}'] = total_inserted
        
    except Exception as e:
        logger.error(f"[Inserter-{worker_id}] è¿›ç¨‹é”™è¯¯: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def batch_copy_insert(cursor, corpusids: list, buffer: StringIO = None) -> int:
    """æ‰¹é‡COPYæ’å…¥"""
    if not corpusids:
        return 0
    
    try:
        if buffer is None:
            buffer = StringIO()
        else:
            buffer.seek(0)
            buffer.truncate(0)
        
        # æ„é€ æ•°æ®ï¼šæ¯è¡Œä¸€ä¸ª corpusid
        for cid in corpusids:
            buffer.write(str(cid))
            buffer.write('\n')
        
        buffer.seek(0)
        
        # COPY æ’å…¥
        cursor.copy_expert(
            f"COPY {TABLE_NAME} (corpusid) FROM STDIN",
            buffer
        )
        return len(corpusids)
    
    except Exception as e:
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        raise


def process_gz_folder(folder_path: str, 
                     num_extractors: int = NUM_EXTRACTORS,
                     num_inserters: int = NUM_INSERTERS,
                     resume: bool = True, 
                     reset_progress: bool = False):
    """å¤„ç† GZ æ–‡ä»¶å¤¹ï¼Œæå–æ‰€æœ‰ corpusid"""
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # ä½¿ç”¨æ–‡ä»¶å¤¹åä½œä¸ºæ—¥å¿—æ ‡è¯†
    folder_name = folder.name
    progress_file = Path(PROGRESS_DIR) / f"{folder_name}_progress.txt"
    failed_file = Path(FAILED_DIR) / f"{folder_name}_failed.txt"
    
    tracker = ProgressTracker(str(progress_file))
    failed_logger = FailedFilesLogger(str(failed_file))
    
    if reset_progress:
        tracker.reset()
        failed_logger.reset()
    
    completed_files = tracker.load_completed() if resume else set()
    failed_files = failed_logger.load_failed() if resume else set()
    
    gz_files = list(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ° .gz æ–‡ä»¶: {folder_path}")
        return
    
    # USBç¡¬ç›˜ä¼˜åŒ–ï¼šæŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œå…ˆå¤„ç†å°æ–‡ä»¶
    # å¥½å¤„ï¼š1) å‡å°‘å†…å­˜å‹åŠ› 2) å¿«é€Ÿçœ‹åˆ°è¿›åº¦ 3) é¡ºåºè®¿é—®ç£ç›˜
    if SORT_BY_SIZE:
        gz_files = sorted(gz_files, key=lambda f: f.stat().st_size)
    
    excluded_files = completed_files | failed_files
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in excluded_files]
    
    logger.info(f"\nğŸ“‚ æ–‡ä»¶å¤¹: {folder_name}")
    logger.info(f"   æ€»è®¡:{len(gz_files)} å·²å®Œæˆ:{len(completed_files)} å¾…å¤„ç†:{len(pending_files)}")
    logger.info(f"   æå–:{num_extractors}è¿›ç¨‹ æ’å…¥:{num_inserters}è¿›ç¨‹")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ\n")
        return
    
    overall_start = time.time()
    
    try:
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        progress_queue = Queue()
        
        manager = Manager()
        stats_dict = manager.dict()
        
        for task in pending_files:
            file_queue.put(task)
        
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨æ’å…¥è¿›ç¨‹
        inserters = []
        for i in range(num_inserters):
            p = Process(
                target=inserter_worker,
                args=(i+1, data_queue, stats_dict, COMMIT_BATCHES),
                name=f'Inserter-{i+1}'
            )
            p.start()
            inserters.append(p)
        
        # å¯åŠ¨æå–è¿›ç¨‹
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, progress_queue, stats_dict, BATCH_SIZE),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        # ç›‘æ§è¿›åº¦
        completed_count = 0
        failed_count = 0
        last_log_time = time.time()
        start_time = time.time()
        
        from datetime import datetime
        start_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"\nâ° å¼€å§‹æ—¶é—´: {start_datetime}")
        print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(pending_files)}\n")
        
        while completed_count + failed_count < len(pending_files):
            try:
                item = progress_queue.get(timeout=2)
                item_type = item[0]
                
                if item_type == 'done':
                    _, file_name, _ = item
                    tracker.mark_completed(file_name)
                    completed_count += 1
                
                elif item_type == 'error':
                    _, file_name, error = item
                    failed_logger.log_failed(file_name, error)
                    failed_count += 1
                
                # å®æ—¶æ›´æ–°è¿›åº¦
                current_time = time.time()
                if current_time - last_log_time >= 1:
                    elapsed = current_time - start_time
                    processed = completed_count + failed_count
                    progress_pct = (processed / len(pending_files) * 100) if pending_files else 0
                    
                    # é¢„ä¼°å‰©ä½™æ—¶é—´
                    if processed > 0:
                        avg_time_per_file = elapsed / processed
                        remaining_files = len(pending_files) - processed
                        eta_seconds = avg_time_per_file * remaining_files
                        eta_hours = int(eta_seconds // 3600)
                        eta_minutes = int((eta_seconds % 3600) // 60)
                        eta_secs = int(eta_seconds % 60)
                        eta_str = f"{eta_hours:02d}:{eta_minutes:02d}:{eta_secs:02d}"
                    else:
                        eta_str = "--:--:--"
                    
                    elapsed_hours = int(elapsed // 3600)
                    elapsed_minutes = int((elapsed % 3600) // 60)
                    elapsed_secs = int(elapsed % 60)
                    elapsed_str = f"{elapsed_hours:02d}:{elapsed_minutes:02d}:{elapsed_secs:02d}"
                    
                    print(f"\rğŸ“Š è¿›åº¦:{processed}/{len(pending_files)} ({progress_pct:.1f}%) | "
                          f"âœ…æˆåŠŸ:{completed_count} âŒå¤±è´¥:{failed_count} | "
                          f"â±ï¸å·²ç”¨:{elapsed_str} é¢„è®¡å‰©ä½™:{eta_str}    ", 
                          end='', flush=True)
                    last_log_time = current_time
            
            except Empty:
                continue
        
        # ç­‰å¾…æå–è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join()
        
        # åœæ­¢æ’å…¥è¿›ç¨‹
        for _ in range(num_inserters):
            data_queue.put(('stop', None))
        
        for p in inserters:
            p.join()
        
        elapsed = time.time() - overall_start
        total_inserted = sum(stats_dict.get(f'inserted_{i}', 0) for i in range(1, num_inserters+1))
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        total_hours = int(elapsed // 3600)
        total_minutes = int((elapsed % 3600) // 60)
        total_secs = int(elapsed % 60)
        
        end_datetime = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        print("\n")
        logger.info(f"{'='*70}")
        logger.info(f"âœ… [{folder_name}] å¤„ç†å®Œæˆ")
        logger.info(f"{'='*70}")
        logger.info(f"â° ç»“æŸæ—¶é—´: {end_datetime}")
        logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        logger.info(f"   - æˆåŠŸæ–‡ä»¶: {completed_count:,}")
        logger.info(f"   - å¤±è´¥æ–‡ä»¶: {failed_count:,}")
        logger.info(f"   - æ’å…¥è®°å½•: {total_inserted:,} æ¡")
        logger.info(f"â±ï¸  æ€§èƒ½ç»Ÿè®¡:")
        logger.info(f"   - æ€»è€—æ—¶: {total_hours:02d}:{total_minutes:02d}:{total_secs:02d}")
        logger.info(f"   - æ’å…¥é€Ÿåº¦: {avg_rate:,.0f} æ¡/ç§’")
        if completed_count > 0:
            logger.info(f"   - å¹³å‡æ¯æ–‡ä»¶: {elapsed/completed_count:.1f} ç§’")
        logger.info(f"{'='*70}\n")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def batch_process_folders(folders: list, 
                          num_extractors: int = NUM_EXTRACTORS,
                          num_inserters: int = NUM_INSERTERS,
                          resume: bool = True):
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶å¤¹"""
    logger.info("="*70)
    logger.info(f"ğŸš€ æ‰¹é‡å¤„ç†å¯åŠ¨")
    logger.info(f"   å¾…å¤„ç†: {len(folders)} ä¸ªæ–‡ä»¶å¤¹")
    for i, folder in enumerate(folders, 1):
        logger.info(f"   [{i}] {folder}")
    logger.info(f"   è¿›ç¨‹é…ç½®: æå–={num_extractors}, æ’å…¥={num_inserters}")
    logger.info("="*70)
    
    overall_start = time.time()
    success_count = 0
    failed_folders = []
    
    for i, folder_path in enumerate(folders, 1):
        folder = Path(folder_path)
        
        logger.info("")
        logger.info(f"ğŸ“ [{i}/{len(folders)}] {folder.name}")
        logger.info("-"*70)
        
        if not folder.exists():
            logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            failed_folders.append(f"{folder.name} (ä¸å­˜åœ¨)")
            continue
        
        try:
            process_gz_folder(
                folder_path=str(folder_path),
                num_extractors=num_extractors,
                num_inserters=num_inserters,
                resume=resume,
                reset_progress=False
            )
            
            success_count += 1
            logger.info(f"âœ… {folder.name} å®Œæˆ\n")
            
        except KeyboardInterrupt:
            logger.warning(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ | å·²å®Œæˆ: {success_count}/{len(folders)}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"âŒ {folder.name} å¤±è´¥: {e}")
            failed_folders.append(f"{folder.name} ({str(e)})")
            continue
    
    elapsed = time.time() - overall_start
    
    logger.info("")
    logger.info("="*70)
    logger.info("ğŸ æ‰¹é‡å¤„ç†å®Œæˆ")
    logger.info(f"   æˆåŠŸ: {success_count}/{len(folders)} | è€—æ—¶: {elapsed/3600:.2f}å°æ—¶")
    
    if failed_folders:
        logger.warning("âš ï¸  å¤±è´¥åˆ—è¡¨:")
        for folder in failed_folders:
            logger.warning(f"     {folder}")
    
    logger.info("="*70)
    
    if success_count == len(folders):
        logger.info("âœ… å…¨éƒ¨æˆåŠŸï¼")
        logger.info("ğŸ’¡ è¿è¡Œå»é‡å’Œå»ºç´¢å¼•: python scripts/all_corpusid_of_5dataset/init_table.py --finalize")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æå– gz æ–‡ä»¶ä¸­çš„ corpusid å¹¶æ’å…¥åˆ° final_delivery è¡¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  # å•ä¸ªæ–‡ä»¶å¤¹
  python scripts/all_corpusid_of_5dataset/extract_corpusid.py \\
    --dir "E:\\data\\s2orc"
  
  # æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶å¤¹
  python scripts/all_corpusid_of_5dataset/extract_corpusid.py \\
    --dirs "E:\\data\\s2orc" "E:\\data\\citations" "E:\\data\\papers"
  
  # è‡ªå®šä¹‰è¿›ç¨‹æ•°
  python scripts/all_corpusid_of_5dataset/extract_corpusid.py \\
    --dirs "E:\\data\\s2orc" "E:\\data\\citations" \\
    --extractors 2 --inserters 6
        """
    )
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--dir', type=str, help='å•ä¸ªGZæ–‡ä»¶å¤¹è·¯å¾„')
    group.add_argument('--dirs', nargs='+', type=str, help='å¤šä¸ªæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰')
    
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS, 
                       help=f'æå–è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--inserters', type=int, default=NUM_INSERTERS, 
                       help=f'æ’å…¥è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_INSERTERS}ï¼‰')
    parser.add_argument('--no-resume', action='store_true', help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true', help='é‡ç½®è¿›åº¦')
    
    args = parser.parse_args()
    
    # å•ä¸ªæ–‡ä»¶å¤¹å¤„ç†
    if args.dir:
        process_gz_folder(
            folder_path=args.dir,
            num_extractors=args.extractors,
            num_inserters=args.inserters,
            resume=not args.no_resume,
            reset_progress=args.reset
        )
    
    # æ‰¹é‡å¤„ç†
    elif args.dirs:
        batch_process_folders(
            folders=args.dirs,
            num_extractors=args.extractors,
            num_inserters=args.inserters,
            resume=not args.no_resume
        )


if __name__ == '__main__':
    main()
