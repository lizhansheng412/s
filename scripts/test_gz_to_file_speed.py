#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼å†™å…¥æ–‡ä»¶æ€§èƒ½æµ‹è¯•
=============================================
åŠŸèƒ½ï¼šå®Œå…¨æ¨¡æ‹Ÿ stream_gz_to_db_optimized.pyï¼Œä½†å°†æ•°æ®åº“æ’å…¥æ›¿æ¢ä¸ºæ–‡ä»¶å†™å…¥
ç›®æ ‡ï¼šæœ€å¤§åŒ–å†™å…¥é€Ÿåº¦ï¼Œæµ‹è¯•çº¯I/Oæ€§èƒ½ç“¶é¢ˆï¼ˆç›®æ ‡ï¼š8000-15000æ¡/ç§’ï¼‰

æè‡´æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼š
âœ… join+å•æ¬¡writeï¼šæ¯”writelineså’Œåˆ—è¡¨æ¨å¯¼å¼æ›´å¿«ï¼Œå‡å°‘å†…å­˜åˆ†é…
âœ… 512MBè¶…å¤§ç¼“å†²åŒºï¼šå¤§å¹…å‡å°‘ç³»ç»Ÿè°ƒç”¨ï¼Œæå‡30-50%æ€§èƒ½
âœ… 16MBè§£å‹ç¼“å†²åŒºï¼šå‡å°‘è§£å‹ç³»ç»Ÿè°ƒç”¨
âœ… å»¶è¿Ÿflushï¼šç´¯ç§¯Næ‰¹æ¬¡åæ‰flushï¼Œå‡å°‘ç£ç›˜I/O
âœ… ç”Ÿäº§è€…-æ¶ˆè´¹è€…ï¼šè§£å‹å’Œå†™å…¥å®Œå…¨å¹¶è¡Œ
âœ… æ–‡ä»¶å³æ—¶å…³é—­ï¼šå¤„ç†å®Œç«‹å³é‡Šæ”¾èµ„æºï¼ˆé¿å…900ä¸ªæ–‡ä»¶åŒæ—¶æ‰“å¼€ï¼‰
âœ… æµå¼å¤„ç†ï¼šä¸ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜

è¾“å‡ºï¼šæ¯ä¸ªgzå¯¹åº”ä¸€ä¸ªtxtæ–‡ä»¶ï¼Œä¿å­˜åˆ° E:\test ç›®å½•
"""

import gzip
import sys
import time
import logging
from pathlib import Path
from typing import Set
from multiprocessing import Process, Queue, Manager
from queue import Empty
import io

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# =============================================================================
# æè‡´ä¼˜åŒ–é…ç½®ï¼ˆé’ˆå¯¹æ–‡ä»¶å†™å…¥ï¼‰
# =============================================================================

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path(r'E:\test')

# é’ˆå¯¹embeddingæ•°æ®ä¼˜åŒ–ï¼ˆ16KB/æ¡ï¼‰
EMBEDDING_BATCH_SIZE = 10000  # 10000æ¡Ã—16KB = 160MB/æ‰¹æ¬¡
BATCH_SIZE = EMBEDDING_BATCH_SIZE

# æ€§èƒ½é…ç½®
NUM_EXTRACTORS = 1  # è§£å‹è¿›ç¨‹æ•°ï¼ˆembeddingæ•°æ®å¤§ï¼Œå•è¿›ç¨‹é¿å…I/Oäº‰ç”¨ï¼‰
QUEUE_SIZE = 50  # æ›´å¤§é˜Ÿåˆ—ï¼Œç¡®ä¿å†™å…¥è¿›ç¨‹ä¸ç©ºé—²
WRITE_BUFFER_SIZE = 512 * 1024 * 1024  # 512MB è¶…å¤§å†™å…¥ç¼“å†²åŒºï¼ˆæè‡´ä¼˜åŒ–ï¼‰
DECOMPRESS_BUFFER_SIZE = 16 * 1024 * 1024  # 16MB è§£å‹ç¼“å†²åŒº

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s'
)
logger = logging.getLogger(__name__)

# =============================================================================
# è¿›åº¦è·Ÿè¸ª
# =============================================================================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        
        completed = set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    completed.add(line)
        return completed
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


class FailedFilesLogger:
    """å¤±è´¥æ–‡ä»¶è®°å½•å™¨"""
    
    def __init__(self, failed_file: str):
        self.failed_file = Path(failed_file)
        self.failed_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_failed(self) -> Set[str]:
        if not self.failed_file.exists():
            return set()
        
        failed = set()
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('|')
                    if len(parts) >= 2:
                        failed.add(parts[1].strip())
        return failed
    
    def log_failed(self, file_name: str, error: str):
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(self.failed_file, 'a', encoding='utf-8') as f:
            f.write(f"{timestamp} | {file_name} | {error}\n")
            f.flush()

# =============================================================================
# ç”Ÿäº§è€…ï¼šè§£å‹è¿›ç¨‹
# =============================================================================

def extractor_worker(
    file_queue: Queue,
    data_queue: Queue,
    stats_dict: dict,
    batch_size: int = BATCH_SIZE
):
    """
    è§£å‹å·¥ä½œè¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰- æè‡´ä¼˜åŒ–ç‰ˆ
    ä»file_queueå–æ–‡ä»¶ï¼Œè§£å‹åæ”¾å…¥data_queue
    
    æ€§èƒ½ä¼˜åŒ–ï¼š
    1. 16MBè§£å‹ç¼“å†²åŒºï¼ˆå‡å°‘è§£å‹ç³»ç»Ÿè°ƒç”¨ï¼‰
    2. æµå¼å¤„ç†ï¼ˆä¸ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜ï¼‰
    3. æ‰¹é‡å‘é€åˆ°é˜Ÿåˆ—ï¼ˆå‡å°‘è¿›ç¨‹é—´é€šä¿¡å¼€é”€ï¼‰
    4. è‡ªåŠ¨è·³è¿‡æŸåçš„gzæ–‡ä»¶
    """
    # ç¦ç”¨æ­¤è¿›ç¨‹çš„æ—¥å¿—è¾“å‡º
    logging.getLogger().setLevel(logging.CRITICAL)
    
    while True:
        try:
            # è·å–æ–‡ä»¶ä»»åŠ¡
            task = file_queue.get(timeout=1)
            if task is None:  # ç»“æŸä¿¡å·
                break
            
            gz_file_path, file_name = task
            
            try:
                batch = []
                valid_count = 0
                
                # æµå¼è§£å‹ - ä½¿ç”¨æ›´å¤§ç¼“å†²åŒº
                try:
                    with gzip.open(gz_file_path, 'rb') as f_binary:
                        # åŒ…è£…ä¸ºå¸¦å¤§ç¼“å†²åŒºçš„æ–‡æœ¬æµï¼ˆ16MBï¼‰
                        f = io.TextIOWrapper(
                            io.BufferedReader(f_binary, buffer_size=DECOMPRESS_BUFFER_SIZE), 
                            encoding='utf-8', 
                            errors='ignore'
                        )
                        for line in f:
                            line = line.strip()
                            if not line:
                                continue
                            
                            valid_count += 1
                            batch.append(line)
                            
                            # æ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€åˆ°é˜Ÿåˆ—
                            if len(batch) >= batch_size:
                                data_queue.put(('data', file_name, batch))
                                batch = []
                    
                    # å‘é€å‰©ä½™æ•°æ®
                    if batch:
                        data_queue.put(('data', file_name, batch))
                    
                    # å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                    data_queue.put(('done', file_name, valid_count))
                    
                    # æ›´æ–°ç»Ÿè®¡
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError):
                    # GZIPæ–‡ä»¶æŸåï¼Œè·³è¿‡
                    data_queue.put(('error', file_name, f"Corrupted"))
                    continue
                
            except Exception as e:
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


# =============================================================================
# æ¶ˆè´¹è€…ï¼šå†™å…¥è¿›ç¨‹
# =============================================================================

def writer_worker(
    data_queue: Queue,
    output_dir: Path,
    folder_name: str,
    stats_dict: dict,
    tracker: ProgressTracker,
    failed_logger: FailedFilesLogger,
    total_files: int = 0,
    commit_batches: int = 3  # æ¯Nä¸ªæ‰¹æ¬¡flushä¸€æ¬¡
):
    """
    å†™å…¥å·¥ä½œè¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰- æè‡´ä¼˜åŒ–ç‰ˆ
    æŒç»­ä»data_queueå–æ•°æ®å¹¶æ‰¹é‡å†™å…¥æ–‡ä»¶
    
    æè‡´æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š
    1. æ¯ä¸ªgzå¯¹åº”ä¸€ä¸ªè¾“å‡ºæ–‡ä»¶ï¼ˆé¿å…å•æ–‡ä»¶è¿‡å¤§ï¼Œembeddingè§£å‹åçº¦3GB/æ–‡ä»¶ï¼‰
    2. join+å•æ¬¡writeï¼ˆæ¯”writelineså’Œåˆ—è¡¨æ¨å¯¼å¼æ›´å¿«ï¼Œå‡å°‘å†…å­˜åˆ†é…ï¼‰
    3. è¶…å¤§ç¼“å†²åŒº512MBï¼ˆå¤§å¹…å‡å°‘ç³»ç»Ÿè°ƒç”¨ï¼Œæå‡30-50%æ€§èƒ½ï¼‰
    4. å»¶è¿Ÿflushï¼ˆç´¯ç§¯Næ‰¹æ¬¡åæ‰flushï¼Œå‡å°‘ç£ç›˜I/Oï¼‰
    5. æ–‡ä»¶å®Œæˆåç«‹å³å…³é—­ï¼ˆé¿å…900ä¸ªæ–‡ä»¶åŒæ—¶æ‰“å¼€ï¼Œé‡Šæ”¾èµ„æºï¼‰
    6. æœ€å°åŒ–å­—å…¸è®¿é—®ï¼ˆç¼“å­˜æ–‡ä»¶å¥æŸ„ï¼‰
    
    é¢„æœŸæ€§èƒ½ï¼š8000-15000æ¡/ç§’ï¼ˆçº¯I/Oï¼Œæ— æ•°æ®åº“å¼€é”€ï¼‰
    """
    file_handles = {}  # åœ¨tryå¤–å®šä¹‰ï¼Œç¡®ä¿finallyèƒ½è®¿é—®
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)
        
        total_written = 0
        completed_files = 0
        last_log_time = time.time()
        start_time = time.time()
        
        while True:
            try:
                # éé˜»å¡è·å–
                item = data_queue.get(timeout=5)
                item_type = item[0]
                
                if item_type == 'stop':
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    try:
                        # è·å–æˆ–åˆ›å»ºæ–‡ä»¶å¥æŸ„
                        if file_name not in file_handles:
                            # è¾“å‡ºæ–‡ä»¶åï¼šåŸæ–‡ä»¶åå»æ‰.gzåç¼€
                            output_file = output_dir / f"{file_name.replace('.gz', '.txt')}"
                            # ä½¿ç”¨è¶…å¤§ç¼“å†²åŒºï¼ˆ512MBï¼‰
                            fh = open(
                                output_file, 
                                'w', 
                                encoding='utf-8', 
                                buffering=WRITE_BUFFER_SIZE
                            )
                            file_handles[file_name] = [fh, 0]  # [handle, batch_count]
                        
                        fh, batch_count = file_handles[file_name]
                        
                        # æé€Ÿå†™å…¥ä¼˜åŒ–ï¼š
                        # 1. ä½¿ç”¨joinä¸€æ¬¡æ€§æ„å»ºå­—ç¬¦ä¸²ï¼ˆé¿å…åˆ—è¡¨æ¨å¯¼å¼çš„å†…å­˜å¼€é”€ï¼‰
                        # 2. å•æ¬¡writeæ¯”writelinesæ›´å¿«ï¼ˆå‡å°‘å‡½æ•°è°ƒç”¨ï¼‰
                        # 3. ç›´æ¥åœ¨å†…å­˜ä¸­æ„å»ºå®Œæ•´å­—ç¬¦ä¸²
                        if batch:  # ç¡®ä¿batchéç©º
                            fh.write('\n'.join(batch) + '\n')
                        
                        batch_count += 1
                        file_handles[file_name][1] = batch_count
                        total_written += len(batch)
                        
                        # æ¯Nä¸ªæ‰¹æ¬¡flushä¸€æ¬¡ï¼ˆå‡å°‘ç£ç›˜I/Oï¼‰
                        if batch_count >= commit_batches:
                            fh.flush()
                            file_handles[file_name][1] = 0  # é‡ç½®è®¡æ•°
                    
                    except Exception as write_error:
                        logger.error(f"å†™å…¥å¤±è´¥ {file_name}: {write_error}")
                        continue
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯3ç§’ï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 3:
                        elapsed = current_time - start_time
                        rate = total_written / elapsed if elapsed > 0 else 0
                        progress_pct = (completed_files / total_files * 100) if total_files > 0 else 0
                        
                        # ä¼°ç®—å‰©ä½™æ—¶é—´
                        if completed_files > 0:
                            avg_time_per_file = elapsed / completed_files
                            remaining_files = total_files - completed_files
                            eta_seconds = remaining_files * avg_time_per_file
                            eta_hours = int(eta_seconds / 3600)
                            eta_mins = int((eta_seconds % 3600) / 60)
                            eta_str = f"{eta_hours}å°æ—¶{eta_mins}åˆ†" if eta_hours > 0 else f"{eta_mins}åˆ†"
                        else:
                            eta_str = "è®¡ç®—ä¸­..."
                        
                        print(f"\rğŸ“Š [{completed_files}/{total_files}] {progress_pct:.1f}% | "
                              f"{total_written:,}æ¡ | {rate:.0f}æ¡/ç§’ | "
                              f"å‰©ä½™: {eta_str}    ", end='', flush=True)
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    
                    # å…³é—­å¹¶åˆ é™¤æ–‡ä»¶å¥æŸ„ï¼ˆé‡Šæ”¾èµ„æºï¼‰
                    if file_name in file_handles:
                        try:
                            fh, batch_count = file_handles[file_name]
                            if batch_count > 0:
                                fh.flush()
                            fh.close()
                        except Exception:
                            pass
                        finally:
                            del file_handles[file_name]
                    
                    # æ ‡è®°æ–‡ä»¶å®Œæˆ
                    tracker.mark_completed(file_name)
                    completed_files += 1
                
                elif item_type == 'error':
                    _, file_name, error = item
                    
                    # å…³é—­æ–‡ä»¶å¥æŸ„ï¼ˆå¦‚æœæœ‰ï¼‰
                    if file_name in file_handles:
                        try:
                            fh, _ = file_handles[file_name]
                            fh.close()
                        except Exception:
                            pass
                        finally:
                            del file_handles[file_name]
                    
                    # è®°å½•å¤±è´¥æ–‡ä»¶
                    failed_logger.log_failed(file_name, error)
                    completed_files += 1
            
            except Empty:
                continue
            except Exception as e:
                logger.error(f"[Writer] å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                continue
        
        # å…³é—­æ‰€æœ‰å‰©ä½™çš„æ–‡ä»¶å¥æŸ„
        for fh, batch_count in file_handles.values():
            try:
                if batch_count > 0:
                    fh.flush()
                fh.close()
            except Exception:
                pass  # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯
        
        stats_dict['written'] = total_written
        
    except Exception as e:
        logger.error(f"[Writer] ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ç¡®ä¿æ‰€æœ‰æ–‡ä»¶å¥æŸ„éƒ½è¢«å…³é—­ï¼ˆå³ä½¿å‘ç”Ÿå¼‚å¸¸ï¼‰
        for file_name, handle_info in list(file_handles.items()):
            try:
                fh, batch_count = handle_info
                if not fh.closed:
                    if batch_count > 0:
                        fh.flush()
                    fh.close()
            except Exception:
                pass  # å¿½ç•¥æ¸…ç†æ—¶çš„é”™è¯¯


# =============================================================================
# ä¸»å¤„ç†å‡½æ•°
# =============================================================================

def process_gz_folder_to_file(
    folder_path: str,
    folder_name: str,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    batch_size: int = BATCH_SIZE,
    commit_batches: int = 3
):
    """
    å¤„ç†GZæ–‡ä»¶å¤¹ï¼Œå°†è§£å‹åçš„æ•°æ®å†™å…¥æ–‡ä»¶ - æè‡´ä¼˜åŒ–ç‰ˆ
    å®Œå…¨æ¨¡æ‹Ÿ stream_gz_to_db_optimized.py çš„æ¶æ„å’Œä¼˜åŒ–ç­–ç•¥
    """
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    progress_file = OUTPUT_DIR / f"progress_{folder_name}.txt"
    failed_file = OUTPUT_DIR / f"failed_{folder_name}.txt"
    
    tracker = ProgressTracker(str(progress_file))
    failed_logger = FailedFilesLogger(str(failed_file))
    
    # åŠ è½½å·²å®Œæˆå’Œå¤±è´¥çš„æ–‡ä»¶
    completed_files = tracker.load_completed() if resume else set()
    failed_files = failed_logger.load_failed() if resume else set()
    
    # æ‰«æGZæ–‡ä»¶
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ°.gzæ–‡ä»¶: {folder_path}")
        return
    
    # è¿‡æ»¤å·²å®Œæˆçš„æ–‡ä»¶ï¼ˆç¡®ä¿æ–‡ä»¶åä¸€è‡´æ€§ï¼‰
    excluded_files = completed_files | failed_files
    pending_files = [(str(f.absolute()), f.name) for f in gz_files if f.name not in excluded_files]
    
    # ç²¾ç®€è¾“å‡º
    failed_info = f", å¤±è´¥: {len(failed_files)}" if failed_files else ""
    logger.info(f"\nâ–¶ [{folder_name}] æ€»è®¡: {len(gz_files)}, å·²å®Œæˆ: {len(completed_files)}{failed_info}, å¾…å¤„ç†: {len(pending_files)}")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼\n")
        return None
    
    overall_start = time.time()
    
    # åˆå§‹åŒ–è¿›ç¨‹å˜é‡ï¼ˆç¡®ä¿å¼‚å¸¸å¤„ç†ä¸­å¯ç”¨ï¼‰
    extractors = []
    writer = None
    
    try:
        # åˆ›å»ºé˜Ÿåˆ—
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
        manager = Manager()
        stats_dict = manager.dict()
        
        # æ·»åŠ æ–‡ä»¶ä»»åŠ¡
        for task in pending_files:
            file_queue.put(task)
        
        # æ·»åŠ ç»“æŸä¿¡å·
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨å†™å…¥è¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
        writer = Process(
            target=writer_worker,
            args=(data_queue, OUTPUT_DIR, folder_name, stats_dict, tracker, failed_logger, len(pending_files), commit_batches),
            name='Writer'
        )
        writer.start()
        
        # å¯åŠ¨è§£å‹è¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, batch_size),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        # ç­‰å¾…æ‰€æœ‰è§£å‹è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join(timeout=None)  # æ˜¾å¼è®¾ç½®timeout
        
        # å‘é€åœæ­¢ä¿¡å·ç»™å†™å…¥è¿›ç¨‹
        data_queue.put(('stop', None, None))
        
        # ç­‰å¾…å†™å…¥è¿›ç¨‹å®Œæˆï¼ˆè®¾ç½®åˆç†çš„è¶…æ—¶ï¼‰
        writer.join(timeout=60)  # æœ€å¤šç­‰å¾…60ç§’å†™å…¥è¿›ç¨‹å…³é—­
        if writer.is_alive():
            logger.warning("å†™å…¥è¿›ç¨‹æœªæ­£å¸¸å…³é—­ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
            writer.terminate()
            writer.join(timeout=5)
        
        elapsed = time.time() - overall_start
        total_written = stats_dict.get('written', 0)
        avg_rate = total_written / elapsed if elapsed > 0 else 0
        
        logger.info(f"\nâœ… [{folder_name}] å®Œæˆ: {len(pending_files)}ä¸ªæ–‡ä»¶, {total_written:,}æ¡, {elapsed/60:.1f}åˆ†é’Ÿ, {avg_rate:.0f}æ¡/ç§’\n")
        
        return {
            'folder': folder_name,
            'files': len(pending_files),
            'lines': total_written,
            'elapsed': elapsed,
            'rate': avg_rate
        }
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
        # æ¸…ç†è¿›ç¨‹
        if extractors:
            for p in extractors:
                if p.is_alive():
                    p.terminate()
        if writer and writer.is_alive():
            writer.terminate()
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        # æ¸…ç†è¿›ç¨‹
        if extractors:
            for p in extractors:
                if p.is_alive():
                    p.terminate()
        if writer and writer.is_alive():
            writer.terminate()
        sys.exit(1)


# =============================================================================
# æ‰¹é‡å¤„ç†å‡½æ•°
# =============================================================================

def batch_process_embeddings(
    base_dir: str, 
    num_extractors: int = NUM_EXTRACTORS,
    batch_size: int = BATCH_SIZE,
    commit_batches: int = 3
):
    """
    æ‰¹é‡å¤„ç†æ‰€æœ‰embeddingæ–‡ä»¶å¤¹ - æè‡´ä¼˜åŒ–ç‰ˆ
    å®Œå…¨æ¨¡æ‹Ÿ batch_process_machine.py çš„æ¶æ„
    """
    base_path = Path(base_dir)
    if not base_path.exists():
        logger.error(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        sys.exit(1)
    
    # embeddingæ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆå¯¹åº”machine1é…ç½®ï¼‰
    folders = [
        ('embeddings-specter_v1', 'embeddings_specter_v1'),
        ('embeddings-specter_v2', 'embeddings_specter_v2'),
    ]
    
    logger.info("="*80)
    logger.info("ğŸš€ æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼å†™å…¥æ–‡ä»¶æ€§èƒ½æµ‹è¯•")
    logger.info("="*80)
    logger.info(f"æ•°æ®æ ¹ç›®å½•: {base_dir}")
    logger.info(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    logger.info(f"è§£å‹è¿›ç¨‹æ•°: {num_extractors}")
    logger.info(f"æ‰¹é‡å¤§å°: {batch_size:,}æ¡")
    logger.info(f"å†™å…¥ç¼“å†²åŒº: {WRITE_BUFFER_SIZE/1024/1024:.0f}MB")
    logger.info(f"ç´¯ç§¯æ‰¹æ¬¡æ•°: {commit_batches} (å‡å°‘flushæ¬¡æ•°)")
    logger.info(f"å¾…å¤„ç†æ–‡ä»¶å¤¹: {len(folders)}")
    for folder_name, _ in folders:
        logger.info(f"  - {folder_name}")
    logger.info("="*80)
    
    overall_start = time.time()
    results = []
    success_count = 0
    failed_folders = []
    
    for i, (folder_name, table_name) in enumerate(folders, 1):
        folder_path = base_path / folder_name
        
        logger.info("")
        logger.info("="*80)
        logger.info(f"ğŸ“ [{i}/{len(folders)}] å¤„ç†æ–‡ä»¶å¤¹: {folder_name}")
        logger.info(f"è¾“å‡º: æ¯ä¸ªgzå¯¹åº”ä¸€ä¸ªtxtæ–‡ä»¶ï¼ˆé¿å…å•æ–‡ä»¶è¿‡å¤§ï¼‰")
        logger.info("="*80)
        
        if not folder_path.exists():
            logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡: {folder_path}")
            failed_folders.append(f"{folder_name} (ä¸å­˜åœ¨)")
            continue
        
        try:
            result = process_gz_folder_to_file(
                folder_path=str(folder_path),
                folder_name=table_name,
                num_extractors=num_extractors,
                resume=True,
                batch_size=batch_size,
                commit_batches=commit_batches
            )
            if result:
                results.append(result)
                success_count += 1
            
        except KeyboardInterrupt:
            logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
            logger.info(f"å·²å®Œæˆ: {success_count}/{len(folders)}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"âŒ {folder_name} å¤„ç†å¤±è´¥: {e}")
            failed_folders.append(f"{folder_name} ({str(e)})")
            continue
    
    # æ€»ç»“
    total_elapsed = time.time() - overall_start
    
    logger.info("")
    logger.info("="*80)
    logger.info("ğŸ æ€§èƒ½æµ‹è¯•å®Œæˆ")
    logger.info("="*80)
    logger.info(f"æ€»æ–‡ä»¶å¤¹æ•°: {len(folders)}")
    logger.info(f"æˆåŠŸ: {success_count}")
    logger.info(f"å¤±è´¥: {len(failed_folders)}")
    logger.info(f"æ€»è€—æ—¶: {total_elapsed/3600:.2f} å°æ—¶ ({total_elapsed/60:.1f} åˆ†é’Ÿ)")
    logger.info("")
    
    if results:
        logger.info("ğŸ“Š è¯¦ç»†ç»Ÿè®¡:")
        logger.info("-"*80)
        total_lines = 0
        total_files = 0
        for result in results:
            logger.info(f"  {result['folder']}:")
            logger.info(f"    æ–‡ä»¶æ•°: {result['files']}")
            logger.info(f"    æ•°æ®è¡Œ: {result['lines']:,}")
            logger.info(f"    è€—æ—¶: {result['elapsed']/60:.1f}åˆ†é’Ÿ")
            logger.info(f"    é€Ÿåº¦: {result['rate']:.0f}æ¡/ç§’")
            total_lines += result['lines']
            total_files += result['files']
        logger.info("-"*80)
        logger.info(f"  æ€»è®¡:")
        logger.info(f"    æ–‡ä»¶æ•°: {total_files}")
        logger.info(f"    æ•°æ®è¡Œ: {total_lines:,}")
        logger.info(f"    å¹³å‡é€Ÿåº¦: {total_lines/total_elapsed:.0f}æ¡/ç§’")
    
    if failed_folders:
        logger.warning("")
        logger.warning("âš ï¸  ä»¥ä¸‹æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥:")
        for folder in failed_folders:
            logger.warning(f"  - {folder}")
    
    logger.info("="*80)
    logger.info(f"\nâœ… æµ‹è¯•æ–‡ä»¶å·²ä¿å­˜åˆ°: {OUTPUT_DIR}")
    
    if success_count == len(folders):
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å¤¹å¤„ç†æˆåŠŸï¼")
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼å†™å…¥æ–‡ä»¶æ€§èƒ½æµ‹è¯•ï¼ˆç›®æ ‡ï¼šæœ€å¤§åŒ–å†™å…¥é€Ÿåº¦ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼š
  ğŸš€ ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œå†™å…¥å®Œå…¨å¹¶è¡Œ
  ğŸš€ writelinesæ‰¹é‡å†™å…¥ï¼šæ¯”å¾ªç¯writeå¿«10-20%
  ğŸš€ è¶…å¤§ç¼“å†²åŒºï¼š512MBå†™å…¥ç¼“å†²åŒºï¼Œ16MBè§£å‹ç¼“å†²åŒº
  ğŸš€ å»¶è¿Ÿflushï¼šç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡åæ‰flushï¼Œå‡å°‘ç£ç›˜I/O
  ğŸš€ å•æ–‡ä»¶åˆå¹¶ï¼šæ‰€æœ‰æ•°æ®å†™å…¥å•ä¸ªå¤§æ–‡ä»¶ï¼Œé¿å…å¤šæ–‡ä»¶å¼€é”€
  ğŸš€ æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­æ¢å¤

æµ‹è¯•ç›®æ ‡ï¼š
  - è¯„ä¼°çº¯I/Oæ€§èƒ½ï¼ˆè§£å‹ + æ–‡ä»¶å†™å…¥ï¼‰
  - é¢„æœŸé€Ÿåº¦ï¼š5000-10000æ¡/ç§’ï¼ˆæ— æ•°æ®åº“å¼€é”€ï¼‰
  - å¯¹æ¯”æ•°æ®åº“æ’å…¥æ€§èƒ½ï¼Œæ‰¾å‡ºç“¶é¢ˆ

è¾“å‡ºè¯´æ˜ï¼š
  - æ¯ä¸ªgzæ–‡ä»¶å¯¹åº”ä¸€ä¸ªtxtæ–‡ä»¶ï¼ˆé¿å…å•æ–‡ä»¶è¿‡å¤§ï¼Œembeddingè§£å‹åçº¦3GB/æ–‡ä»¶ï¼‰
  - è¾“å‡ºç›®å½•ï¼šE:\\test
  - å®æ—¶æ˜¾ç¤ºè¿›åº¦ã€é€Ÿåº¦ã€é¢„ä¼°å‰©ä½™æ—¶é—´

ç¤ºä¾‹ï¼š
  # æ‰¹é‡æµ‹è¯•æ‰€æœ‰embeddingæ–‡ä»¶å¤¹ï¼ˆæ¨èï¼‰
  python scripts/test_gz_to_file_speed.py --base-dir "E:\\2025-09-30" --batch
  
  # è‡ªå®šä¹‰é…ç½®
  python scripts/test_gz_to_file_speed.py --base-dir "E:\\2025-09-30" --batch --batch-size 20000 --commit-batches 5
  
  # æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤¹
  python scripts/test_gz_to_file_speed.py --dir "E:\\2025-09-30\\embeddings-specter_v1" --folder embeddings_specter_v1
        """
    )
    
    parser.add_argument('--dir', type=str,
                       help='GZæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå•ä¸ªæ–‡ä»¶å¤¹æµ‹è¯•ï¼‰')
    parser.add_argument('--folder', type=str,
                       help='æ–‡ä»¶å¤¹åç§°ï¼ˆç”¨äºè¾“å‡ºæ–‡ä»¶å‘½åï¼‰')
    parser.add_argument('--base-dir', type=str,
                       help='æ•°æ®æ ¹ç›®å½•ï¼ˆæ‰¹é‡æµ‹è¯•æ‰€æœ‰embeddingæ–‡ä»¶å¤¹ï¼‰')
    parser.add_argument('--batch', action='store_true',
                       help='æ‰¹é‡å¤„ç†æ‰€æœ‰embeddingæ–‡ä»¶å¤¹')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--batch-size', type=int, default=BATCH_SIZE,
                       help=f'æ‰¹é‡å†™å…¥å¤§å°ï¼ˆé»˜è®¤: {BATCH_SIZE:,}ï¼‰')
    parser.add_argument('--commit-batches', type=int, default=3,
                       help='ç´¯ç§¯Nä¸ªæ‰¹æ¬¡åæ‰flushï¼ˆé»˜è®¤: 3ï¼Œå‡å°‘ç£ç›˜I/Oï¼‰')
    
    args = parser.parse_args()
    
    if args.batch and args.base_dir:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        batch_process_embeddings(
            base_dir=args.base_dir,
            num_extractors=args.extractors,
            batch_size=args.batch_size,
            commit_batches=args.commit_batches
        )
    elif args.dir and args.folder:
        # å•ä¸ªæ–‡ä»¶å¤¹æµ‹è¯•æ¨¡å¼
        process_gz_folder_to_file(
            folder_path=args.dir,
            folder_name=args.folder,
            num_extractors=args.extractors,
            resume=True,
            batch_size=args.batch_size,
            commit_batches=args.commit_batches
        )
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()

