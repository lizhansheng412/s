#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†è„šæœ¬ - è‡ªåŠ¨å¤„ç†è¯¥æœºå™¨åˆ†é…çš„æ‰€æœ‰æ–‡ä»¶å¤¹
æ”¯æŒ3å°æœºå™¨çš„åˆ†å¸ƒå¼å¹¶è¡Œå¤„ç†
"""

import sys
import time
import logging
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from machine_config import get_machine_config, FOLDER_TO_TABLE_MAP
from scripts.stream_gz_to_db_optimized import process_gz_folder_pipeline, NUM_EXTRACTORS

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def batch_process_machine(
    machine_id: str,
    base_dir: str,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    use_upsert: bool = False  # é»˜è®¤ä½¿ç”¨INSERTæ¨¡å¼ï¼ˆæ›´å¿«ï¼‰
):
    """
    æ‰¹é‡å¤„ç†è¯¥æœºå™¨åˆ†é…çš„æ‰€æœ‰æ–‡ä»¶å¤¹
    
    Args:
        machine_id: æœºå™¨ID ('machine1', 'machine2', 'machine3')
        base_dir: S2ORCæ•°æ®æ ¹ç›®å½•ï¼ˆåŒ…å«æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼‰
        num_extractors: è§£å‹è¿›ç¨‹æ•°
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
    """
    # è·å–æœºå™¨é…ç½®
    config = get_machine_config(machine_id)
    folders = config['folders']
    tables = config['tables']
    
    logger.info("="*80)
    logger.info(f"ğŸš€ æ‰¹é‡å¤„ç†å¯åŠ¨")
    logger.info("="*80)
    logger.info(f"æœºå™¨ID: {machine_id}")
    logger.info(f"é…ç½®: {config['description']}")
    logger.info(f"æ•°æ®æ ¹ç›®å½•: {base_dir}")
    logger.info(f"å¾…å¤„ç†æ–‡ä»¶å¤¹: {len(folders)}")
    for folder, table in zip(folders, tables):
        logger.info(f"  - {folder} â†’ {table}")
    logger.info(f"è§£å‹è¿›ç¨‹æ•°: {num_extractors}")
    logger.info(f"æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
    logger.info("="*80)
    logger.info("")
    
    base_path = Path(base_dir)
    if not base_path.exists():
        logger.error(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        sys.exit(1)
    
    overall_start = time.time()
    total_processed = 0
    success_count = 0
    failed_folders = []
    
    for i, (folder_name, table_name) in enumerate(zip(folders, tables), 1):
        folder_path = base_path / folder_name
        
        logger.info("")
        logger.info("="*80)
        logger.info(f"ğŸ“ [{i}/{len(folders)}] å¤„ç†æ–‡ä»¶å¤¹: {folder_name}")
        logger.info(f"ç›®æ ‡è¡¨: {table_name}")
        logger.info("="*80)
        logger.info("")
        
        if not folder_path.exists():
            logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡: {folder_path}")
            failed_folders.append(f"{folder_name} (ä¸å­˜åœ¨)")
            continue
        
        try:
            # å¤„ç†è¯¥æ–‡ä»¶å¤¹ï¼ˆä¸»é”®å­—æ®µæ ¹æ®è¡¨åè‡ªåŠ¨ç¡®å®šï¼‰
            process_gz_folder_pipeline(
                folder_path=str(folder_path),
                table_name=table_name,
                use_upsert=use_upsert,
                num_extractors=num_extractors,
                resume=resume,
                reset_progress=False
            )
            
            success_count += 1
            logger.info(f"âœ… [{i}/{len(folders)}] {folder_name} å¤„ç†å®Œæˆ\n")
            
        except KeyboardInterrupt:
            logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
            logger.info(f"å·²å®Œæˆ: {success_count}/{len(folders)}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"âŒ [{i}/{len(folders)}] {folder_name} å¤„ç†å¤±è´¥: {e}")
            failed_folders.append(f"{folder_name} ({str(e)})")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶å¤¹
            continue
    
    # æ€»ç»“
    elapsed = time.time() - overall_start
    
    logger.info("")
    logger.info("="*80)
    logger.info("ğŸ æ‰¹é‡å¤„ç†å®Œæˆ")
    logger.info("="*80)
    logger.info(f"æœºå™¨ID: {machine_id}")
    logger.info(f"æ€»æ–‡ä»¶å¤¹æ•°: {len(folders)}")
    logger.info(f"æˆåŠŸ: {success_count}")
    logger.info(f"å¤±è´¥: {len(failed_folders)}")
    logger.info(f"æ€»è€—æ—¶: {elapsed/3600:.2f} å°æ—¶")
    
    if failed_folders:
        logger.warning("")
        logger.warning("âš ï¸  ä»¥ä¸‹æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥:")
        for folder in failed_folders:
            logger.warning(f"  - {folder}")
    
    logger.info("="*80)
    logger.info("")
    
    if success_count == len(folders):
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å¤¹å¤„ç†æˆåŠŸï¼")
        logger.info("ä¸‹ä¸€æ­¥ï¼šç­‰å¾…å…¶ä»–æœºå™¨å®Œæˆï¼Œç„¶åè¿›è¡Œæ•°æ®åˆå¹¶")
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡å¤„ç†è¯¥æœºå™¨åˆ†é…çš„æ‰€æœ‰æ–‡ä»¶å¤¹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æœºå™¨é…ç½®ï¼š
  machine1: embeddings-specter_v1, s2orc
  machine2: embeddings-specter_v2, s2orc_v2
  machine3: abstracts, authors, citations, paper_ids, papers, publication_venues, tldrs

ç¤ºä¾‹ï¼š
  # ç”µè„‘1ï¼šå¤„ç†åˆ†é…çš„æ–‡ä»¶å¤¹
  python scripts/batch_process_machine.py --machine machine1 --base-dir "E:\\S2ORC_Data"
  
  # ç”µè„‘2
  python scripts/batch_process_machine.py --machine machine2 --base-dir "F:\\S2ORC_Data"
  
  # ç”µè„‘3
  python scripts/batch_process_machine.py --machine machine3 --base-dir "D:\\S2ORC_Data"
  
  # è‡ªå®šä¹‰è§£å‹è¿›ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°ï¼‰
  python scripts/batch_process_machine.py --machine machine1 --base-dir "E:\\data" --extractors 12
        """
    )
    
    parser.add_argument('--machine', type=str, required=True,
                       choices=['machine1', 'machine2', 'machine3'],
                       help='æœºå™¨ID')
    parser.add_argument('--base-dir', type=str, required=True,
                       help='S2ORCæ•°æ®æ ¹ç›®å½•ï¼ˆåŒ…å«æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼‰')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                       help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆä»å¤´å¼€å§‹ï¼‰')
    parser.add_argument('--upsert', action='store_true',
                       help='ä½¿ç”¨UPSERTæ¨¡å¼ï¼ˆå¤„ç†é‡å¤æ•°æ®ï¼‰')
    
    args = parser.parse_args()
    
    batch_process_machine(
        machine_id=args.machine,
        base_dir=args.base_dir,
        num_extractors=args.extractors,
        resume=not args.no_resume,
        use_upsert=args.upsert
    )


if __name__ == '__main__':
    main()

