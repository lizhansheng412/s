#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥
æ€§èƒ½ä¼˜åŒ–ï¼š
  âœ… ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨åˆ†ç¦»å¹¶è¡Œ
  âœ… æ­£åˆ™å¿«é€Ÿæå–corpusidï¼šé¿å…å®Œæ•´JSONè§£æ
  âœ… æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  âœ… é˜Ÿåˆ—ç¼“å†²ï¼šè§£å‹å¿«æ—¶ä¸ç­‰å¾…ï¼Œæ’å…¥å¿«æ—¶ä¸ç©ºé—²
  âœ… æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­æ¢å¤
  
ç›®æ ‡ï¼š10å€æ€§èƒ½æå‡ï¼ˆ3000-5000æ¡/ç§’ï¼‰
"""

import gzip
import re
import sys
import time
import logging
from pathlib import Path
from typing import Optional, Set
from multiprocessing import Process, Queue, Manager, cpu_count
from queue import Empty

import psycopg2

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from database.config.db_config_v2 import DB_CONFIG, FIELD_TABLES

# =============================================================================
# é…ç½®ä¼˜åŒ–
# =============================================================================

BATCH_SIZE = 150000                     # æ›´å¤§æ‰¹æ¬¡ï¼Œå‡å°‘commitæ¬¡æ•°
QUEUE_SIZE = 20                         # é˜Ÿåˆ—å®¹é‡ï¼ˆå‡å°ï¼ŒèŠ‚çœå†…å­˜ï¼‰
NUM_EXTRACTORS = 8                      # è§£å‹è¿›ç¨‹æ•°ï¼ˆæ¨è8ä¸ªï¼‰
PROGRESS_FILE = 'logs/gz_progress.txt'
COMMIT_BATCHES = 3                      # æ¯3ä¸ªæ‰¹æ¬¡commitä¸€æ¬¡ï¼ˆå‡å°‘commitå¼€é”€ï¼‰

# æ­£åˆ™è¡¨è¾¾å¼ï¼šå¿«é€Ÿæå–corpusidï¼ˆæ¯”å®Œæ•´JSONè§£æå¿«10å€ï¼‰
CORPUSID_PATTERN = re.compile(r'"corpusid"\s*:\s*(\d+)', re.IGNORECASE)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(processName)s] - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


# =============================================================================
# æ–­ç‚¹ç»­ä¼ 
# =============================================================================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        
        completed = set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    completed.add(line)
        
        if completed:
            logger.info(f"âœ“ å·²åŠ è½½è¿›åº¦: {len(completed)} ä¸ªæ–‡ä»¶å·²å®Œæˆ")
        return completed
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


# =============================================================================
# ç”Ÿäº§è€…ï¼šè§£å‹è¿›ç¨‹ï¼ˆå¤šä¸ªå¹¶è¡Œï¼‰
# =============================================================================

def extractor_worker(
    file_queue: Queue,
    data_queue: Queue,
    stats_dict: dict,
    corpusid_key: str = 'corpusid'
):
    """
    è§£å‹å·¥ä½œè¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
    ä»file_queueå–æ–‡ä»¶ï¼Œè§£å‹åæ”¾å…¥data_queue
    """
    worker_name = f"Extractor-{id(file_queue) % 1000}"
    
    while True:
        try:
            # è·å–æ–‡ä»¶ä»»åŠ¡
            task = file_queue.get(timeout=1)
            if task is None:  # ç»“æŸä¿¡å·
                break
            
            gz_file_path, file_name = task
            start_time = time.time()
            
            logger.info(f"[{worker_name}] ğŸ”„ è§£å‹: {file_name}")
            
            try:
                batch = []
                line_count = 0
                valid_count = 0
                
                # æµå¼è§£å‹
                with gzip.open(gz_file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                    for line in f:
                        line_count += 1
                        line = line.strip()
                        if not line:
                            continue
                        
                        # æ­£åˆ™å¿«é€Ÿæå–corpusidï¼ˆé¿å…å®Œæ•´JSONè§£æï¼‰
                        match = CORPUSID_PATTERN.search(line)
                        if not match:
                            continue
                        
                        corpusid = int(match.group(1))
                        valid_count += 1
                        
                        # é›¶æ‹·è´ï¼šç›´æ¥ä½¿ç”¨åŸå§‹JSONå­—ç¬¦ä¸²
                        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ç”¨äºCOPY
                        json_escaped = line.replace('\\', '\\\\').replace('\n', '\\n').replace('\t', '\\t')
                        
                        batch.append((corpusid, json_escaped))
                        
                        # æ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€åˆ°é˜Ÿåˆ—
                        if len(batch) >= BATCH_SIZE:
                            data_queue.put(('data', file_name, batch))
                            batch = []
                
                # å‘é€å‰©ä½™æ•°æ®
                if batch:
                    data_queue.put(('data', file_name, batch))
                
                # å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                data_queue.put(('done', file_name, valid_count))
                
                elapsed = time.time() - start_time
                rate = valid_count / elapsed if elapsed > 0 else 0
                
                logger.info(f"[{worker_name}] âœ… {file_name}: æå– {valid_count:,} æ¡ ({rate:.0f}æ¡/ç§’)")
                
                # æ›´æ–°ç»Ÿè®¡
                stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                
            except Exception as e:
                logger.error(f"[{worker_name}] âŒ {file_name}: {e}")
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception as e:
            logger.error(f"[{worker_name}] å¼‚å¸¸: {e}")
            break


# =============================================================================
# æ¶ˆè´¹è€…ï¼šæ’å…¥è¿›ç¨‹ï¼ˆå•ä¸ªï¼Œé«˜æ•ˆæ‰¹é‡æ’å…¥ï¼‰
# =============================================================================

def inserter_worker(
    data_queue: Queue,
    table_name: str,
    stats_dict: dict,
    tracker: ProgressTracker,
    use_upsert: bool = False
):
    """
    æ’å…¥å·¥ä½œè¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
    æŒç»­ä»data_queueå–æ•°æ®å¹¶æ‰¹é‡æ’å…¥
    """
    logger.info("[Inserter] ğŸš€ å¯åŠ¨æ’å…¥è¿›ç¨‹")
    
    try:
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        conn = psycopg2.connect(**DB_CONFIG)
        conn.autocommit = False
        cursor = conn.cursor()
        
        # ç¦ç”¨è§¦å‘å™¨ï¼ˆINSERTæ¨¡å¼ï¼‰
        if not use_upsert:
            cursor.execute(f"ALTER TABLE {table_name} DISABLE TRIGGER ALL")
            conn.commit()
            logger.info("[Inserter] âœ“ è§¦å‘å™¨å·²ç¦ç”¨")
        
        total_inserted = 0
        file_stats = {}  # {file_name: inserted_count}
        last_log_time = time.time()
        start_time = time.time()
        batch_count = 0  # æ‰¹æ¬¡è®¡æ•°å™¨
        
        while True:
            try:
                # éé˜»å¡è·å–ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
                item = data_queue.get(timeout=5)
                
                item_type = item[0]
                
                if item_type == 'stop':
                    logger.info("[Inserter] æ”¶åˆ°åœæ­¢ä¿¡å·")
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    # æ‰¹é‡æ’å…¥
                    inserted = batch_insert_copy(cursor, table_name, batch, use_upsert)
                    batch_count += 1
                    
                    # æ¯Nä¸ªæ‰¹æ¬¡commitä¸€æ¬¡ï¼Œå‡å°‘commitå¼€é”€
                    if batch_count >= COMMIT_BATCHES:
                        conn.commit()
                        batch_count = 0
                    
                    total_inserted += inserted
                    file_stats[file_name] = file_stats.get(file_name, 0) + inserted
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯10ç§’ï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 10:
                        elapsed = current_time - start_time
                        rate = total_inserted / elapsed if elapsed > 0 else 0
                        logger.info(f"[Inserter] ğŸ“Š å·²æ’å…¥: {total_inserted:,} æ¡ ({rate:.0f}æ¡/ç§’) | é˜Ÿåˆ—: {data_queue.qsize()}")
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    # æ ‡è®°æ–‡ä»¶å®Œæˆ
                    tracker.mark_completed(file_name)
                    inserted = file_stats.get(file_name, 0)
                    logger.info(f"[Inserter] âœ… {file_name}: å·²å®Œæˆ ({inserted:,} æ¡)")
                
                elif item_type == 'error':
                    _, file_name, error = item
                    logger.warning(f"[Inserter] âš ï¸  {file_name}: æå–å¤±è´¥ - {error}")
            
            except Empty:
                # é˜Ÿåˆ—ç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"[Inserter] å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                conn.rollback()
                continue
        
        # æäº¤å‰©ä½™æœªcommitçš„æ•°æ®
        if batch_count > 0:
            conn.commit()
        
        # å¯ç”¨è§¦å‘å™¨
        if not use_upsert:
            cursor.execute(f"ALTER TABLE {table_name} ENABLE TRIGGER ALL")
            conn.commit()
            logger.info("[Inserter] âœ“ è§¦å‘å™¨å·²å¯ç”¨")
        
        elapsed = time.time() - start_time
        rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"\n[Inserter] ğŸ æ’å…¥å®Œæˆ: {total_inserted:,} æ¡ï¼Œå¹³å‡ {rate:.0f} æ¡/ç§’\n")
        
        cursor.close()
        conn.close()
        
        stats_dict['inserted'] = total_inserted
        
    except Exception as e:
        logger.error(f"[Inserter] ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def batch_insert_copy(cursor, table_name: str, batch: list, use_upsert: bool = False) -> int:
    """
    ä½¿ç”¨COPYæ‰¹é‡æ’å…¥ï¼ˆæœ€å¿«æ–¹æ³•ï¼‰
    """
    if not batch:
        return 0
    
    try:
        if use_upsert:
            # UPSERTæ¨¡å¼
            from io import StringIO
            buffer = StringIO()
            for corpusid, json_str in batch:
                buffer.write(f"{corpusid}\t{json_str}\n")
            buffer.seek(0)
            
            cursor.execute("CREATE TEMP TABLE IF NOT EXISTS temp_batch (corpusid BIGINT, data JSONB) ON COMMIT DROP")
            cursor.copy_expert(
                "COPY temp_batch (corpusid, data) FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t')",
                buffer
            )
            cursor.execute(f"""
                INSERT INTO {table_name} (corpusid, data)
                SELECT corpusid, data FROM temp_batch
                ON CONFLICT (corpusid) DO UPDATE SET
                    data = EXCLUDED.data,
                    update_time = NOW()
                WHERE {table_name}.data IS DISTINCT FROM EXCLUDED.data
            """)
        else:
            # INSERTæ¨¡å¼ï¼šç›´æ¥COPYï¼ˆæœ€å¿«ï¼‰
            from io import StringIO
            buffer = StringIO()
            for corpusid, json_str in batch:
                buffer.write(f"{corpusid}\t{json_str}\t\\N\t\\N\n")
            buffer.seek(0)
            
            cursor.copy_expert(
                f"""
                COPY {table_name} (corpusid, data, insert_time, update_time)
                FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')
                """,
                buffer
            )
        
        return len(batch)
        
    except Exception as e:
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        raise


# =============================================================================
# ä¸»åè°ƒå™¨
# =============================================================================

def process_gz_folder_pipeline(
    folder_path: str,
    table_name: str,
    corpusid_key: str = 'corpusid',
    use_upsert: bool = False,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    reset_progress: bool = False
):
    """
    æµæ°´çº¿å¹¶è¡Œå¤„ç†ï¼šå¤šä¸ªè§£å‹è¿›ç¨‹ + å•ä¸ªæ’å…¥è¿›ç¨‹
    """
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    tracker = ProgressTracker(PROGRESS_FILE)
    
    if reset_progress:
        tracker.reset()
    
    completed_files = tracker.load_completed() if resume else set()
    
    # æ‰«æGZæ–‡ä»¶
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ°.gzæ–‡ä»¶: {folder_path}")
        return
    
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in completed_files]
    
    logger.info(f"\n{'='*80}")
    logger.info(f"æµæ°´çº¿å¹¶è¡Œå¤„ç† GZ æ–‡ä»¶ï¼ˆç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼‰")
    logger.info(f"{'='*80}")
    logger.info(f"æ–‡ä»¶å¤¹: {folder_path}")
    logger.info(f"ç›®æ ‡è¡¨: {table_name}")
    logger.info(f"æ€»æ–‡ä»¶æ•°: {len(gz_files)}")
    logger.info(f"å·²å®Œæˆ: {len(completed_files)}")
    logger.info(f"å¾…å¤„ç†: {len(pending_files)}")
    logger.info(f"è§£å‹è¿›ç¨‹: {num_extractors}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE:,}")
    logger.info(f"é˜Ÿåˆ—å®¹é‡: {QUEUE_SIZE} æ‰¹æ¬¡")
    logger.info(f"æ¨¡å¼: {'UPSERT' if use_upsert else 'INSERT (COPY)'}")
    logger.info(f"{'='*80}\n")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
        return
    
    overall_start = time.time()
    
    try:
        # åˆ›å»ºé˜Ÿåˆ—
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
        manager = Manager()
        stats_dict = manager.dict()
        
        # æ·»åŠ æ–‡ä»¶ä»»åŠ¡
        for task in pending_files:
            file_queue.put(task)
        
        # æ·»åŠ ç»“æŸä¿¡å·ï¼ˆæ¯ä¸ªè§£å‹è¿›ç¨‹ä¸€ä¸ªï¼‰
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨æ’å…¥è¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
        inserter = Process(
            target=inserter_worker,
            args=(data_queue, table_name, stats_dict, tracker, use_upsert),
            name='Inserter'
        )
        inserter.start()
        
        # å¯åŠ¨è§£å‹è¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, corpusid_key),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        logger.info(f"âœ“ å·²å¯åŠ¨ {num_extractors} ä¸ªè§£å‹è¿›ç¨‹ + 1 ä¸ªæ’å…¥è¿›ç¨‹\n")
        
        # ç­‰å¾…æ‰€æœ‰è§£å‹è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join()
        
        logger.info("âœ“ æ‰€æœ‰è§£å‹è¿›ç¨‹å·²å®Œæˆ")
        
        # å‘é€åœæ­¢ä¿¡å·ç»™æ’å…¥è¿›ç¨‹
        data_queue.put(('stop', None, None))
        
        # ç­‰å¾…æ’å…¥è¿›ç¨‹å®Œæˆ
        inserter.join()
        
        elapsed = time.time() - overall_start
        total_inserted = stats_dict.get('inserted', 0)
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… å…¨éƒ¨å®Œæˆï¼")
        logger.info(f"  æ–‡ä»¶æ•°: {len(pending_files)}")
        logger.info(f"  æ€»æ’å…¥: {total_inserted:,} æ¡")
        logger.info(f"  æ€»è€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"  å¹³å‡é€Ÿåº¦: {avg_rate:.0f} æ¡/ç§’")
        logger.info(f"{'='*80}\n")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥ï¼ˆç›®æ ‡ï¼š3000-5000æ¡/ç§’ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼š
  ğŸš€ ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨å¹¶è¡Œ
  ğŸš€ æ­£åˆ™å¿«é€Ÿæå–ï¼šé¿å…å®Œæ•´JSONè§£æï¼ˆæé€Ÿ10å€ï¼‰
  ğŸš€ æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  ğŸš€ é˜Ÿåˆ—ç¼“å†²ï¼šæŒç»­ä¾›åº”æ•°æ®ï¼Œæ— ç©ºé—²ç­‰å¾…
  ğŸš€ å¤šè¿›ç¨‹è§£å‹ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPU

ç¤ºä¾‹ï¼š
  # æé€Ÿå¤„ç†papersæ–‡ä»¶å¤¹
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\papers" --table papers
  
  # è‡ªå®šä¹‰è§£å‹è¿›ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\s2orc" --table s2orc --extractors 8
  
  # ä¸­æ–­åç»§ç»­
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\papers" --table papers --resume
        """
    )
    
    parser.add_argument('--dir', type=str, required=True,
                       help='GZæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--table', type=str, required=True,
                       choices=FIELD_TABLES,
                       help='ç›®æ ‡æ•°æ®åº“è¡¨å')
    parser.add_argument('--key', type=str, default='corpusid',
                       help='corpusidå­—æ®µåï¼ˆé»˜è®¤: corpusidï¼‰')
    parser.add_argument('--upsert', action='store_true',
                       help='ä½¿ç”¨UPSERTæ¨¡å¼')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--resume', action='store_true',
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true',
                       help='é‡ç½®è¿›åº¦')
    
    parser.set_defaults(resume=True)
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¤„ç†
    process_gz_folder_pipeline(
        folder_path=args.dir,
        table_name=args.table,
        corpusid_key=args.key,
        use_upsert=args.upsert,
        num_extractors=args.extractors,
        resume=args.resume,
        reset_progress=args.reset
    )


if __name__ == '__main__':
    main()

