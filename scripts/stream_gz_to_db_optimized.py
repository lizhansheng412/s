#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥
æ€§èƒ½ä¼˜åŒ–ï¼š
  âœ… ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨åˆ†ç¦»å¹¶è¡Œ
  âœ… æ­£åˆ™å¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼šé¿å…å®Œæ•´JSONè§£æ
  âœ… æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  âœ… é˜Ÿåˆ—ç¼“å†²ï¼šè§£å‹å¿«æ—¶ä¸ç­‰å¾…ï¼Œæ’å…¥å¿«æ—¶ä¸ç©ºé—²
  âœ… æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­æ¢å¤
  âœ… çµæ´»ä¸»é”®é…ç½®ï¼šä¸åŒè¡¨ä½¿ç”¨ä¸åŒä¸»é”®å­—æ®µ
     - authorsè¡¨: authorid (JSONå­—ç¬¦ä¸²â†’DB BIGINT)
     - citationsè¡¨: idï¼ˆè‡ªå¢ï¼‰ï¼Œé¢å¤–å­—æ®µcitingcorpusidï¼ˆå…è®¸é‡å¤ï¼‰
     - publication_venuesè¡¨: publicationvenueid (JSONå­—ç¬¦ä¸²â†’DB TEXTï¼Œå€¼ä»idæå–)
     - å…¶ä»–è¡¨: corpusid (JSONæ•°å­—â†’DB BIGINT)
  
ç›®æ ‡ï¼š10å€æ€§èƒ½æå‡ï¼ˆ3000-5000æ¡/ç§’ï¼‰
"""

import gzip
import re
import sys
import time
import logging
from pathlib import Path
from typing import Optional, Set
from multiprocessing import Process, Queue, Manager, cpu_count
from queue import Empty

import psycopg2

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from database.config import db_config_v2
from database.config.db_config_v2 import FIELD_TABLES

# =============================================================================
# æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€é…ç½®ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
# =============================================================================

# é’ˆå¯¹TEXTç±»å‹ä¼˜åŒ–é…ç½®ï¼ˆå½»åº•é¿å…ç¼“å†²åŒºé—®é¢˜ï¼šä¸ä½¿ç”¨ä¸´æ—¶è¡¨ï¼Œä½¿ç”¨VALUESï¼‰
TABLE_CONFIGS = {
    # è¶…å¤§æ•°æ® (60-120KB/æ¡): s2orcç³»åˆ— 
    's2orc': {'batch_size': 1000, 'commit_batches': 3, 'extractors': 1},
    's2orc_v2': {'batch_size': 1000, 'commit_batches': 3, 'extractors': 1},
    # 1000æ¡Ã—100KBÃ—3æ‰¹=300MB/æ‰¹æ¬¡
        
    # ä¸­ç­‰æ•°æ® (16KB/æ¡): embeddingsç³»åˆ— 
    'embeddings_specter_v1': {'batch_size': 6250, 'commit_batches': 3, 'extractors': 1},
    'embeddings_specter_v2': {'batch_size': 6250, 'commit_batches': 3, 'extractors': 1},
    # 6250æ¡Ã—16KBÃ—3æ‰¹=300MB/æ‰¹æ¬¡
    
    # å°æ•°æ® (1-3KB/æ¡): å…¶ä»–è¡¨
    'papers': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'abstracts': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'authors': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'citations': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 1},
    'publication_venues': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'tldrs': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    # 50,000æ¡Ã—2KBÃ—3æ‰¹=300MB/æ‰¹æ¬¡
    
    # æé€Ÿæ¨¡å¼ï¼špaper_idsï¼ˆåªæœ‰ä¸€ä¸ªBIGINTå­—æ®µï¼Œ8å­—èŠ‚/æ¡ï¼‰
    # å¤§æ•°æ®é‡ä¼˜åŒ–ï¼šæ‰¹æ¬¡å¢å¤§ä»¥åˆ†æ‘Šç´¢å¼•ç»´æŠ¤å¼€é”€
    'paper_ids': {'batch_size': 1000000, 'commit_batches': 1, 'extractors': 3},
    # 100ä¸‡æ¡Ã—8å­—èŠ‚Ã—1æ‰¹=8MB/æ‰¹æ¬¡ï¼ˆæå°ï¼Œæé€Ÿï¼Œå‡å°‘commitæ¬¡æ•°ï¼‰
}

DEFAULT_CONFIG = {'batch_size': 20000, 'commit_batches': 1, 'extractors': 1}
NUM_EXTRACTORS = 1
QUEUE_SIZE = 20
# æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå°†æ ¹æ®è¡¨ååŠ¨æ€ç”Ÿæˆï¼‰
PROGRESS_DIR = 'logs/progress'
FAILED_DIR = 'logs/failed'
ALWAYS_FAILED_DIR = 'logs/always_failed'

# ä¸åŒè¡¨ä½¿ç”¨ä¸åŒçš„ä¸»é”®å­—æ®µï¼ˆæ•°æ®åº“å­—æ®µåï¼‰
TABLE_PRIMARY_KEY_MAP = {
    'authors': 'authorid',
    'publication_venues': 'publicationvenueid',
    # citationsè¡¨ä½¿ç”¨è‡ªå¢ä¸»é”®ï¼Œæ— éœ€åœ¨æ­¤é…ç½®
    # å…¶ä»–è¡¨é»˜è®¤ä½¿ç”¨corpusid
}

# JSONä¸­ä¸»é”®å­—æ®µæ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„è¡¨ï¼ˆéœ€è¦ç”¨å¼•å·åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼ï¼‰
JSON_STRING_KEY_TABLES = {
    'authors',  # JSON: "authorid":"5232161" (å¸¦å¼•å·)ï¼ŒDB: BIGINT (éœ€è½¬æ¢)
    'publication_venues'  # JSON: "id":"uuid-..." (å¸¦å¼•å·)ï¼ŒDB: TEXT (ä¿æŒå­—ç¬¦ä¸²)
}

# åªéœ€æ’å…¥ä¸»é”®çš„è¡¨ï¼ˆæ— dataå­—æ®µï¼‰
NO_DATA_TABLES = {
    'paper_ids'  # åªæ’å…¥corpusidï¼Œå…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤å€¼
}

# æ•°æ®åº“ä¸­ä¸»é”®æ˜¯TEXTç±»å‹çš„è¡¨ï¼ˆæ’å…¥æ—¶ä¸éœ€è¦è½¬æ¢ï¼‰
DB_TEXT_KEY_TABLES = {
    'publication_venues'  # publicationvenueidæ˜¯TEXTç±»å‹ï¼ˆUUIDå­—ç¬¦ä¸²ï¼‰
}

# JSONå­—æ®µååˆ°æ•°æ®åº“å­—æ®µåçš„æ˜ å°„ï¼ˆç”¨äºæ­£åˆ™æå–ï¼‰
JSON_FIELD_MAP = {
    'citations': 'citingcorpusid',  # ä»JSONçš„citingcorpusidå­—æ®µæå–å€¼
    'publication_venues': 'id',  # JSONä¸­çš„idå­—æ®µå¯¹åº”æ•°æ®åº“çš„publicationvenueid
    # å…¶ä»–è¡¨çš„JSONå­—æ®µåå’Œæ•°æ®åº“å­—æ®µåä¸€è‡´ï¼Œä½¿ç”¨æ•°æ®åº“å­—æ®µåå³å¯
}

# æ­£åˆ™è¡¨è¾¾å¼ï¼šå¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼ˆæ¯”å®Œæ•´JSONè§£æå¿«10å€ï¼‰
def get_key_pattern(field_name: str, is_uuid: bool = False):
    """
    æ ¹æ®å­—æ®µåç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼
    
    Args:
        field_name: JSONå­—æ®µå
        is_uuid: æ˜¯å¦æ˜¯UUIDç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå¦åˆ™æ˜¯æ•°å­—ç±»å‹
    """
    if is_uuid:
        # UUIDæ ¼å¼: "id": "0b0cbb6c-54d7-4989-9265-abb19476957d"
        return re.compile(rf'"{field_name}"\s*:\s*"([0-9a-f\-]+)"', re.IGNORECASE)
    else:
        # æ•°å­—æ ¼å¼: "corpusid": 12345
        return re.compile(rf'"{field_name}"\s*:\s*(\d+)', re.IGNORECASE)

def get_log_files(table_name: str):
    """
    æ ¹æ®è¡¨åè·å–ä¸“å±çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    
    Args:
        table_name: è¡¨å
    
    Returns:
        (progress_file, failed_file, always_failed_file) å…ƒç»„
    """
    progress_dir = Path(PROGRESS_DIR)
    failed_dir = Path(FAILED_DIR)
    always_failed_dir = Path(ALWAYS_FAILED_DIR)
    
    # åˆ›å»ºç›®å½•
    progress_dir.mkdir(parents=True, exist_ok=True)
    failed_dir.mkdir(parents=True, exist_ok=True)
    always_failed_dir.mkdir(parents=True, exist_ok=True)
    
    progress_file = progress_dir / f"{table_name}_progress.txt"
    failed_file = failed_dir / f"{table_name}_failed.txt"
    always_failed_file = always_failed_dir / f"{table_name}_failed.txt"
    
    return str(progress_file), str(failed_file), str(always_failed_file)

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œåªæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
logging.basicConfig(
    level=logging.ERROR,
    format='%(message)s'
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # ä¸»è¿›ç¨‹å¯ä»¥INFO


# =============================================================================
# æ–­ç‚¹ç»­ä¼ 
# =============================================================================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        
        completed = set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    completed.add(line)
        
        return completed
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


class FailedFilesLogger:
    """å¤±è´¥æ–‡ä»¶è®°å½•å™¨ï¼ˆæ”¯æŒ failed å’Œ always_failedï¼‰"""
    
    def __init__(self, failed_file: str, always_failed_file: str = None, is_retry: bool = False):
        self.failed_file = Path(failed_file)
        self.always_failed_file = Path(always_failed_file) if always_failed_file else None
        self.is_retry = is_retry
        self.failed_file.parent.mkdir(parents=True, exist_ok=True)
        if self.always_failed_file:
            self.always_failed_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_failed(self) -> Set[str]:
        """åŠ è½½å·²çŸ¥å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨"""
        if not self.failed_file.exists():
            return set()
        
        failed = set()
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('|')
                    if len(parts) >= 2:
                        failed.add(parts[1].strip())
        
        return failed
    
    def log_failed(self, file_name: str, error: str):
        """è®°å½•å¤±è´¥æ–‡ä»¶"""
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        try:
            # é‡è¯•æ¨¡å¼ï¼šå¤±è´¥è®°å½•åˆ° always_failed
            if self.is_retry and self.always_failed_file:
                with open(self.always_failed_file, 'a', encoding='utf-8') as f:
                    f.write(f"{timestamp} | {file_name} | {error}\n")
                    f.flush()
            else:
                # é¦–æ¬¡å¤„ç†ï¼šè®°å½•åˆ° failed
                with open(self.failed_file, 'a', encoding='utf-8') as f:
                    f.write(f"{timestamp} | {file_name} | {error}\n")
                    f.flush()
        except Exception as e:
            logger.error(f"è®°å½•å¤±è´¥æ–‡ä»¶æ—¥å¿—æ—¶å‡ºé”™: {e}")
    
    def remove_from_failed(self, file_name: str):
        """ä» failed ä¸­åˆ é™¤è®°å½•ï¼ˆé‡è¯•æˆåŠŸæ—¶è°ƒç”¨ï¼‰"""
        if not self.failed_file.exists():
            return
        
        try:
            lines = []
            with open(self.failed_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # ç²¾ç¡®åŒ¹é…æ–‡ä»¶åï¼ˆæ ¼å¼ï¼šæ—¶é—´æˆ³ | æ–‡ä»¶å | é”™è¯¯ä¿¡æ¯ï¼‰
                    parts = line.split('|')
                    if len(parts) >= 2 and parts[1].strip() != file_name:
                        lines.append(line)
            
            # å†™å›æ–‡ä»¶
            with open(self.failed_file, 'w', encoding='utf-8') as f:
                f.writelines(lines)
        except Exception as e:
            # åˆ é™¤å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œè®°å½•æ—¥å¿—å³å¯
            logger.warning(f"ä» failed æ—¥å¿—ä¸­åˆ é™¤è®°å½•å¤±è´¥: {e}")
    
    def reset(self):
        if self.failed_file.exists():
            self.failed_file.unlink()


# =============================================================================
# ç”Ÿäº§è€…ï¼šè§£å‹è¿›ç¨‹ï¼ˆå¤šä¸ªå¹¶è¡Œï¼‰
# =============================================================================

def extractor_worker(
    file_queue: Queue,
    data_queue: Queue,
    stats_dict: dict,
    table_name: str,
    batch_size: int = 10000
):
    """
    è§£å‹å·¥ä½œè¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
    ä»file_queueå–æ–‡ä»¶ï¼Œè§£å‹åæ”¾å…¥data_queue
    """
    # å®Œå…¨ç¦ç”¨æ­¤è¿›ç¨‹çš„æ—¥å¿—è¾“å‡º
    import logging
    logging.getLogger().setLevel(logging.CRITICAL)
    
    # æ ¹æ®è¡¨åç¡®å®šä¸»é”®å­—æ®µï¼ˆæ•°æ®åº“å­—æ®µåï¼‰
    primary_key_field = TABLE_PRIMARY_KEY_MAP.get(table_name, 'corpusid')
    # ç¡®å®šJSONä¸­å¯¹åº”çš„å­—æ®µåï¼ˆç”¨äºæ­£åˆ™æå–ï¼‰
    json_field_name = JSON_FIELD_MAP.get(table_name, primary_key_field)
    # åˆ¤æ–­JSONä¸­ä¸»é”®æ˜¯å¦æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆç”¨äºæ­£åˆ™åŒ¹é…ï¼‰
    is_json_string = table_name in JSON_STRING_KEY_TABLES
    # åˆ¤æ–­æ•°æ®åº“ä¸­ä¸»é”®æ˜¯å¦æ˜¯TEXTç±»å‹ï¼ˆç”¨äºå†³å®šæ˜¯å¦è½¬æ¢ï¼‰
    is_db_text = table_name in DB_TEXT_KEY_TABLES
    key_pattern = get_key_pattern(json_field_name, is_uuid=is_json_string)
    
    worker_name = f"Extractor-{id(file_queue) % 1000}"
    
    while True:
        try:
            # è·å–æ–‡ä»¶ä»»åŠ¡
            task = file_queue.get(timeout=1)
            if task is None:  # ç»“æŸä¿¡å·
                break
            
            gz_file_path, file_name = task
            start_time = time.time()
            
            try:
                batch = []
                line_count = 0
                valid_count = 0
                
                # æµå¼è§£å‹ - é‡åˆ°æŸåç›´æ¥è·³è¿‡ï¼Œä¸é‡è¯•
                # USBç›˜ä¼˜åŒ–ï¼šä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼+æ›´å¤§ç¼“å†²åŒºï¼Œå‡å°‘éšæœºI/O
                try:
                    import io
                    # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€ï¼Œè®¾ç½®8MBç¼“å†²åŒº
                    with gzip.open(gz_file_path, 'rb') as f_binary:
                        # åŒ…è£…ä¸ºå¸¦å¤§ç¼“å†²åŒºçš„æ–‡æœ¬æµ
                        f = io.TextIOWrapper(io.BufferedReader(f_binary, buffer_size=8*1024*1024), 
                                            encoding='utf-8', errors='ignore')
                        for line in f:
                            line_count += 1
                            line = line.strip()
                            if not line:
                                continue
                            
                            # æ­£åˆ™å¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼ˆé¿å…å®Œæ•´JSONè§£æï¼‰
                            match = key_pattern.search(line)
                            if not match:
                                continue
                            
                            # æå–ä¸»é”®å€¼
                            # - å¦‚æœDBæ˜¯TEXTç±»å‹ï¼Œä¿æŒå­—ç¬¦ä¸²ï¼ˆå¦‚publication_venuesï¼‰
                            # - å¦åˆ™è½¬æ¢ä¸ºintï¼ˆå¦‚authorsçš„authoridè™½ç„¶JSONæ˜¯å­—ç¬¦ä¸²ï¼Œä½†DBæ˜¯BIGINTï¼‰
                            key_value = match.group(1) if is_db_text else int(match.group(1))
                            valid_count += 1
                            
                            # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨åŸå§‹è¡Œï¼Œé¿å…ä¸å¿…è¦çš„è½¬ä¹‰æ£€æŸ¥
                            # PostgreSQL COPYå¯ä»¥å¤„ç†å¤§å¤šæ•°JSONå­—ç¬¦
                            batch.append((key_value, line))
                            
                            # æ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€åˆ°é˜Ÿåˆ—
                            if len(batch) >= batch_size:
                                data_queue.put(('data', file_name, batch))
                                batch = []
                    
                    # å‘é€å‰©ä½™æ•°æ®
                    if batch:
                        data_queue.put(('data', file_name, batch))
                    
                    # å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                    data_queue.put(('done', file_name, valid_count))
                    
                    # æ›´æ–°ç»Ÿè®¡
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError) as gz_error:
                    # GZIPæ–‡ä»¶æŸåï¼Œç›´æ¥è·³è¿‡ï¼Œä¸é‡è¯•
                    data_queue.put(('error', file_name, f"Corrupted"))
                    continue
                
            except Exception as e:
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


# =============================================================================
# æ¶ˆè´¹è€…ï¼šæ’å…¥è¿›ç¨‹ï¼ˆå•ä¸ªï¼Œé«˜æ•ˆæ‰¹é‡æ’å…¥ï¼‰
# =============================================================================

def inserter_worker(
    data_queue: Queue,
    table_name: str,
    stats_dict: dict,
    tracker: ProgressTracker,
    failed_logger: FailedFilesLogger,
    use_upsert: bool = False,
    commit_batches: int = 3,
    total_files: int = 0,
    primary_key: str = 'corpusid',
    is_retry: bool = False,
    initial_completed: int = 0,
    db_config: dict = None
):
    """
    æ’å…¥å·¥ä½œè¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
    æŒç»­ä»data_queueå–æ•°æ®å¹¶æ‰¹é‡æ’å…¥
    
    Args:
        primary_key: ä¸»é”®å­—æ®µåï¼ˆé»˜è®¤corpusidï¼‰
        db_config: æ•°æ®åº“é…ç½®å­—å…¸
    """
    try:
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        if db_config is None:
            db_config = db_config_v2.DB_CONFIG
        conn = psycopg2.connect(**db_config)
        conn.autocommit = False
        cursor = conn.cursor()
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®ï¼ˆåˆ†ç¦»è¯»å†™è·¯å¾„åçš„ä¼˜åŒ–é…ç½®ï¼Œå……åˆ†åˆ©ç”¨8æ ¸32GBï¼‰
        try:
            cursor.execute("SET synchronous_commit = OFF")  # å¼‚æ­¥æäº¤ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
            cursor.execute("SET commit_delay = 100000")  # å»¶è¿Ÿæäº¤100ms
            cursor.execute("SET maintenance_work_mem = '2GB'")  # ç»´æŠ¤å†…å­˜ï¼ˆé€‚ä¸­ï¼Œé¿å…è„é¡µç§¯å‹ï¼‰
            cursor.execute("SET work_mem = '512MB'")  # å·¥ä½œå†…å­˜ï¼ˆé€‚ä¸­ï¼‰
            cursor.execute("SET temp_buffers = '4GB'")  # ä¸´æ—¶ç¼“å†²åŒº
            cursor.execute("SET effective_cache_size = '16GB'")  # ç¼“å­˜å¤§å°
            cursor.execute("SET max_parallel_workers_per_gather = 0")  # å…³é—­å¹¶è¡Œ
            # ä¸´æ—¶è¡¨ç©ºé—´ä½¿ç”¨Dç›˜ï¼ˆå‰æï¼šå·²åˆ›å»ºd1_tempè¡¨ç©ºé—´ï¼‰
            try:
                cursor.execute("SET temp_tablespaces = 'd1_temp'")
            except:
                pass  # å¦‚æœè¡¨ç©ºé—´ä¸å­˜åœ¨ï¼Œå¿½ç•¥
        except Exception as e:
            conn.rollback()
            logger.warning(f"éƒ¨åˆ†æ€§èƒ½é…ç½®å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        # è¿™äº›å‚æ•°éœ€è¦åœ¨postgresql.confä¸­è®¾ç½®ï¼Œä¸èƒ½åœ¨ä¼šè¯çº§åˆ«ä¿®æ”¹ï¼Œæ³¨é‡Šæ‰é¿å…è­¦å‘Š
        # bgwriter_delay = 1000ms  # åå°å†™å…¥å»¶è¿Ÿï¼ˆå‡å°‘éšæœºå†™ï¼‰
        # checkpoint_timeout = 30min  # æ£€æŸ¥ç‚¹é—´éš”ï¼ˆå‡å°‘åˆ·ç›˜ï¼‰
        # checkpoint_completion_target = 0.9  # æ£€æŸ¥ç‚¹å®Œæˆç›®æ ‡
        
        # WALè®¾ç½®ï¼ˆå¯èƒ½å¤±è´¥ï¼Œå•ç‹¬å¤„ç†ï¼‰
        try:
            cursor.execute("SET wal_writer_delay = '1000ms'")
        except Exception:
            conn.rollback()  # å›æ»šå¹¶ç»§ç»­
        
        # ç¦ç”¨è§¦å‘å™¨ï¼ˆINSERTæ¨¡å¼ï¼‰
        if not use_upsert:
            try:
                cursor.execute(f"ALTER TABLE {table_name} DISABLE TRIGGER ALL")
                conn.commit()
            except Exception as e:
                conn.rollback()
                logger.warning(f"ç¦ç”¨è§¦å‘å™¨å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        total_inserted = 0
        file_stats = {}  # {file_name: inserted_count}
        completed_files = 0  # å½“å‰æ‰¹æ¬¡å®Œæˆçš„æ–‡ä»¶æ•°
        last_log_time = time.time()
        start_time = time.time()
        batch_count = 0  # æ‰¹æ¬¡è®¡æ•°å™¨
        
        while True:
            try:
                # éé˜»å¡è·å–ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
                item = data_queue.get(timeout=5)
                
                item_type = item[0]
                
                if item_type == 'stop':
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    try:
                        # æ‰¹é‡æ’å…¥ï¼ˆä¼ å…¥ä¸»é”®å­—æ®µï¼‰
                        inserted = batch_insert_copy(cursor, table_name, batch, use_upsert, primary_key)
                        batch_count += 1
                        
                        # æ¯Nä¸ªæ‰¹æ¬¡commitä¸€æ¬¡ï¼Œå‡å°‘commitå¼€é”€
                        if batch_count >= commit_batches:
                            conn.commit()
                            batch_count = 0
                        
                        total_inserted += inserted
                        file_stats[file_name] = file_stats.get(file_name, 0) + inserted
                    
                    except Exception as insert_error:
                        # æ’å…¥å¤±è´¥ï¼Œå›æ»šå½“å‰äº‹åŠ¡
                        conn.rollback()
                        batch_count = 0  # é‡ç½®æ‰¹æ¬¡è®¡æ•°
                        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼ˆå·²å›æ»šï¼‰: {insert_error}")
                        # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                        continue
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯3ç§’ï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 3:
                        elapsed = current_time - start_time
                        rate = total_inserted / elapsed if elapsed > 0 else 0
                        
                        # æ€»è¿›åº¦ï¼ˆåŒ…å«åˆå§‹å·²å®Œæˆçš„æ–‡ä»¶ï¼‰
                        total_completed_now = initial_completed + completed_files
                        total_all_files = initial_completed + total_files
                        overall_progress = (total_completed_now / total_all_files * 100) if total_all_files > 0 else 0
                        
                        # ä¼°ç®—å‰©ä½™æ—¶é—´
                        if completed_files > 0:
                            avg_time_per_file = elapsed / completed_files
                            remaining_files = total_files - completed_files
                            eta_seconds = remaining_files * avg_time_per_file
                            eta_hours = int(eta_seconds / 3600)
                            eta_mins = int((eta_seconds % 3600) / 60)
                            eta_str = f"{eta_hours}h{eta_mins}m" if eta_hours > 0 else f"{eta_mins}m"
                        else:
                            eta_str = "calculating..."
                        
                        # æ˜¾ç¤ºï¼šæ€»è¿›åº¦ï¼ˆåŒ…å«åˆå§‹å·²å®Œæˆçš„ï¼‰
                        print(f"\r[{total_completed_now}/{total_all_files}] {overall_progress:.1f}% | "
                              f"{total_inserted:,} rows | {rate:.0f}/s | "
                              f"ETA: {eta_str}    ", end='', flush=True)
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    # æ ‡è®°æ–‡ä»¶å®Œæˆ
                    tracker.mark_completed(file_name)
                    # é‡è¯•æˆåŠŸï¼šä» failed ä¸­åˆ é™¤
                    if is_retry:
                        failed_logger.remove_from_failed(file_name)
                    completed_files += 1
                    inserted = file_stats.get(file_name, 0)
                
                elif item_type == 'error':
                    _, file_name, error = item
                    # è®°å½•å¤±è´¥æ–‡ä»¶
                    failed_logger.log_failed(file_name, error)
                    completed_files += 1
            
            except Empty:
                # é˜Ÿåˆ—ç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"[Inserter] å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                conn.rollback()
                continue
        
        # æäº¤å‰©ä½™æœªcommitçš„æ•°æ®
        if batch_count > 0:
            conn.commit()
        
        # æ¢å¤è¡¨çŠ¶æ€
        if not use_upsert:
            # å¯ç”¨è§¦å‘å™¨
            cursor.execute(f"ALTER TABLE {table_name} ENABLE TRIGGER ALL")
            conn.commit()
        
        cursor.close()
        conn.close()
        
        stats_dict['inserted'] = total_inserted
        
    except Exception as e:
        logger.error(f"[Inserter] ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def clean_utf8(text: str) -> str:
    """æ¸…ç†å­—ç¬¦ä¸²ä¸­çš„æ— æ•ˆ UTF-8 å­—ç¬¦"""
    if not text:
        return text
    # ç¼–ç ä¸º UTF-8ï¼Œæ›¿æ¢æ— æ•ˆå­—ç¬¦ï¼Œå†è§£ç å›æ¥
    return text.encode('utf-8', errors='replace').decode('utf-8', errors='replace')


def batch_insert_copy(cursor, table_name: str, batch: list, use_upsert: bool = False, primary_key: str = 'corpusid') -> int:
    """
    ä½¿ç”¨COPYæ‰¹é‡æ’å…¥ï¼ˆæœ€å¿«æ–¹æ³•ï¼‰
    
    æ•°æ®ä»¥TEXTæ ¼å¼å­˜å‚¨ï¼ˆä¸éªŒè¯ä¸è§£æï¼Œæé€Ÿï¼‰
    
    æ³¨æ„ï¼š
    - paper_idsè¡¨ï¼šåªæ’å…¥corpusidå­—æ®µï¼Œæ— dataå­—æ®µï¼ˆæé€Ÿæ¨¡å¼ï¼‰
    - authorsè¡¨ï¼šä¸»é”®åˆ—åæ˜¯authorid (BIGINT)
    - citationsè¡¨ï¼šä¸»é”®æ˜¯id (BIGSERIALè‡ªå¢)ï¼Œcitingcorpusidä¸ºæ™®é€šå­—æ®µ
    - publication_venuesè¡¨ï¼šä¸»é”®åˆ—åæ˜¯publicationvenueid (TEXT)ï¼Œå€¼ä»idæå–ï¼ˆUUIDå­—ç¬¦ä¸²ï¼‰
    - å…¶ä»–è¡¨ï¼šä¸»é”®åˆ—åæ˜¯corpusid (BIGINT)
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_name: è¡¨å
        batch: æ•°æ®æ‰¹æ¬¡ [(key_value, json_line), ...] - key_valueå·²ä»JSONæ­£ç¡®å­—æ®µæå–
        use_upsert: æ˜¯å¦ä½¿ç”¨UPSERTæ¨¡å¼
        primary_key: ä¸»é”®åˆ—åï¼ˆauthorid/publicationvenueid/corpusidï¼Œcitationsè¡¨æ— éœ€ï¼‰
    """
    if not batch:
        return 0
    
    from io import StringIO
    import psycopg2.errors
    
    # æ£€æŸ¥æ•°æ®åº“ä¸»é”®æ˜¯å¦æ˜¯TEXTç±»å‹ï¼ˆç”¨äºVALUESè¯­å¥æ ¼å¼åŒ–ï¼‰
    is_string_key = table_name in DB_TEXT_KEY_TABLES
    
    try:
        # paper_idsè¡¨ç‰¹æ®Šå¤„ç†ï¼šåªæ’å…¥corpusidï¼Œæ— å…¶ä»–å­—æ®µï¼ˆæé€Ÿæ¨¡å¼ï¼‰
        if table_name == 'paper_ids':
            # æ‰¹å†…å»é‡ï¼ˆåªä¿ç•™å”¯ä¸€corpusidï¼‰
            unique_ids = list(set(key_val for key_val, _ in batch))
            
            buffer = StringIO()
            # åªæ’å…¥corpusidï¼Œæ— æ—¶é—´æˆ³å­—æ®µï¼ˆæé€Ÿä¼˜åŒ–ï¼‰
            buffer.write(''.join(f"{cid}\n" for cid in unique_ids))
            buffer.seek(0)
            
            try:
                # åªæ’å…¥corpusidå­—æ®µï¼Œå…¶ä»–å­—æ®µä½¿ç”¨é»˜è®¤å€¼
                cursor.copy_expert(
                    f"COPY {table_name} (corpusid) FROM STDIN",
                    buffer
                )
                return len(unique_ids)
            except psycopg2.errors.UniqueViolation:
                # æœ‰é‡å¤keyï¼Œé¢„æŸ¥è¯¢è¿‡æ»¤ï¼ˆæ–­ç‚¹ç»­ä¼ åœºæ™¯ï¼‰
                cursor.connection.rollback()
                
                # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„IDï¼ˆåˆ†æ‰¹æŸ¥è¯¢ï¼Œé¿å…å ç”¨è¿‡å¤šå†…å­˜ï¼‰
                chunk_size = 100000
                existing_ids = set()
                
                for i in range(0, len(unique_ids), chunk_size):
                    chunk = unique_ids[i:i + chunk_size]
                    placeholders = ','.join(['%s'] * len(chunk))
                    cursor.execute(
                        f"SELECT corpusid FROM {table_name} WHERE corpusid IN ({placeholders})",
                        chunk
                    )
                    existing_ids.update(row[0] for row in cursor.fetchall())
                
                # è¿‡æ»¤å¹¶æ’å…¥æ–°æ•°æ®
                new_ids = [cid for cid in unique_ids if cid not in existing_ids]
                if not new_ids:
                    return 0
                
                buffer = StringIO()
                buffer.write(''.join(f"{cid}\n" for cid in new_ids))
                buffer.seek(0)
                
                try:
                    cursor.copy_expert(
                        f"COPY {table_name} (corpusid) FROM STDIN",
                        buffer
                    )
                    return len(new_ids)
                except Exception:
                    cursor.connection.rollback()
                    return 0
        
        # ä»¥ä¸‹æ˜¯å…¶ä»–è¡¨çš„åŸæœ‰é€»è¾‘
        elif use_upsert:
            # UPSERTæ¨¡å¼ï¼šæ‰¹å†…å»é‡ + åˆ†æ‰¹æ’å…¥
            seen = {key_value: clean_utf8(data) for key_value, data in batch}
            chunk_size = 1000
            total_processed = 0
            
            for i in range(0, len(seen), chunk_size):
                chunk = list(seen.items())[i:i + chunk_size]
                
                # æ„å»ºVALUESå­å¥ï¼ˆå­—ç¬¦ä¸²ä¸»é”®éœ€åŠ å¼•å·ï¼‰
                values_list = []
                for k, v in chunk:
                    escaped_data = v.replace("'", "''")
                    if is_string_key:
                        escaped_key = str(k).replace("'", "''")
                        values_list.append(f"('{escaped_key}', '{escaped_data}', NOW(), NOW())")
                    else:
                        values_list.append(f"({k}, '{escaped_data}', NOW(), NOW())")
                values = ','.join(values_list)
                
                cursor.execute(f"""
                    INSERT INTO {table_name} ({primary_key}, data, insert_time, update_time)
                    VALUES {values}
                    ON CONFLICT ({primary_key}) DO UPDATE SET
                        data = EXCLUDED.data, update_time = EXCLUDED.update_time
                """)
                total_processed += len(chunk)
            
            return total_processed
        else:
            # INSERTæ¨¡å¼ï¼šæé€ŸCOPYï¼ˆTEXTç±»å‹ï¼Œä¸éªŒè¯ä¸è§£æï¼Œæœ€å¿«ï¼‰
            
            # citationsè¡¨ï¼šè‡ªå¢ä¸»é”®ï¼Œç›´æ¥æ’å…¥citingcorpusidå­—æ®µ
            if table_name == 'citations':
                buffer = StringIO()
                buffer.write(''.join(f"{key_val}\t{clean_utf8(data)}\t\\N\t\\N\n" for key_val, data in batch))
                buffer.seek(0)
                
                cursor.copy_expert(
                    f"COPY {table_name} (citingcorpusid, data, insert_time, update_time) "
                    f"FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')",
                    buffer
                )
                return len(batch)
            
            # å…¶ä»–è¡¨ï¼šä¸»é”®å”¯ä¸€ï¼Œå¤„ç†å†²çª
            buffer = StringIO()
            buffer.write(''.join(f"{key_val}\t{clean_utf8(data)}\t\\N\t\\N\n" for key_val, data in batch))
            buffer.seek(0)
            
            try:
                cursor.copy_expert(
                    f"COPY {table_name} ({primary_key}, data, insert_time, update_time) "
                    f"FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')",
                    buffer
                )
                return len(batch)
            except psycopg2.errors.UniqueViolation:
                # æœ‰é‡å¤keyï¼Œä½¿ç”¨é¢„æŸ¥è¯¢è¿‡æ»¤æ³•
                cursor.connection.rollback()
                
                batch_ids = [key_val for key_val, _ in batch]
                if not batch_ids:
                    return 0
                
                # æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„ID
                placeholders = ','.join(['%s'] * len(batch_ids))
                cursor.execute(
                    f"SELECT {primary_key} FROM {table_name} WHERE {primary_key} IN ({placeholders})",
                    batch_ids
                )
                existing_ids = set(row[0] for row in cursor.fetchall())
                
                # è¿‡æ»¤å¹¶æ’å…¥æ–°æ•°æ®
                new_batch = [(k, v) for k, v in batch if k not in existing_ids]
                if not new_batch:
                    return 0
                
                buffer = StringIO()
                buffer.write(''.join(f"{key_val}\t{clean_utf8(data)}\t\\N\t\\N\n" for key_val, data in new_batch))
                buffer.seek(0)
                
                try:
                    cursor.copy_expert(
                        f"COPY {table_name} ({primary_key}, data, insert_time, update_time) "
                        f"FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')",
                        buffer
                    )
                    return len(new_batch)
                except Exception:
                    cursor.connection.rollback()
                    return 0
        
    except Exception as e:
        import traceback
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
        raise


# =============================================================================
# ä¸»åè°ƒå™¨
# =============================================================================

def process_gz_folder_pipeline(
    folder_path: str,
    table_name: str,
    use_upsert: bool = False,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    reset_progress: bool = False,
    retry_failed: bool = False,
    is_retry: bool = False
):
    """
    æµæ°´çº¿å¹¶è¡Œå¤„ç†ï¼šå¤šä¸ªè§£å‹è¿›ç¨‹ + å•ä¸ªæ’å…¥è¿›ç¨‹
    """
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # æ ¹æ®è¡¨åè·å–ä¸“å±çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    progress_file, failed_file, always_failed_file = get_log_files(table_name)
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªï¼ˆæ¯ä¸ªè¡¨ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶ï¼‰
    tracker = ProgressTracker(progress_file)
    failed_logger = FailedFilesLogger(failed_file, always_failed_file, is_retry)
    
    if reset_progress:
        tracker.reset()
        failed_logger.reset()
    
    # åŠ è½½å·²å®Œæˆå’Œå¤±è´¥çš„æ–‡ä»¶
    completed_files = tracker.load_completed() if resume else set()
    failed_files = failed_logger.load_failed() if (resume and not retry_failed) else set()
    
    # æ‰«æGZæ–‡ä»¶
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ°.gzæ–‡ä»¶: {folder_path}")
        return
    
    total_gz_count = len(gz_files)
    
    # è¿‡æ»¤ï¼šæ’é™¤å·²å®Œæˆçš„æ–‡ä»¶ï¼Œä»¥åŠå·²çŸ¥å¤±è´¥çš„æ–‡ä»¶ï¼ˆé™¤éretry_failed=Trueï¼‰
    excluded_files = completed_files | failed_files
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in excluded_files]
    
    # ç»Ÿè®¡ä¿¡æ¯
    completed_count = len(completed_files)
    failed_count = len(failed_files)
    
    # è·å–è¯¥è¡¨ä½¿ç”¨çš„ä¸»é”®å­—æ®µ
    primary_key_field = TABLE_PRIMARY_KEY_MAP.get(table_name, 'corpusid')
    
    # æ ¹æ®è¡¨åè·å–ä¼˜åŒ–é…ç½®
    config = TABLE_CONFIGS.get(table_name, DEFAULT_CONFIG)
    batch_size = config['batch_size']
    commit_batches = config['commit_batches']
    # å¦‚æœç”¨æˆ·æ²¡æŒ‡å®šextractorsï¼Œä½¿ç”¨é…ç½®ä¸­çš„å€¼
    if num_extractors == NUM_EXTRACTORS:  # é»˜è®¤å€¼
        num_extractors = config['extractors']
    
    # ç²¾ç®€è¾“å‡ºï¼šä¸€è¡Œæ˜¾ç¤ºæ ¸å¿ƒä¿¡æ¯
    failed_info = f", failed: {failed_count}" if failed_files else ""
    logger.info(f"\n[{table_name}] Total: {total_gz_count}, Completed: {completed_count}{failed_info}, Pending: {len(pending_files)}")
    
    if not pending_files:
        logger.info("All files processed!\n")
        return
    
    overall_start = time.time()
    
    try:
        # åˆ›å»ºé˜Ÿåˆ—
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
        manager = Manager()
        stats_dict = manager.dict()
        
        # æ·»åŠ æ–‡ä»¶ä»»åŠ¡
        for task in pending_files:
            file_queue.put(task)
        
        # æ·»åŠ ç»“æŸä¿¡å·ï¼ˆæ¯ä¸ªè§£å‹è¿›ç¨‹ä¸€ä¸ªï¼‰
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨æ’å…¥è¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
        # ä¼ å…¥æ€»æ–‡ä»¶æ•°å’Œå·²å®Œæˆæ•°ï¼Œç”¨äºæ­£ç¡®æ˜¾ç¤ºè¿›åº¦
        # ä¼ å…¥å½“å‰çš„ DB_CONFIG ç¡®ä¿å­è¿›ç¨‹ä½¿ç”¨æ­£ç¡®çš„æ•°æ®åº“é…ç½®
        inserter = Process(
            target=inserter_worker,
            args=(data_queue, table_name, stats_dict, tracker, failed_logger, use_upsert, commit_batches, len(pending_files), primary_key_field, is_retry, completed_count, db_config_v2.DB_CONFIG.copy()),
            name='Inserter'
        )
        inserter.start()
        
        # å¯åŠ¨è§£å‹è¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, table_name, batch_size),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        # ç­‰å¾…æ‰€æœ‰è§£å‹è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join()
        
        # å‘é€åœæ­¢ä¿¡å·ç»™æ’å…¥è¿›ç¨‹
        data_queue.put(('stop', None, None))
        
        # ç­‰å¾…æ’å…¥è¿›ç¨‹å®Œæˆ
        inserter.join()
        
        elapsed = time.time() - overall_start
        total_inserted = stats_dict.get('inserted', 0)
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"[{table_name}] DONE: {len(pending_files)} files, {total_inserted:,} rows, {elapsed/60:.1f} min, {avg_rate:.0f} rows/s\n")
        
    except KeyboardInterrupt:
        logger.warning("\nInterrupted by user (progress saved)")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nProcessing failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥ï¼ˆç›®æ ‡ï¼š3000-5000æ¡/ç§’ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼š
  ğŸš€ ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨å¹¶è¡Œ
  ğŸš€ æ­£åˆ™å¿«é€Ÿæå–ï¼šé¿å…å®Œæ•´JSONè§£æï¼ˆæé€Ÿ10å€ï¼‰
  ğŸš€ æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  ğŸš€ é˜Ÿåˆ—ç¼“å†²ï¼šæŒç»­ä¾›åº”æ•°æ®ï¼Œæ— ç©ºé—²ç­‰å¾…
  ğŸš€ å¤šè¿›ç¨‹è§£å‹ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
  ğŸš€ çµæ´»ä¸»é”®é…ç½®ï¼šæ ¹æ®è¡¨åè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„ä¸»é”®å­—æ®µ
     - authorsè¡¨ä½¿ç”¨authorid
     - citationsè¡¨ä½¿ç”¨è‡ªå¢idï¼Œcitingcorpusidä¸ºæ™®é€šå­—æ®µï¼ˆå…è®¸é‡å¤ï¼Œæé€Ÿæ’å…¥ï¼‰
     - publication_venuesè¡¨ä½¿ç”¨publicationvenueidï¼ˆå€¼ä»idæå–ï¼ŒUUIDå­—ç¬¦ä¸²ï¼‰
     - å…¶ä»–è¡¨ä½¿ç”¨corpusid

ç¤ºä¾‹ï¼š
  # å¤„ç†papersæ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨corpusidä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\papers" --table papers
  
  # å¤„ç†authorsæ–‡ä»¶å¤¹ï¼ˆè‡ªåŠ¨ä½¿ç”¨authoridä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\authors" --table authors
  
  # å¤„ç†citationsæ–‡ä»¶å¤¹ï¼ˆè‡ªå¢ä¸»é”®ï¼Œæé€Ÿæ’å…¥ï¼Œæ— éœ€å»é‡ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\citations" --table citations
  
  # è‡ªå®šä¹‰è§£å‹è¿›ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\s2orc" --table s2orc --extractors 8
  
  # ä¸­æ–­åç»§ç»­
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\papers" --table papers --resume
        """
    )
    
    parser.add_argument('--dir', type=str, required=True,
                       help='GZæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--table', type=str, required=True,
                       choices=FIELD_TABLES,
                       help='ç›®æ ‡æ•°æ®åº“è¡¨å')
    parser.add_argument('--upsert', action='store_true',
                       help='ä½¿ç”¨UPSERTæ¨¡å¼')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--resume', action='store_true',
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true',
                       help='é‡ç½®è¿›åº¦')
    parser.add_argument('--retry-failed', action='store_true',
                       help='é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ä»¶ï¼ˆé‡æ–°ä¸‹è½½åä½¿ç”¨ï¼‰')
    
    parser.set_defaults(resume=True)
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¤„ç†
    process_gz_folder_pipeline(
        folder_path=args.dir,
        table_name=args.table,
        use_upsert=args.upsert,
        num_extractors=args.extractors,
        resume=args.resume,
        reset_progress=args.reset,
        retry_failed=args.retry_failed
    )


if __name__ == '__main__':
    main()

