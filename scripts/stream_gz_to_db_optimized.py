#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥
æ€§èƒ½ä¼˜åŒ–ï¼š
  âœ… ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨åˆ†ç¦»å¹¶è¡Œ
  âœ… æ­£åˆ™å¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼šé¿å…å®Œæ•´JSONè§£æ
  âœ… æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  âœ… é˜Ÿåˆ—ç¼“å†²ï¼šè§£å‹å¿«æ—¶ä¸ç­‰å¾…ï¼Œæ’å…¥å¿«æ—¶ä¸ç©ºé—²
  âœ… æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­æ¢å¤
  âœ… çµæ´»ä¸»é”®é…ç½®ï¼šä¸åŒè¡¨ä½¿ç”¨ä¸åŒä¸»é”®å­—æ®µ
     - authorsè¡¨: authorid
     - citationsè¡¨: citedcorpusid
     - å…¶ä»–è¡¨: corpusid
  
ç›®æ ‡ï¼š10å€æ€§èƒ½æå‡ï¼ˆ3000-5000æ¡/ç§’ï¼‰
"""

import gzip
import re
import sys
import time
import logging
from pathlib import Path
from typing import Optional, Set
from multiprocessing import Process, Queue, Manager, cpu_count
from queue import Empty

import psycopg2

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from database.config.db_config_v2 import DB_CONFIG, FIELD_TABLES

# =============================================================================
# æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€é…ç½®ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
# =============================================================================

# é’ˆå¯¹TEXTç±»å‹ä¼˜åŒ–é…ç½®ï¼ˆå½»åº•é¿å…ç¼“å†²åŒºé—®é¢˜ï¼šä¸ä½¿ç”¨ä¸´æ—¶è¡¨ï¼Œä½¿ç”¨VALUESï¼‰
TABLE_CONFIGS = {
    # è¶…å¤§æ•°æ® (60-120KB/æ¡): s2orcç³»åˆ— - æ§åˆ¶å†…å­˜ä½¿ç”¨ï¼Œé¢‘ç¹commit
    's2orc': {'batch_size': 2000, 'commit_batches': 3, 'extractors': 6},
    's2orc_v2': {'batch_size': 2000, 'commit_batches': 3, 'extractors': 6},
    # 2000æ¡Ã—100KB=200MB/æ‰¹ï¼Œ3æ‰¹=600MBæäº¤ï¼ˆå®‰å…¨+é¢‘ç¹é‡Šæ”¾ï¼‰
        
    # ä¸­ç­‰æ•°æ® (16KB/æ¡): embeddingsç³»åˆ— - å‡å°æ‰¹æ¬¡ï¼Œé¢‘ç¹commit
    'embeddings_specter_v1': {'batch_size': 15000, 'commit_batches': 3, 'extractors': 6},
    'embeddings_specter_v2': {'batch_size': 15000, 'commit_batches': 3, 'extractors': 6},
    # 15000æ¡Ã—16KB=240MB/æ‰¹ï¼Œ3æ‰¹=720MBæäº¤ï¼ˆå®‰å…¨èŒƒå›´ï¼‰
    
    # å°æ•°æ® (1-3KB/æ¡): å…¶ä»–è¡¨ - å°æ‰¹æ¬¡+é¢‘ç¹commit
    'papers': {'batch_size': 25000, 'commit_batches': 3, 'extractors': 7},
    'abstracts': {'batch_size': 25000, 'commit_batches': 3, 'extractors': 7},
    'authors': {'batch_size': 25000, 'commit_batches': 3, 'extractors': 7},
    'citations': {'batch_size': 8000, 'commit_batches': 2, 'extractors': 6},  # citationsæœ€å®¹æ˜“é‡å¤
    'paper_ids': {'batch_size': 25000, 'commit_batches': 3, 'extractors': 7},
    'publication_venues': {'batch_size': 25000, 'commit_batches': 3, 'extractors': 7},
    'tldrs': {'batch_size': 25000, 'commit_batches': 3, 'extractors': 7},
    # æ‰€æœ‰è¡¨ç»Ÿä¸€ç­–ç•¥ï¼šå°æ‰¹æ¬¡+é¢‘ç¹commitï¼Œå½»åº•é¿å…ç¼“å†²åŒºé—®é¢˜
}

DEFAULT_CONFIG = {'batch_size': 100000, 'commit_batches': 5, 'extractors': 6}
NUM_EXTRACTORS = 6  # é»˜è®¤è§£å‹è¿›ç¨‹æ•°ï¼ˆå·²ä¼˜åŒ–ï¼‰
QUEUE_SIZE = 80  # é˜Ÿåˆ—å¤§å°ï¼šå¢å¤§ç¼“å†²ï¼ˆå·²ä¼˜åŒ–ï¼‰
PROGRESS_FILE = 'logs/gz_progress.txt'

# æ€§èƒ½æå‡æ¨¡å¼ï¼ˆè­¦å‘Šï¼šTURBOæ¨¡å¼ä¼šç¦ç”¨WALæ—¥å¿—ï¼Œæ•°æ®åº“å´©æºƒæ—¶å¯èƒ½ä¸¢å¤±æ•°æ®ï¼‰
TURBO_MODE = False  # è®¾ç½®ä¸ºTrueå¯ç”¨æé€Ÿæ¨¡å¼ï¼ˆä»…å»ºè®®åˆæ¬¡æ‰¹é‡å¯¼å…¥æ—¶ä½¿ç”¨ï¼‰

# ä¸åŒè¡¨ä½¿ç”¨ä¸åŒçš„ä¸»é”®å­—æ®µ
TABLE_PRIMARY_KEY_MAP = {
    'authors': 'authorid',
    'citations': 'citedcorpusid',
    # å…¶ä»–è¡¨é»˜è®¤ä½¿ç”¨corpusid
}

# æ­£åˆ™è¡¨è¾¾å¼ï¼šå¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼ˆæ¯”å®Œæ•´JSONè§£æå¿«10å€ï¼‰
def get_key_pattern(field_name: str):
    """æ ¹æ®å­—æ®µåç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼"""
    return re.compile(rf'"{field_name}"\s*:\s*(\d+)', re.IGNORECASE)

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œåªæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
logging.basicConfig(
    level=logging.ERROR,
    format='%(message)s'
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # ä¸»è¿›ç¨‹å¯ä»¥INFO


# =============================================================================
# æ–­ç‚¹ç»­ä¼ 
# =============================================================================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        
        completed = set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    completed.add(line)
        
        if completed:
            logger.info(f"âœ“ å·²åŠ è½½è¿›åº¦: {len(completed)} ä¸ªæ–‡ä»¶å·²å®Œæˆ")
        return completed
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


# =============================================================================
# ç”Ÿäº§è€…ï¼šè§£å‹è¿›ç¨‹ï¼ˆå¤šä¸ªå¹¶è¡Œï¼‰
# =============================================================================

def extractor_worker(
    file_queue: Queue,
    data_queue: Queue,
    stats_dict: dict,
    table_name: str,
    batch_size: int = 10000
):
    """
    è§£å‹å·¥ä½œè¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
    ä»file_queueå–æ–‡ä»¶ï¼Œè§£å‹åæ”¾å…¥data_queue
    """
    # å®Œå…¨ç¦ç”¨æ­¤è¿›ç¨‹çš„æ—¥å¿—è¾“å‡º
    import logging
    logging.getLogger().setLevel(logging.CRITICAL)
    
    # æ ¹æ®è¡¨åç¡®å®šä¸»é”®å­—æ®µ
    primary_key_field = TABLE_PRIMARY_KEY_MAP.get(table_name, 'corpusid')
    key_pattern = get_key_pattern(primary_key_field)
    
    worker_name = f"Extractor-{id(file_queue) % 1000}"
    
    while True:
        try:
            # è·å–æ–‡ä»¶ä»»åŠ¡
            task = file_queue.get(timeout=1)
            if task is None:  # ç»“æŸä¿¡å·
                break
            
            gz_file_path, file_name = task
            start_time = time.time()
            
            try:
                batch = []
                line_count = 0
                valid_count = 0
                
                # æµå¼è§£å‹ - é‡åˆ°æŸåç›´æ¥è·³è¿‡ï¼Œä¸é‡è¯•
                try:
                    with gzip.open(gz_file_path, 'rt', encoding='utf-8', errors='ignore') as f:
                        for line in f:
                            line_count += 1
                            line = line.strip()
                            if not line:
                                continue
                            
                            # æ­£åˆ™å¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼ˆé¿å…å®Œæ•´JSONè§£æï¼‰
                            match = key_pattern.search(line)
                            if not match:
                                continue
                            
                            key_value = int(match.group(1))
                            valid_count += 1
                            
                            # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨åŸå§‹è¡Œï¼Œé¿å…ä¸å¿…è¦çš„è½¬ä¹‰æ£€æŸ¥
                            # PostgreSQL COPYå¯ä»¥å¤„ç†å¤§å¤šæ•°JSONå­—ç¬¦
                            batch.append((key_value, line))
                            
                            # æ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€åˆ°é˜Ÿåˆ—
                            if len(batch) >= batch_size:
                                data_queue.put(('data', file_name, batch))
                                batch = []
                    
                    # å‘é€å‰©ä½™æ•°æ®
                    if batch:
                        data_queue.put(('data', file_name, batch))
                    
                    # å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                    data_queue.put(('done', file_name, valid_count))
                    
                    # æ›´æ–°ç»Ÿè®¡
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError) as gz_error:
                    # GZIPæ–‡ä»¶æŸåï¼Œç›´æ¥è·³è¿‡ï¼Œä¸é‡è¯•
                    data_queue.put(('error', file_name, f"Corrupted"))
                    continue
                
            except Exception as e:
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


# =============================================================================
# æ¶ˆè´¹è€…ï¼šæ’å…¥è¿›ç¨‹ï¼ˆå•ä¸ªï¼Œé«˜æ•ˆæ‰¹é‡æ’å…¥ï¼‰
# =============================================================================

def inserter_worker(
    data_queue: Queue,
    table_name: str,
    stats_dict: dict,
    tracker: ProgressTracker,
    use_upsert: bool = False,
    commit_batches: int = 3,
    total_files: int = 0,
    turbo_mode: bool = False,
    primary_key: str = 'corpusid'
):
    """
    æ’å…¥å·¥ä½œè¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
    æŒç»­ä»data_queueå–æ•°æ®å¹¶æ‰¹é‡æ’å…¥
    
    Args:
        primary_key: ä¸»é”®å­—æ®µåï¼ˆé»˜è®¤corpusidï¼‰
    """
    print("\nğŸš€ æ•°æ®æ’å…¥è¿›ç¨‹å·²å¯åŠ¨\n")
    
    try:
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        conn = psycopg2.connect(**DB_CONFIG)
        conn.autocommit = False
        cursor = conn.cursor()
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®ï¼ˆå¹³è¡¡ï¼šæ€§èƒ½ vs å†…å­˜å®‰å…¨ï¼‰
        try:
            cursor.execute("SET synchronous_commit = OFF")  # å¼‚æ­¥æäº¤ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
            cursor.execute("SET commit_delay = 100000")  # å»¶è¿Ÿæäº¤100msï¼ˆPostgreSQLæœ€å¤§å€¼ï¼‰
            cursor.execute("SET maintenance_work_mem = '4GB'")  # ç»´æŠ¤å†…å­˜ï¼ˆå®‰å…¨å€¼ï¼‰
            cursor.execute("SET work_mem = '1GB'")  # å·¥ä½œå†…å­˜ï¼ˆå‡å°ï¼Œé¿å…ç¼“å†²åŒºè€—å°½ï¼‰
            cursor.execute("SET temp_buffers = '8GB'")  # ä¸´æ—¶ç¼“å†²åŒºï¼ˆç¿»å€ï¼Œå…³é”®ï¼ï¼‰
            cursor.execute("SET effective_cache_size = '24GB'")  # ç¼“å­˜å¤§å°ï¼ˆå‡è®¾ç³»ç»Ÿ32GBå†…å­˜ï¼‰
            cursor.execute("SET max_parallel_workers_per_gather = 0")  # å…³é—­å¹¶è¡Œï¼ˆæ‰¹é‡æ’å…¥ä¸éœ€è¦ï¼‰
        except Exception as e:
            conn.rollback()  # å›æ»šå¤±è´¥çš„è®¾ç½®
            logger.warning(f"éƒ¨åˆ†æ€§èƒ½é…ç½®å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        # WALè®¾ç½®ï¼ˆå¯èƒ½å¤±è´¥ï¼Œå•ç‹¬å¤„ç†ï¼‰
        try:
            cursor.execute("SET wal_writer_delay = '1000ms'")
        except Exception:
            conn.rollback()  # å›æ»šå¹¶ç»§ç»­
        
        # TURBOæ¨¡å¼ï¼šä¸´æ—¶ç¦ç”¨WALï¼ˆæé€Ÿä½†æœ‰é£é™©ï¼‰
        if turbo_mode and not use_upsert:
            print("âš ï¸  TURBOæ¨¡å¼å·²å¯ç”¨ - è¡¨å°†ä¸´æ—¶è®¾ä¸ºUNLOGGEDï¼ˆæ•°æ®åº“å´©æºƒå¯èƒ½ä¸¢å¤±æ•°æ®ï¼‰")
            try:
                cursor.execute(f"ALTER TABLE {table_name} SET UNLOGGED")
                conn.commit()
            except Exception as e:
                conn.rollback()  # å…³é”®ï¼šå›æ»šå¤±è´¥çš„äº‹åŠ¡
                print(f"âš ï¸  æ— æ³•å¯ç”¨UNLOGGEDæ¨¡å¼: {e}")
                print("âš ï¸  ç»§ç»­ä½¿ç”¨LOGGEDæ¨¡å¼ï¼ˆæ€§èƒ½ä¼šç¨æ…¢ï¼‰")
        
        # ç¦ç”¨è§¦å‘å™¨ï¼ˆINSERTæ¨¡å¼ï¼‰
        if not use_upsert:
            try:
                cursor.execute(f"ALTER TABLE {table_name} DISABLE TRIGGER ALL")
                conn.commit()
            except Exception as e:
                conn.rollback()
                logger.warning(f"ç¦ç”¨è§¦å‘å™¨å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        total_inserted = 0
        file_stats = {}  # {file_name: inserted_count}
        completed_files = 0
        last_log_time = time.time()
        start_time = time.time()
        batch_count = 0  # æ‰¹æ¬¡è®¡æ•°å™¨
        
        while True:
            try:
                # éé˜»å¡è·å–ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
                item = data_queue.get(timeout=5)
                
                item_type = item[0]
                
                if item_type == 'stop':
                    logger.info("[Inserter] æ”¶åˆ°åœæ­¢ä¿¡å·")
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    try:
                        # æ‰¹é‡æ’å…¥ï¼ˆä¼ å…¥ä¸»é”®å­—æ®µï¼‰
                        inserted = batch_insert_copy(cursor, table_name, batch, use_upsert, primary_key)
                        batch_count += 1
                        
                        # æ¯Nä¸ªæ‰¹æ¬¡commitä¸€æ¬¡ï¼Œå‡å°‘commitå¼€é”€
                        if batch_count >= commit_batches:
                            conn.commit()
                            batch_count = 0
                        
                        total_inserted += inserted
                        file_stats[file_name] = file_stats.get(file_name, 0) + inserted
                    
                    except Exception as insert_error:
                        # æ’å…¥å¤±è´¥ï¼Œå›æ»šå½“å‰äº‹åŠ¡
                        conn.rollback()
                        batch_count = 0  # é‡ç½®æ‰¹æ¬¡è®¡æ•°
                        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼ˆå·²å›æ»šï¼‰: {insert_error}")
                        # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                        continue
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯10ç§’ï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 3:
                        elapsed = current_time - start_time
                        rate = total_inserted / elapsed if elapsed > 0 else 0
                        
                        # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
                        progress_pct = (completed_files / total_files * 100) if total_files > 0 else 0
                        remaining_files = total_files - completed_files
                        eta_seconds = (remaining_files * elapsed / completed_files) if completed_files > 0 else 0
                        eta_hours = int(eta_seconds / 3600)
                        eta_mins = int((eta_seconds % 3600) / 60)
                        eta_str = f"{eta_hours}h{eta_mins}m" if eta_hours > 0 else f"{eta_mins}åˆ†"
                        
                        print(f"\rğŸ“Š [{completed_files}/{total_files}] {progress_pct:.1f}% | "
                              f"{total_inserted:,}æ¡ | {rate:.0f}æ¡/ç§’ | "
                              f"å‰©ä½™: {eta_str}    ", end='', flush=True)
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    # æ ‡è®°æ–‡ä»¶å®Œæˆ
                    tracker.mark_completed(file_name)
                    completed_files += 1
                    inserted = file_stats.get(file_name, 0)
                
                elif item_type == 'error':
                    _, file_name, error = item
                    # æ ‡è®°æŸåæ–‡ä»¶ä¸ºå·²å®Œæˆï¼Œé¿å…é‡å¤å¤„ç†
                    tracker.mark_completed(file_name)
                    completed_files += 1
            
            except Empty:
                # é˜Ÿåˆ—ç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"[Inserter] å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                conn.rollback()
                continue
        
        # æäº¤å‰©ä½™æœªcommitçš„æ•°æ®
        if batch_count > 0:
            conn.commit()
        
        # æ¢å¤è¡¨çŠ¶æ€
        if not use_upsert:
            # æ¢å¤LOGGEDçŠ¶æ€ï¼ˆå¦‚æœä¹‹å‰è®¾ä¸ºUNLOGGEDï¼‰
            if turbo_mode:
                print("\nğŸ”„ æ¢å¤è¡¨ä¸ºLOGGEDçŠ¶æ€...")
                try:
                    cursor.execute(f"ALTER TABLE {table_name} SET LOGGED")
                    conn.commit()
                    print("âœ… è¡¨å·²æ¢å¤LOGGEDçŠ¶æ€")
                except Exception as e:
                    print(f"âš ï¸  æ¢å¤LOGGEDå¤±è´¥: {e}")
            
            # å¯ç”¨è§¦å‘å™¨
            cursor.execute(f"ALTER TABLE {table_name} ENABLE TRIGGER ALL")
            conn.commit()
        
        elapsed = time.time() - start_time
        rate = total_inserted / elapsed if elapsed > 0 else 0
        
        print(f"\n\nâœ… æ’å…¥å®Œæˆ: {total_inserted:,}æ¡ | å¹³å‡é€Ÿåº¦: {rate:.0f}æ¡/ç§’ | ç”¨æ—¶: {elapsed/60:.1f}åˆ†é’Ÿ\n")
        
        cursor.close()
        conn.close()
        
        stats_dict['inserted'] = total_inserted
        
    except Exception as e:
        logger.error(f"[Inserter] ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def batch_insert_copy(cursor, table_name: str, batch: list, use_upsert: bool = False, primary_key: str = 'corpusid') -> int:
    """
    ä½¿ç”¨COPYæ‰¹é‡æ’å…¥ï¼ˆæœ€å¿«æ–¹æ³•ï¼‰
    
    æ•°æ®ä»¥TEXTæ ¼å¼å­˜å‚¨ï¼ˆä¸éªŒè¯ä¸è§£æï¼Œæé€Ÿï¼‰
    
    æ³¨æ„ï¼š
    - authorsè¡¨ï¼šä¸»é”®åˆ—åæ˜¯authorid
    - citationsè¡¨ï¼šä¸»é”®åˆ—åæ˜¯citedcorpusid
    - å…¶ä»–è¡¨ï¼šä¸»é”®åˆ—åæ˜¯corpusid
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_name: è¡¨å
        batch: æ•°æ®æ‰¹æ¬¡ [(key_value, json_line), ...] - key_valueå·²ä»JSONæ­£ç¡®å­—æ®µæå–
        use_upsert: æ˜¯å¦ä½¿ç”¨UPSERTæ¨¡å¼
        primary_key: ä¸»é”®åˆ—åï¼ˆauthorid/citedcorpusid/corpusidï¼‰
    """
    if not batch:
        return 0
    
    from io import StringIO
    import psycopg2.errors
    
    try:
        if use_upsert:
            # UPSERTæ¨¡å¼ - ä¼˜åŒ–ç‰ˆæœ¬
            # 1. æ‰¹å†…å»é‡ï¼ˆå­—å…¸æ›´å¿«ï¼‰
            seen = {}
            for key_value, data in batch:
                seen[key_value] = data
            
            # 2. ä½¿ç”¨VALUESæ‰¹é‡UPSERTï¼ˆé¿å…ä¸´æ—¶è¡¨è€—å°½ç¼“å†²åŒºï¼‰
            # åˆ†å°æ‰¹æ¬¡æ’å…¥ï¼Œæ¯æ¬¡æœ€å¤š1000æ¡
            chunk_size = 1000
            total_processed = 0
            
            for i in range(0, len(seen), chunk_size):
                chunk_items = list(seen.items())[i:i + chunk_size]
                
                # æ„å»ºVALUESå­å¥
                values_list = []
                for key_val, data in chunk_items:
                    # è½¬ä¹‰å•å¼•å·
                    escaped_data = data.replace("'", "''")
                    values_list.append(f"({key_val}, '{escaped_data}', NOW(), NOW())")
                
                values_clause = ','.join(values_list)
                
                # æ‰¹é‡UPSERT
                cursor.execute(f"""
                    INSERT INTO {table_name} ({primary_key}, data, insert_time, update_time)
                    VALUES {values_clause}
                    ON CONFLICT ({primary_key}) DO UPDATE SET
                        data = EXCLUDED.data,
                        update_time = EXCLUDED.update_time
                """)
                total_processed += len(chunk_items)
            
            return total_processed
        else:
            # INSERTæ¨¡å¼ï¼šæé€ŸCOPYï¼ˆTEXTç±»å‹ï¼Œä¸éªŒè¯ä¸è§£æï¼Œæœ€å¿«ï¼‰
            buffer = StringIO()
            lines = [f"{key_val}\t{data}\t\\N\t\\N\n" for key_val, data in batch]
            buffer.write(''.join(lines))
            buffer.seek(0)
            
            try:
                cursor.copy_expert(
                    f"""
                    COPY {table_name} ({primary_key}, data, insert_time, update_time)
                    FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')
                    """,
                    buffer
                )
                return len(batch)
            except psycopg2.errors.UniqueViolation:
                # æœ‰é‡å¤keyï¼Œä½¿ç”¨VALUESé€æ¡æ’å…¥ï¼ˆé¿å…ä¸´æ—¶è¡¨è€—å°½ç¼“å†²åŒºï¼‰
                cursor.connection.rollback()
                
                # æ‰¹é‡VALUESæ’å…¥ï¼ŒON CONFLICT DO NOTHINGï¼ˆä¸ä½¿ç”¨ä¸´æ—¶è¡¨ï¼‰
                inserted_count = 0
                # åˆ†å°æ‰¹æ¬¡æ’å…¥ï¼Œæ¯æ¬¡æœ€å¤š1000æ¡
                chunk_size = 1000
                for i in range(0, len(batch), chunk_size):
                    chunk = batch[i:i + chunk_size]
                    
                    # æ„å»ºVALUESå­å¥
                    values_list = []
                    for key_val, data in chunk:
                        # è½¬ä¹‰å•å¼•å·
                        escaped_data = data.replace("'", "''")
                        values_list.append(f"({key_val}, '{escaped_data}', NOW(), NOW())")
                    
                    values_clause = ','.join(values_list)
                    
                    try:
                        cursor.execute(f"""
                            INSERT INTO {table_name} ({primary_key}, data, insert_time, update_time)
                            VALUES {values_clause}
                            ON CONFLICT ({primary_key}) DO NOTHING
                        """)
                        inserted_count += len(chunk)
                    except Exception as e:
                        # å¦‚æœVALUESä¹Ÿå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªchunk
                        logger.warning(f"VALUESæ’å…¥å¤±è´¥ï¼Œè·³è¿‡{len(chunk)}æ¡: {e}")
                        continue
                
                return inserted_count
        
    except psycopg2.errors.UniqueViolation:
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¯´æ˜äº‹åŠ¡å·²ä¸­æ­¢ï¼Œéœ€è¦å¤–å±‚å¤„ç†
        raise
    except Exception as e:
        # å…¶ä»–é”™è¯¯
        import traceback
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
        raise


# =============================================================================
# ä¸»åè°ƒå™¨
# =============================================================================

def process_gz_folder_pipeline(
    folder_path: str,
    table_name: str,
    use_upsert: bool = False,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    reset_progress: bool = False,
    turbo_mode: bool = False
):
    """
    æµæ°´çº¿å¹¶è¡Œå¤„ç†ï¼šå¤šä¸ªè§£å‹è¿›ç¨‹ + å•ä¸ªæ’å…¥è¿›ç¨‹
    """
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    tracker = ProgressTracker(PROGRESS_FILE)
    
    if reset_progress:
        tracker.reset()
    
    completed_files = tracker.load_completed() if resume else set()
    
    # æ‰«æGZæ–‡ä»¶
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ°.gzæ–‡ä»¶: {folder_path}")
        return
    
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in completed_files]
    
    # è·å–è¯¥è¡¨ä½¿ç”¨çš„ä¸»é”®å­—æ®µ
    primary_key_field = TABLE_PRIMARY_KEY_MAP.get(table_name, 'corpusid')
    
    logger.info(f"\n{'='*80}")
    logger.info(f"æµæ°´çº¿å¹¶è¡Œå¤„ç† GZ æ–‡ä»¶ï¼ˆç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼‰")
    logger.info(f"{'='*80}")
    logger.info(f"æ–‡ä»¶å¤¹: {folder_path}")
    logger.info(f"ç›®æ ‡è¡¨: {table_name}")
    logger.info(f"ä¸»é”®å­—æ®µ: {primary_key_field}")
    # æ ¹æ®è¡¨åè·å–ä¼˜åŒ–é…ç½®
    config = TABLE_CONFIGS.get(table_name, DEFAULT_CONFIG)
    batch_size = config['batch_size']
    commit_batches = config['commit_batches']
    # å¦‚æœç”¨æˆ·æ²¡æŒ‡å®šextractorsï¼Œä½¿ç”¨é…ç½®ä¸­çš„å€¼
    if num_extractors == NUM_EXTRACTORS:  # é»˜è®¤å€¼
        num_extractors = config['extractors']
    
    logger.info(f"æ€»æ–‡ä»¶æ•°: {len(gz_files)}")
    logger.info(f"å·²å®Œæˆ: {len(completed_files)}")
    logger.info(f"å¾…å¤„ç†: {len(pending_files)}")
    logger.info(f"ä¼˜åŒ–é…ç½®: æ‰¹æ¬¡={batch_size:,}, commité—´éš”={commit_batches}, è¿›ç¨‹={num_extractors}")
    logger.info(f"æ¨¡å¼: {'UPSERT' if use_upsert else 'INSERT (COPY)'}")
    if turbo_mode:
        logger.warning(f"âš ï¸  TURBOæ¨¡å¼: å·²å¯ç”¨ï¼ˆè¡¨å°†ä¸´æ—¶è®¾ä¸ºUNLOGGEDï¼Œæå‡æ€§èƒ½ä½†æœ‰é£é™©ï¼‰")
    logger.info(f"{'='*80}\n")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼")
        return
    
    overall_start = time.time()
    
    try:
        # åˆ›å»ºé˜Ÿåˆ—
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
        manager = Manager()
        stats_dict = manager.dict()
        
        # æ·»åŠ æ–‡ä»¶ä»»åŠ¡
        for task in pending_files:
            file_queue.put(task)
        
        # æ·»åŠ ç»“æŸä¿¡å·ï¼ˆæ¯ä¸ªè§£å‹è¿›ç¨‹ä¸€ä¸ªï¼‰
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨æ’å…¥è¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
        inserter = Process(
            target=inserter_worker,
            args=(data_queue, table_name, stats_dict, tracker, use_upsert, commit_batches, len(pending_files), turbo_mode, primary_key_field),
            name='Inserter'
        )
        inserter.start()
        
        # å¯åŠ¨è§£å‹è¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, table_name, batch_size),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        print(f"âœ“ å·²å¯åŠ¨ {num_extractors} ä¸ªè§£å‹è¿›ç¨‹ + 1 ä¸ªæ’å…¥è¿›ç¨‹")
        
        # ç­‰å¾…æ‰€æœ‰è§£å‹è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join()
        
        logger.info("âœ“ æ‰€æœ‰è§£å‹è¿›ç¨‹å·²å®Œæˆ")
        
        # å‘é€åœæ­¢ä¿¡å·ç»™æ’å…¥è¿›ç¨‹
        data_queue.put(('stop', None, None))
        
        # ç­‰å¾…æ’å…¥è¿›ç¨‹å®Œæˆ
        inserter.join()
        
        elapsed = time.time() - overall_start
        total_inserted = stats_dict.get('inserted', 0)
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… å…¨éƒ¨å®Œæˆï¼")
        logger.info(f"  æ–‡ä»¶æ•°: {len(pending_files)}")
        logger.info(f"  æ€»æ’å…¥: {total_inserted:,} æ¡")
        logger.info(f"  æ€»è€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"  å¹³å‡é€Ÿåº¦: {avg_rate:.0f} æ¡/ç§’")
        logger.info(f"{'='*80}\n")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥ï¼ˆç›®æ ‡ï¼š3000-5000æ¡/ç§’ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼š
  ğŸš€ ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨å¹¶è¡Œ
  ğŸš€ æ­£åˆ™å¿«é€Ÿæå–ï¼šé¿å…å®Œæ•´JSONè§£æï¼ˆæé€Ÿ10å€ï¼‰
  ğŸš€ æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  ğŸš€ é˜Ÿåˆ—ç¼“å†²ï¼šæŒç»­ä¾›åº”æ•°æ®ï¼Œæ— ç©ºé—²ç­‰å¾…
  ğŸš€ å¤šè¿›ç¨‹è§£å‹ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
  ğŸš€ çµæ´»ä¸»é”®é…ç½®ï¼šæ ¹æ®è¡¨åè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„ä¸»é”®å­—æ®µ
     - authorsè¡¨ä½¿ç”¨authorid
     - citationsè¡¨ä½¿ç”¨citedcorpusid
     - å…¶ä»–è¡¨ä½¿ç”¨corpusid

ç¤ºä¾‹ï¼š
  # å¤„ç†papersæ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨corpusidä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\papers" --table papers
  
  # å¤„ç†authorsæ–‡ä»¶å¤¹ï¼ˆè‡ªåŠ¨ä½¿ç”¨authoridä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\authors" --table authors
  
  # å¤„ç†citationsæ–‡ä»¶å¤¹ï¼ˆè‡ªåŠ¨ä½¿ç”¨citedcorpusidä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\citations" --table citations
  
  # è‡ªå®šä¹‰è§£å‹è¿›ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\s2orc" --table s2orc --extractors 8
  
  # ä¸­æ–­åç»§ç»­
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\papers" --table papers --resume
        """
    )
    
    parser.add_argument('--dir', type=str, required=True,
                       help='GZæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--table', type=str, required=True,
                       choices=FIELD_TABLES,
                       help='ç›®æ ‡æ•°æ®åº“è¡¨å')
    parser.add_argument('--upsert', action='store_true',
                       help='ä½¿ç”¨UPSERTæ¨¡å¼')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--resume', action='store_true',
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true',
                       help='é‡ç½®è¿›åº¦')
    parser.add_argument('--turbo', action='store_true',
                       help='å¯ç”¨TURBOæ¨¡å¼ï¼ˆä¸´æ—¶å°†è¡¨è®¾ä¸ºUNLOGGEDï¼Œæé€Ÿä½†æœ‰é£é™©ï¼‰')
    
    parser.set_defaults(resume=True)
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¤„ç†
    process_gz_folder_pipeline(
        folder_path=args.dir,
        table_name=args.table,
        use_upsert=args.upsert,
        num_extractors=args.extractors,
        resume=args.resume,
        reset_progress=args.reset,
        turbo_mode=args.turbo
    )


if __name__ == '__main__':
    main()

