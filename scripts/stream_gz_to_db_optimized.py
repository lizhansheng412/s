#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥
æ€§èƒ½ä¼˜åŒ–ï¼š
  âœ… ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨åˆ†ç¦»å¹¶è¡Œ
  âœ… æ­£åˆ™å¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼šé¿å…å®Œæ•´JSONè§£æ
  âœ… æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  âœ… é˜Ÿåˆ—ç¼“å†²ï¼šè§£å‹å¿«æ—¶ä¸ç­‰å¾…ï¼Œæ’å…¥å¿«æ—¶ä¸ç©ºé—²
  âœ… æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­æ¢å¤
  âœ… çµæ´»ä¸»é”®é…ç½®ï¼šä¸åŒè¡¨ä½¿ç”¨ä¸åŒä¸»é”®å­—æ®µ
     - authorsè¡¨: authorid
     - citationsè¡¨: citedcorpusid
     - å…¶ä»–è¡¨: corpusid
  
ç›®æ ‡ï¼š10å€æ€§èƒ½æå‡ï¼ˆ3000-5000æ¡/ç§’ï¼‰
"""

import gzip
import re
import sys
import time
import logging
from pathlib import Path
from typing import Optional, Set
from multiprocessing import Process, Queue, Manager, cpu_count
from queue import Empty

import psycopg2

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from database.config.db_config_v2 import DB_CONFIG, FIELD_TABLES

# =============================================================================
# æ ¹æ®æ•°æ®å¤§å°åŠ¨æ€é…ç½®ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
# =============================================================================

# é’ˆå¯¹TEXTç±»å‹ä¼˜åŒ–é…ç½®ï¼ˆå½»åº•é¿å…ç¼“å†²åŒºé—®é¢˜ï¼šä¸ä½¿ç”¨ä¸´æ—¶è¡¨ï¼Œä½¿ç”¨VALUESï¼‰
TABLE_CONFIGS = {
    # è¶…å¤§æ•°æ® (60-120KB/æ¡): s2orcç³»åˆ— 
    's2orc': {'batch_size': 1000, 'commit_batches': 3, 'extractors': 1},
    's2orc_v2': {'batch_size': 1000, 'commit_batches': 3, 'extractors': 1},
    # 1000æ¡Ã—100KBÃ—3æ‰¹=300MB/æ‰¹æ¬¡
        
    # ä¸­ç­‰æ•°æ® (16KB/æ¡): embeddingsç³»åˆ— 
    'embeddings_specter_v1': {'batch_size': 6250, 'commit_batches': 3, 'extractors': 1},
    'embeddings_specter_v2': {'batch_size': 6250, 'commit_batches': 3, 'extractors': 1},
    # 6250æ¡Ã—16KBÃ—3æ‰¹=300MB/æ‰¹æ¬¡
    
    # å°æ•°æ® (1-3KB/æ¡): å…¶ä»–è¡¨
    'papers': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'abstracts': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'authors': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'citations': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 1},
    'paper_ids': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'publication_venues': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    'tldrs': {'batch_size': 50000, 'commit_batches': 3, 'extractors': 2},
    # 50,000æ¡Ã—2KBÃ—3æ‰¹=300MB/æ‰¹æ¬¡
}

DEFAULT_CONFIG = {'batch_size': 20000, 'commit_batches': 1, 'extractors': 1}
NUM_EXTRACTORS = 1
QUEUE_SIZE = 20
# æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå°†æ ¹æ®è¡¨ååŠ¨æ€ç”Ÿæˆï¼‰
PROGRESS_DIR = 'logs/progress'
FAILED_DIR = 'logs/failed'

# ä¸åŒè¡¨ä½¿ç”¨ä¸åŒçš„ä¸»é”®å­—æ®µ
TABLE_PRIMARY_KEY_MAP = {
    'authors': 'authorid',
    'citations': 'citedcorpusid',
    # å…¶ä»–è¡¨é»˜è®¤ä½¿ç”¨corpusid
}

# æ­£åˆ™è¡¨è¾¾å¼ï¼šå¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼ˆæ¯”å®Œæ•´JSONè§£æå¿«10å€ï¼‰
def get_key_pattern(field_name: str):
    """æ ¹æ®å­—æ®µåç”Ÿæˆæ­£åˆ™è¡¨è¾¾å¼"""
    return re.compile(rf'"{field_name}"\s*:\s*(\d+)', re.IGNORECASE)

def get_log_files(table_name: str):
    """
    æ ¹æ®è¡¨åè·å–ä¸“å±çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    
    Args:
        table_name: è¡¨å
    
    Returns:
        (progress_file, failed_file) å…ƒç»„
    """
    progress_dir = Path(PROGRESS_DIR)
    failed_dir = Path(FAILED_DIR)
    
    # åˆ›å»ºç›®å½•
    progress_dir.mkdir(parents=True, exist_ok=True)
    failed_dir.mkdir(parents=True, exist_ok=True)
    
    progress_file = progress_dir / f"{table_name}_progress.txt"
    failed_file = failed_dir / f"{table_name}_failed.txt"
    
    return str(progress_file), str(failed_file)

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºERRORï¼Œåªæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
logging.basicConfig(
    level=logging.ERROR,
    format='%(message)s'
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # ä¸»è¿›ç¨‹å¯ä»¥INFO


# =============================================================================
# æ–­ç‚¹ç»­ä¼ 
# =============================================================================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        
        completed = set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    completed.add(line)
        
        return completed
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


class FailedFilesLogger:
    """å¤±è´¥æ–‡ä»¶è®°å½•å™¨"""
    
    def __init__(self, failed_file: str):
        self.failed_file = Path(failed_file)
        self.failed_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_failed(self) -> Set[str]:
        """åŠ è½½å·²çŸ¥å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨"""
        if not self.failed_file.exists():
            return set()
        
        failed = set()
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # æå–æ–‡ä»¶åï¼ˆæ ¼å¼ï¼šæ—¶é—´æˆ³ | æ–‡ä»¶å | é”™è¯¯ä¿¡æ¯ï¼‰
                    parts = line.split('|')
                    if len(parts) >= 2:
                        failed.add(parts[1].strip())
        
        return failed
    
    def log_failed(self, file_name: str, error: str):
        """è®°å½•å¤±è´¥æ–‡ä»¶"""
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(self.failed_file, 'a', encoding='utf-8') as f:
            f.write(f"{timestamp} | {file_name} | {error}\n")
            f.flush()
    
    def reset(self):
        if self.failed_file.exists():
            self.failed_file.unlink()


# =============================================================================
# ç”Ÿäº§è€…ï¼šè§£å‹è¿›ç¨‹ï¼ˆå¤šä¸ªå¹¶è¡Œï¼‰
# =============================================================================

def extractor_worker(
    file_queue: Queue,
    data_queue: Queue,
    stats_dict: dict,
    table_name: str,
    batch_size: int = 10000
):
    """
    è§£å‹å·¥ä½œè¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
    ä»file_queueå–æ–‡ä»¶ï¼Œè§£å‹åæ”¾å…¥data_queue
    """
    # å®Œå…¨ç¦ç”¨æ­¤è¿›ç¨‹çš„æ—¥å¿—è¾“å‡º
    import logging
    logging.getLogger().setLevel(logging.CRITICAL)
    
    # æ ¹æ®è¡¨åç¡®å®šä¸»é”®å­—æ®µ
    primary_key_field = TABLE_PRIMARY_KEY_MAP.get(table_name, 'corpusid')
    key_pattern = get_key_pattern(primary_key_field)
    
    worker_name = f"Extractor-{id(file_queue) % 1000}"
    
    while True:
        try:
            # è·å–æ–‡ä»¶ä»»åŠ¡
            task = file_queue.get(timeout=1)
            if task is None:  # ç»“æŸä¿¡å·
                break
            
            gz_file_path, file_name = task
            start_time = time.time()
            
            try:
                batch = []
                line_count = 0
                valid_count = 0
                
                # æµå¼è§£å‹ - é‡åˆ°æŸåç›´æ¥è·³è¿‡ï¼Œä¸é‡è¯•
                # USBç›˜ä¼˜åŒ–ï¼šä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼+æ›´å¤§ç¼“å†²åŒºï¼Œå‡å°‘éšæœºI/O
                try:
                    import io
                    # ä½¿ç”¨äºŒè¿›åˆ¶æ¨¡å¼æ‰“å¼€ï¼Œè®¾ç½®8MBç¼“å†²åŒº
                    with gzip.open(gz_file_path, 'rb') as f_binary:
                        # åŒ…è£…ä¸ºå¸¦å¤§ç¼“å†²åŒºçš„æ–‡æœ¬æµ
                        f = io.TextIOWrapper(io.BufferedReader(f_binary, buffer_size=8*1024*1024), 
                                            encoding='utf-8', errors='ignore')
                        for line in f:
                            line_count += 1
                            line = line.strip()
                            if not line:
                                continue
                            
                            # æ­£åˆ™å¿«é€Ÿæå–ä¸»é”®å­—æ®µï¼ˆé¿å…å®Œæ•´JSONè§£æï¼‰
                            match = key_pattern.search(line)
                            if not match:
                                continue
                            
                            key_value = int(match.group(1))
                            valid_count += 1
                            
                            # ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨åŸå§‹è¡Œï¼Œé¿å…ä¸å¿…è¦çš„è½¬ä¹‰æ£€æŸ¥
                            # PostgreSQL COPYå¯ä»¥å¤„ç†å¤§å¤šæ•°JSONå­—ç¬¦
                            batch.append((key_value, line))
                            
                            # æ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€åˆ°é˜Ÿåˆ—
                            if len(batch) >= batch_size:
                                data_queue.put(('data', file_name, batch))
                                batch = []
                    
                    # å‘é€å‰©ä½™æ•°æ®
                    if batch:
                        data_queue.put(('data', file_name, batch))
                    
                    # å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                    data_queue.put(('done', file_name, valid_count))
                    
                    # æ›´æ–°ç»Ÿè®¡
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError) as gz_error:
                    # GZIPæ–‡ä»¶æŸåï¼Œç›´æ¥è·³è¿‡ï¼Œä¸é‡è¯•
                    data_queue.put(('error', file_name, f"Corrupted"))
                    continue
                
            except Exception as e:
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


# =============================================================================
# æ¶ˆè´¹è€…ï¼šæ’å…¥è¿›ç¨‹ï¼ˆå•ä¸ªï¼Œé«˜æ•ˆæ‰¹é‡æ’å…¥ï¼‰
# =============================================================================

def inserter_worker(
    data_queue: Queue,
    table_name: str,
    stats_dict: dict,
    tracker: ProgressTracker,
    failed_logger: FailedFilesLogger,
    use_upsert: bool = False,
    commit_batches: int = 3,
    total_files: int = 0,
    primary_key: str = 'corpusid'
):
    """
    æ’å…¥å·¥ä½œè¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
    æŒç»­ä»data_queueå–æ•°æ®å¹¶æ‰¹é‡æ’å…¥
    
    Args:
        primary_key: ä¸»é”®å­—æ®µåï¼ˆé»˜è®¤corpusidï¼‰
    """
    try:
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        conn = psycopg2.connect(**DB_CONFIG)
        conn.autocommit = False
        cursor = conn.cursor()
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®ï¼ˆåˆ†ç¦»è¯»å†™è·¯å¾„åçš„ä¼˜åŒ–é…ç½®ï¼Œå……åˆ†åˆ©ç”¨8æ ¸32GBï¼‰
        try:
            cursor.execute("SET synchronous_commit = OFF")  # å¼‚æ­¥æäº¤ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
            cursor.execute("SET commit_delay = 100000")  # å»¶è¿Ÿæäº¤100ms
            cursor.execute("SET maintenance_work_mem = '2GB'")  # ç»´æŠ¤å†…å­˜ï¼ˆé€‚ä¸­ï¼Œé¿å…è„é¡µç§¯å‹ï¼‰
            cursor.execute("SET work_mem = '512MB'")  # å·¥ä½œå†…å­˜ï¼ˆé€‚ä¸­ï¼‰
            cursor.execute("SET temp_buffers = '4GB'")  # ä¸´æ—¶ç¼“å†²åŒº
            cursor.execute("SET effective_cache_size = '16GB'")  # ç¼“å­˜å¤§å°
            cursor.execute("SET max_parallel_workers_per_gather = 0")  # å…³é—­å¹¶è¡Œ
            # ä¸´æ—¶è¡¨ç©ºé—´ä½¿ç”¨Dç›˜ï¼ˆå‰æï¼šå·²åˆ›å»ºd1_tempè¡¨ç©ºé—´ï¼‰
            try:
                cursor.execute("SET temp_tablespaces = 'd1_temp'")
            except:
                pass  # å¦‚æœè¡¨ç©ºé—´ä¸å­˜åœ¨ï¼Œå¿½ç•¥
        except Exception as e:
            conn.rollback()
            logger.warning(f"éƒ¨åˆ†æ€§èƒ½é…ç½®å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        # è¿™äº›å‚æ•°éœ€è¦åœ¨postgresql.confä¸­è®¾ç½®ï¼Œä¸èƒ½åœ¨ä¼šè¯çº§åˆ«ä¿®æ”¹ï¼Œæ³¨é‡Šæ‰é¿å…è­¦å‘Š
        # bgwriter_delay = 1000ms  # åå°å†™å…¥å»¶è¿Ÿï¼ˆå‡å°‘éšæœºå†™ï¼‰
        # checkpoint_timeout = 30min  # æ£€æŸ¥ç‚¹é—´éš”ï¼ˆå‡å°‘åˆ·ç›˜ï¼‰
        # checkpoint_completion_target = 0.9  # æ£€æŸ¥ç‚¹å®Œæˆç›®æ ‡
        
        # WALè®¾ç½®ï¼ˆå¯èƒ½å¤±è´¥ï¼Œå•ç‹¬å¤„ç†ï¼‰
        try:
            cursor.execute("SET wal_writer_delay = '1000ms'")
        except Exception:
            conn.rollback()  # å›æ»šå¹¶ç»§ç»­
        
        # ç¦ç”¨è§¦å‘å™¨ï¼ˆINSERTæ¨¡å¼ï¼‰
        if not use_upsert:
            try:
                cursor.execute(f"ALTER TABLE {table_name} DISABLE TRIGGER ALL")
                conn.commit()
            except Exception as e:
                conn.rollback()
                logger.warning(f"ç¦ç”¨è§¦å‘å™¨å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        total_inserted = 0
        file_stats = {}  # {file_name: inserted_count}
        completed_files = 0
        last_log_time = time.time()
        start_time = time.time()
        batch_count = 0  # æ‰¹æ¬¡è®¡æ•°å™¨
        
        while True:
            try:
                # éé˜»å¡è·å–ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
                item = data_queue.get(timeout=5)
                
                item_type = item[0]
                
                if item_type == 'stop':
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    try:
                        # æ‰¹é‡æ’å…¥ï¼ˆä¼ å…¥ä¸»é”®å­—æ®µï¼‰
                        inserted = batch_insert_copy(cursor, table_name, batch, use_upsert, primary_key)
                        batch_count += 1
                        
                        # æ¯Nä¸ªæ‰¹æ¬¡commitä¸€æ¬¡ï¼Œå‡å°‘commitå¼€é”€
                        if batch_count >= commit_batches:
                            conn.commit()
                            batch_count = 0
                        
                        total_inserted += inserted
                        file_stats[file_name] = file_stats.get(file_name, 0) + inserted
                    
                    except Exception as insert_error:
                        # æ’å…¥å¤±è´¥ï¼Œå›æ»šå½“å‰äº‹åŠ¡
                        conn.rollback()
                        batch_count = 0  # é‡ç½®æ‰¹æ¬¡è®¡æ•°
                        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼ˆå·²å›æ»šï¼‰: {insert_error}")
                        # è·³è¿‡è¿™ä¸ªæ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                        continue
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯3ç§’ï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 3:
                        elapsed = current_time - start_time
                        rate = total_inserted / elapsed if elapsed > 0 else 0
                        progress_pct = (completed_files / total_files * 100) if total_files > 0 else 0
                        
                        # ä¼°ç®—å‰©ä½™æ—¶é—´ï¼šå¿…é¡»å®Œæˆè‡³å°‘1ä¸ªæ–‡ä»¶æ‰èƒ½å‡†ç¡®ä¼°ç®—
                        if completed_files > 0:
                            # åŸºäºå·²å®Œæˆæ–‡ä»¶çš„å¹³å‡æ—¶é—´ä¼°ç®—å‰©ä½™æ—¶é—´
                            avg_time_per_file = elapsed / completed_files
                            remaining_files = total_files - completed_files
                            eta_seconds = remaining_files * avg_time_per_file
                            eta_hours = int(eta_seconds / 3600)
                            eta_mins = int((eta_seconds % 3600) / 60)
                            eta_str = f"{eta_hours}å°æ—¶{eta_mins}åˆ†" if eta_hours > 0 else f"{eta_mins}åˆ†"
                        else:
                            # æ²¡æœ‰å®Œæˆæ–‡ä»¶æ—¶ï¼Œæ˜¾ç¤º"è®¡ç®—ä¸­..."è€Œä¸æ˜¯"0åˆ†"
                            eta_str = "è®¡ç®—ä¸­..."
                        
                        print(f"\rğŸ“Š [{completed_files}/{total_files}] {progress_pct:.1f}% | "
                              f"{total_inserted:,}æ¡ | {rate:.0f}æ¡/ç§’ | "
                              f"å‰©ä½™: {eta_str}    ", end='', flush=True)
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    # æ ‡è®°æ–‡ä»¶å®Œæˆ
                    tracker.mark_completed(file_name)
                    completed_files += 1
                    inserted = file_stats.get(file_name, 0)
                
                elif item_type == 'error':
                    _, file_name, error = item
                    # è®°å½•å¤±è´¥æ–‡ä»¶ï¼ˆä¸æ ‡è®°ä¸ºå·²å®Œæˆï¼‰
                    failed_logger.log_failed(file_name, error)
                    completed_files += 1  # è®¡æ•°ä½†ä¸æ ‡è®°ä¸ºæˆåŠŸ
            
            except Empty:
                # é˜Ÿåˆ—ç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                logger.error(f"[Inserter] å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                conn.rollback()
                continue
        
        # æäº¤å‰©ä½™æœªcommitçš„æ•°æ®
        if batch_count > 0:
            conn.commit()
        
        # æ¢å¤è¡¨çŠ¶æ€
        if not use_upsert:
            # å¯ç”¨è§¦å‘å™¨
            cursor.execute(f"ALTER TABLE {table_name} ENABLE TRIGGER ALL")
            conn.commit()
        
        cursor.close()
        conn.close()
        
        stats_dict['inserted'] = total_inserted
        
    except Exception as e:
        logger.error(f"[Inserter] ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def batch_insert_copy(cursor, table_name: str, batch: list, use_upsert: bool = False, primary_key: str = 'corpusid') -> int:
    """
    ä½¿ç”¨COPYæ‰¹é‡æ’å…¥ï¼ˆæœ€å¿«æ–¹æ³•ï¼‰
    
    æ•°æ®ä»¥TEXTæ ¼å¼å­˜å‚¨ï¼ˆä¸éªŒè¯ä¸è§£æï¼Œæé€Ÿï¼‰
    
    æ³¨æ„ï¼š
    - authorsè¡¨ï¼šä¸»é”®åˆ—åæ˜¯authorid
    - citationsè¡¨ï¼šä¸»é”®åˆ—åæ˜¯citedcorpusid
    - å…¶ä»–è¡¨ï¼šä¸»é”®åˆ—åæ˜¯corpusid
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        table_name: è¡¨å
        batch: æ•°æ®æ‰¹æ¬¡ [(key_value, json_line), ...] - key_valueå·²ä»JSONæ­£ç¡®å­—æ®µæå–
        use_upsert: æ˜¯å¦ä½¿ç”¨UPSERTæ¨¡å¼
        primary_key: ä¸»é”®åˆ—åï¼ˆauthorid/citedcorpusid/corpusidï¼‰
    """
    if not batch:
        return 0
    
    from io import StringIO
    import psycopg2.errors
    
    try:
        if use_upsert:
            # UPSERTæ¨¡å¼ - ä¼˜åŒ–ç‰ˆæœ¬
            # 1. æ‰¹å†…å»é‡ï¼ˆå­—å…¸æ›´å¿«ï¼‰
            seen = {}
            for key_value, data in batch:
                seen[key_value] = data
            
            # 2. ä½¿ç”¨VALUESæ‰¹é‡UPSERTï¼ˆé¿å…ä¸´æ—¶è¡¨è€—å°½ç¼“å†²åŒºï¼‰
            # åˆ†å°æ‰¹æ¬¡æ’å…¥ï¼Œæ¯æ¬¡æœ€å¤š1000æ¡
            chunk_size = 1000
            total_processed = 0
            
            for i in range(0, len(seen), chunk_size):
                chunk_items = list(seen.items())[i:i + chunk_size]
                
                # æ„å»ºVALUESå­å¥
                values_list = []
                for key_val, data in chunk_items:
                    # è½¬ä¹‰å•å¼•å·
                    escaped_data = data.replace("'", "''")
                    values_list.append(f"({key_val}, '{escaped_data}', NOW(), NOW())")
                
                values_clause = ','.join(values_list)
                
                # æ‰¹é‡UPSERT
                cursor.execute(f"""
                    INSERT INTO {table_name} ({primary_key}, data, insert_time, update_time)
                    VALUES {values_clause}
                    ON CONFLICT ({primary_key}) DO UPDATE SET
                        data = EXCLUDED.data,
                        update_time = EXCLUDED.update_time
                """)
                total_processed += len(chunk_items)
            
            return total_processed
        else:
            # INSERTæ¨¡å¼ï¼šæé€ŸCOPYï¼ˆTEXTç±»å‹ï¼Œä¸éªŒè¯ä¸è§£æï¼Œæœ€å¿«ï¼‰
            
            # ç‰¹æ®Šå¤„ç†ï¼šcitationsè¡¨é¢„å»é‡ï¼ˆåŒä¸€ç¯‡è®ºæ–‡è¢«å¤šæ¬¡å¼•ç”¨å¯¼è‡´é«˜é‡å¤ç‡ï¼‰
            if table_name == 'citations':
                # 1. æ‰¹å†…å»é‡ï¼ˆå¯¹citationsè¡¨è‡³å…³é‡è¦ï¼å‡å°‘58%çš„æ— æ•ˆæ’å…¥ï¼‰
                seen = {}
                original_count = len(batch)
                for key_value, data in batch:
                    seen[key_value] = data  # ç›¸åŒkeyä¿ç•™æœ€åä¸€ä¸ª
                
                dedup_count = len(seen)
                if dedup_count == 0:
                    return 0  # æ‰¹æ¬¡ä¸ºç©ºï¼Œè·³è¿‡
                
                # 2. å»é‡åç›´æ¥COPYæ’å…¥ï¼ˆæ— éœ€ON CONFLICTæ£€æŸ¥ï¼Œæœ€å¿«ï¼‰
                buffer = StringIO()
                lines = [f"{key_val}\t{data}\t\\N\t\\N\n" for key_val, data in seen.items()]
                buffer.write(''.join(lines))
                buffer.seek(0)
                
                try:
                    cursor.copy_expert(
                        f"""
                        COPY {table_name} ({primary_key}, data, insert_time, update_time)
                        FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')
                        """,
                        buffer
                    )
                    return dedup_count
                except psycopg2.errors.UniqueViolation:
                    # è·¨æ‰¹æ¬¡é‡å¤ï¼Œä½¿ç”¨é¢„æŸ¥è¯¢è¿‡æ»¤æ³•ï¼ˆé™é»˜å¤„ç†ï¼Œé¿å…åˆ·å±ï¼‰
                    cursor.connection.rollback()
                    
                    # 1. æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„IDï¼ˆä¸€æ¬¡ç´¢å¼•æŸ¥è¯¢ï¼Œæ¯”Næ¬¡ON CONFLICTå¿«å¾—å¤šï¼‰
                    seen_ids = list(seen.keys())
                    if not seen_ids:
                        return 0  # æ— æ•°æ®å¯æŸ¥
                        
                    placeholders = ','.join(['%s'] * len(seen_ids))
                    cursor.execute(f"""
                        SELECT {primary_key} FROM {table_name} 
                        WHERE {primary_key} IN ({placeholders})
                    """, seen_ids)
                    
                    existing_ids = set(row[0] for row in cursor.fetchall())
                    
                    # 2. è¿‡æ»¤æ‰å·²å­˜åœ¨çš„ID
                    new_items = {k: v for k, v in seen.items() if k not in existing_ids}
                    
                    if not new_items:
                        return 0  # å…¨éƒ¨å·²å­˜åœ¨
                    
                    # 3. ç›´æ¥COPYæ’å…¥æ–°æ•°æ®ï¼ˆæ— éœ€ON CONFLICTï¼Œæœ€å¿«ï¼‰
                    buffer = StringIO()
                    lines = [f"{key_val}\t{data}\t\\N\t\\N\n" for key_val, data in new_items.items()]
                    buffer.write(''.join(lines))
                    buffer.seek(0)
                    
                    try:
                        cursor.copy_expert(
                            f"""
                            COPY {table_name} ({primary_key}, data, insert_time, update_time)
                            FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')
                            """,
                            buffer
                        )
                        return len(new_items)
                    except Exception as e:
                        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼ˆç†è®ºä¸Šä¸åº”è¯¥ï¼‰ï¼Œé™é»˜è·³è¿‡
                        cursor.connection.rollback()
                        return 0
            
            else:
                # å…¶ä»–è¡¨ï¼šä¸»é”®å”¯ä¸€ï¼Œç›´æ¥COPYï¼ˆæœ€å¿«ï¼‰
                buffer = StringIO()
                lines = [f"{key_val}\t{data}\t\\N\t\\N\n" for key_val, data in batch]
                buffer.write(''.join(lines))
                buffer.seek(0)
                
                try:
                    cursor.copy_expert(
                        f"""
                        COPY {table_name} ({primary_key}, data, insert_time, update_time)
                        FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')
                        """,
                        buffer
                    )
                    return len(batch)
                except psycopg2.errors.UniqueViolation:
                    # æœ‰é‡å¤keyï¼ˆé™é»˜å¤„ç†ï¼Œä½¿ç”¨é¢„æŸ¥è¯¢è¿‡æ»¤æ³•ï¼‰
                    cursor.connection.rollback()
                    
                    # 1. æå–æ‰€æœ‰ID
                    batch_ids = [key_val for key_val, _ in batch]
                    if not batch_ids:
                        return 0  # æ— æ•°æ®å¯æŸ¥
                        
                    placeholders = ','.join(['%s'] * len(batch_ids))
                    
                    # 2. æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„ID
                    cursor.execute(f"""
                        SELECT {primary_key} FROM {table_name} 
                        WHERE {primary_key} IN ({placeholders})
                    """, batch_ids)
                    
                    existing_ids = set(row[0] for row in cursor.fetchall())
                    
                    # 3. è¿‡æ»¤æ‰å·²å­˜åœ¨çš„ID
                    new_batch = [(k, v) for k, v in batch if k not in existing_ids]
                    
                    if not new_batch:
                        return 0  # å…¨éƒ¨å·²å­˜åœ¨
                    
                    # 4. ç›´æ¥COPYæ’å…¥æ–°æ•°æ®
                    buffer = StringIO()
                    lines = [f"{key_val}\t{data}\t\\N\t\\N\n" for key_val, data in new_batch]
                    buffer.write(''.join(lines))
                    buffer.seek(0)
                    
                    try:
                        cursor.copy_expert(
                            f"""
                            COPY {table_name} ({primary_key}, data, insert_time, update_time)
                            FROM STDIN WITH (FORMAT TEXT, NULL '\\N', DELIMITER E'\\t')
                            """,
                            buffer
                        )
                        return len(new_batch)
                    except Exception as e:
                        # é™é»˜è·³è¿‡
                        cursor.connection.rollback()
                        return 0
        
    except psycopg2.errors.UniqueViolation:
        # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¯´æ˜äº‹åŠ¡å·²ä¸­æ­¢ï¼Œéœ€è¦å¤–å±‚å¤„ç†
        raise
    except Exception as e:
        # å…¶ä»–é”™è¯¯
        import traceback
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}")
        raise


# =============================================================================
# ä¸»åè°ƒå™¨
# =============================================================================

def process_gz_folder_pipeline(
    folder_path: str,
    table_name: str,
    use_upsert: bool = False,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    reset_progress: bool = False,
    retry_failed: bool = False
):
    """
    æµæ°´çº¿å¹¶è¡Œå¤„ç†ï¼šå¤šä¸ªè§£å‹è¿›ç¨‹ + å•ä¸ªæ’å…¥è¿›ç¨‹
    """
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # æ ¹æ®è¡¨åè·å–ä¸“å±çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
    progress_file, failed_file = get_log_files(table_name)
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªï¼ˆæ¯ä¸ªè¡¨ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶ï¼‰
    tracker = ProgressTracker(progress_file)
    failed_logger = FailedFilesLogger(failed_file)
    
    if reset_progress:
        tracker.reset()
        failed_logger.reset()
    
    # åŠ è½½å·²å®Œæˆå’Œå¤±è´¥çš„æ–‡ä»¶
    completed_files = tracker.load_completed() if resume else set()
    failed_files = failed_logger.load_failed() if (resume and not retry_failed) else set()
    
    # æ‰«æGZæ–‡ä»¶
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ°.gzæ–‡ä»¶: {folder_path}")
        return
    
    # è¿‡æ»¤ï¼šæ’é™¤å·²å®Œæˆçš„æ–‡ä»¶ï¼Œä»¥åŠå·²çŸ¥å¤±è´¥çš„æ–‡ä»¶ï¼ˆé™¤éretry_failed=Trueï¼‰
    excluded_files = completed_files | failed_files
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in excluded_files]
    
    # è·å–è¯¥è¡¨ä½¿ç”¨çš„ä¸»é”®å­—æ®µ
    primary_key_field = TABLE_PRIMARY_KEY_MAP.get(table_name, 'corpusid')
    
    # æ ¹æ®è¡¨åè·å–ä¼˜åŒ–é…ç½®
    config = TABLE_CONFIGS.get(table_name, DEFAULT_CONFIG)
    batch_size = config['batch_size']
    commit_batches = config['commit_batches']
    # å¦‚æœç”¨æˆ·æ²¡æŒ‡å®šextractorsï¼Œä½¿ç”¨é…ç½®ä¸­çš„å€¼
    if num_extractors == NUM_EXTRACTORS:  # é»˜è®¤å€¼
        num_extractors = config['extractors']
    
    # ç²¾ç®€è¾“å‡ºï¼šä¸€è¡Œæ˜¾ç¤ºæ ¸å¿ƒä¿¡æ¯
    failed_info = f", å¤±è´¥: {len(failed_files)}" if failed_files else ""
    logger.info(f"\nâ–¶ [{table_name}] æ€»è®¡: {len(gz_files)}, å·²å®Œæˆ: {len(completed_files)}{failed_info}, å¾…å¤„ç†: {len(pending_files)}")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼\n")
        return
    
    overall_start = time.time()
    
    try:
        # åˆ›å»ºé˜Ÿåˆ—
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
        manager = Manager()
        stats_dict = manager.dict()
        
        # æ·»åŠ æ–‡ä»¶ä»»åŠ¡
        for task in pending_files:
            file_queue.put(task)
        
        # æ·»åŠ ç»“æŸä¿¡å·ï¼ˆæ¯ä¸ªè§£å‹è¿›ç¨‹ä¸€ä¸ªï¼‰
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨æ’å…¥è¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
        inserter = Process(
            target=inserter_worker,
            args=(data_queue, table_name, stats_dict, tracker, failed_logger, use_upsert, commit_batches, len(pending_files), primary_key_field),
            name='Inserter'
        )
        inserter.start()
        
        # å¯åŠ¨è§£å‹è¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, table_name, batch_size),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        # ç­‰å¾…æ‰€æœ‰è§£å‹è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join()
        
        # å‘é€åœæ­¢ä¿¡å·ç»™æ’å…¥è¿›ç¨‹
        data_queue.put(('stop', None, None))
        
        # ç­‰å¾…æ’å…¥è¿›ç¨‹å®Œæˆ
        inserter.join()
        
        elapsed = time.time() - overall_start
        total_inserted = stats_dict.get('inserted', 0)
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"âœ… [{table_name}] å®Œæˆ: {len(pending_files)}ä¸ªæ–‡ä»¶, {total_inserted:,}æ¡, {elapsed/60:.1f}åˆ†é’Ÿ, {avg_rate:.0f}æ¡/ç§’\n")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æè‡´ä¼˜åŒ–ç‰ˆ - GZæ–‡ä»¶æµå¼æ‰¹é‡æ’å…¥ï¼ˆç›®æ ‡ï¼š3000-5000æ¡/ç§’ï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼š
  ğŸš€ ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼šè§£å‹å’Œæ’å…¥å®Œå…¨å¹¶è¡Œ
  ğŸš€ æ­£åˆ™å¿«é€Ÿæå–ï¼šé¿å…å®Œæ•´JSONè§£æï¼ˆæé€Ÿ10å€ï¼‰
  ğŸš€ æ›´å¤§æ‰¹æ¬¡ï¼š50000æ¡/æ‰¹æ¬¡ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
  ğŸš€ é˜Ÿåˆ—ç¼“å†²ï¼šæŒç»­ä¾›åº”æ•°æ®ï¼Œæ— ç©ºé—²ç­‰å¾…
  ğŸš€ å¤šè¿›ç¨‹è§£å‹ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
  ğŸš€ çµæ´»ä¸»é”®é…ç½®ï¼šæ ¹æ®è¡¨åè‡ªåŠ¨ä½¿ç”¨æ­£ç¡®çš„ä¸»é”®å­—æ®µ
     - authorsè¡¨ä½¿ç”¨authorid
     - citationsè¡¨ä½¿ç”¨citedcorpusid
     - å…¶ä»–è¡¨ä½¿ç”¨corpusid

ç¤ºä¾‹ï¼š
  # å¤„ç†papersæ–‡ä»¶å¤¹ï¼ˆä½¿ç”¨corpusidä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\papers" --table papers
  
  # å¤„ç†authorsæ–‡ä»¶å¤¹ï¼ˆè‡ªåŠ¨ä½¿ç”¨authoridä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\authors" --table authors
  
  # å¤„ç†citationsæ–‡ä»¶å¤¹ï¼ˆè‡ªåŠ¨ä½¿ç”¨citedcorpusidä½œä¸ºä¸»é”®ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\machine_win01\\2025-09-30\\citations" --table citations
  
  # è‡ªå®šä¹‰è§£å‹è¿›ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°ï¼‰
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\s2orc" --table s2orc --extractors 8
  
  # ä¸­æ–­åç»§ç»­
  python scripts/stream_gz_to_db_optimized.py --dir "E:\\path\\to\\papers" --table papers --resume
        """
    )
    
    parser.add_argument('--dir', type=str, required=True,
                       help='GZæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--table', type=str, required=True,
                       choices=FIELD_TABLES,
                       help='ç›®æ ‡æ•°æ®åº“è¡¨å')
    parser.add_argument('--upsert', action='store_true',
                       help='ä½¿ç”¨UPSERTæ¨¡å¼')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--resume', action='store_true',
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true',
                       help='é‡ç½®è¿›åº¦')
    parser.add_argument('--retry-failed', action='store_true',
                       help='é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ä»¶ï¼ˆé‡æ–°ä¸‹è½½åä½¿ç”¨ï¼‰')
    
    parser.set_defaults(resume=True)
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¤„ç†
    process_gz_folder_pipeline(
        folder_path=args.dir,
        table_name=args.table,
        use_upsert=args.upsert,
        num_extractors=args.extractors,
        resume=args.resume,
        reset_progress=args.reset,
        retry_failed=args.retry_failed
    )


if __name__ == '__main__':
    main()

