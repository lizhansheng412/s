#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
 é‡çº§æ˜ å°„è¡¨æ’å…¥è„šæœ¬
åªæå– corpusid åˆ° corpus_filename_mapping è¡¨
"""

import gzip
import sys
import time
import logging
from pathlib import Path
from typing import Set
from multiprocessing import Process, Queue, Manager
from queue import Empty
from io import StringIO, BufferedReader, TextIOWrapper

import psycopg2

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from database.config import get_db_config

# é…ç½®å‚æ•°ï¼ˆå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ï¼‰
BATCH_SIZE = 500000   # 50ä¸‡æ¡/æ‰¹æ¬¡ï¼ˆcorpusidåªæœ‰8å­—èŠ‚ï¼‰
COMMIT_BATCHES = 6    # æ¯6æ‰¹æ¬¡æäº¤ï¼ˆ300ä¸‡æ¡/äº‹åŠ¡ï¼Œçº¦24MBï¼‰
NUM_EXTRACTORS = 4
QUEUE_SIZE = 30

MAPPING_TABLE = 'corpus_bigdataset'
SUPPORTED_TABLES = {'embeddings_specter_v1', 'embeddings_specter_v2', 's2orc', 's2orc_v2'}

PROGRESS_DIR = 'logs/progress_mapping'
FAILED_DIR = 'logs/failed_mapping'

logging.basicConfig(level=logging.ERROR, format='%(message)s')
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ª"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f if line.strip())
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


class FailedFilesLogger:
    """å¤±è´¥æ–‡ä»¶è®°å½•"""
    
    def __init__(self, failed_file: str):
        self.failed_file = Path(failed_file)
        self.failed_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_failed(self) -> Set[str]:
        if not self.failed_file.exists():
            return set()
        failed = set()
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('|')
                    if len(parts) >= 2:
                        failed.add(parts[1].strip())
        return failed
    
    def log_failed(self, file_name: str, error: str):
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(self.failed_file, 'a', encoding='utf-8') as f:
            f.write(f"{timestamp} | {file_name} | {error}\n")
            f.flush()


def get_log_files(dataset_type: str):
    """è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    progress_dir = Path(PROGRESS_DIR)
    failed_dir = Path(FAILED_DIR)
    progress_dir.mkdir(parents=True, exist_ok=True)
    failed_dir.mkdir(parents=True, exist_ok=True)
    
    progress_file = progress_dir / f"{dataset_type}_mapping_progress.txt"
    failed_file = failed_dir / f"{dataset_type}_mapping_failed.txt"
    
    return str(progress_file), str(failed_file)


def fast_extract_corpusid(line: str) -> int:
    """å¿«é€Ÿæå– corpusidï¼ˆæ¯”æ­£åˆ™å¿«3-5å€ï¼‰"""
    try:
        # æŸ¥æ‰¾ "corpusid" å­—æ®µï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        idx = line.lower().find('"corpusid"')
        if idx == -1:
            return None
        
        # æŸ¥æ‰¾å†’å·åçš„æ•°å­—
        idx = line.find(':', idx)
        if idx == -1:
            return None
        
        # è·³è¿‡ç©ºç™½å’Œå¼•å·
        idx += 1
        while idx < len(line) and line[idx] in ' \t':
            idx += 1
        
        # æå–æ•°å­—
        start = idx
        while idx < len(line) and line[idx].isdigit():
            idx += 1
        
        if idx > start:
            return int(line[start:idx])
        return None
    except (ValueError, IndexError):
        return None


def extractor_worker(file_queue: Queue, data_queue: Queue, stats_dict: dict, batch_size: int = BATCH_SIZE):
    """ç”Ÿäº§è€…ï¼šè§£å‹å¹¶æå– corpusidï¼ˆå·²ä¼˜åŒ–ï¼‰"""
    import logging
    logging.getLogger().setLevel(logging.CRITICAL)
    
    while True:
        try:
            task = file_queue.get(timeout=1)
            if task is None:
                break
            
            gz_file_path, file_name = task
            
            try:
                # ä½¿ç”¨ set åœ¨ extractor ä¾§å»é‡ï¼Œå‡å°‘ä¼ è¾“é‡
                batch_set = set()
                valid_count = 0
                
                try:
                    with gzip.open(gz_file_path, 'rb') as f_binary:
                        # ä¼˜åŒ–ï¼š32MB ç¼“å†²åŒºï¼Œæå‡è¯»å–é€Ÿåº¦
                        f = TextIOWrapper(BufferedReader(f_binary, buffer_size=32*1024*1024), 
                                        encoding='utf-8', errors='ignore')
                        
                        for line in f:
                            line = line.strip()
                            if not line or len(line) < 15:  # "corpusid":1 æœ€çŸ­15å­—ç¬¦
                                continue
                            
                            # å¿«é€Ÿæå–ï¼ˆæ¯”æ­£åˆ™å¿«3-5å€ï¼‰
                            corpusid = fast_extract_corpusid(line)
                            if corpusid is None:
                                continue
                            
                            valid_count += 1
                            batch_set.add(corpusid)
                            
                            # æ‰¹æ¬¡è¾¾åˆ°ä¸Šé™ï¼Œè½¬ä¸ºlistå‘é€ï¼ˆsetå†…å­˜æ›´ç´§å‡‘ï¼‰
                            if len(batch_set) >= batch_size:
                                data_queue.put(('data', file_name, list(batch_set)))
                                batch_set.clear()
                    
                    # å‘é€å‰©ä½™æ•°æ®
                    if batch_set:
                        data_queue.put(('data', file_name, list(batch_set)))
                    
                    data_queue.put(('done', file_name, valid_count))
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError, gzip.BadGzipFile):
                    data_queue.put(('error', file_name, "Corrupted file"))
                    continue
                except MemoryError:
                    import gc
                    gc.collect()
                    data_queue.put(('error', file_name, "Memory error"))
                    continue
                
            except Exception as e:
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


def inserter_worker(data_queue: Queue, dataset_type: str, stats_dict: dict, 
                   tracker: ProgressTracker, failed_logger: FailedFilesLogger, 
                   commit_batches: int = COMMIT_BATCHES, total_files: int = 0):
    """æ¶ˆè´¹è€…ï¼šæ‰¹é‡æ’å…¥ corpusidï¼ˆå·²ä¼˜åŒ–ï¼‰"""
    conn = None
    cursor = None
    buffer_pool = StringIO()  # å¤ç”¨ StringIO å¯¹è±¡ï¼Œå‡å°‘ GC å‹åŠ›
    
    try:
        # å§‹ç»ˆè¿æ¥æœ¬æœºæ•°æ®åº“ï¼ˆmachine1 çš„ 5431 ç«¯å£ï¼‰
        db_config = get_db_config('machine1')
        conn = psycopg2.connect(**db_config)
        conn.autocommit = False
        cursor = conn.cursor()
        
        # æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ï¼ˆä¼šè¯çº§åˆ«ï¼Œä¿å®ˆé…ç½®ï¼‰
        try:
            cursor.execute("SET synchronous_commit = OFF")
            cursor.execute("SET work_mem = '1GB'")
            cursor.execute("SET maintenance_work_mem = '2GB'")
        except Exception as e:
            pass  # å¿½ç•¥é…ç½®å¤±è´¥
        
        total_inserted = 0
        completed_files = 0
        last_log_time = time.time()
        start_time = time.time()
        batch_count = 0
        pending_done = []  # æ‰¹é‡å†™å…¥è¿›åº¦æ–‡ä»¶
        
        while True:
            try:
                item = data_queue.get(timeout=5)
                item_type = item[0]
                
                if item_type == 'stop':
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    # é‡è¯•æœºåˆ¶ï¼ˆæœ€å¤š3æ¬¡ï¼‰
                    max_retries = 3
                    for attempt in range(max_retries):
                        try:
                            # ä½¿ç”¨å¤ç”¨çš„ StringIO
                            inserted = batch_insert_corpusids(cursor, batch, buffer_pool)
                            batch_count += 1
                            
                            # è¶…å¤§äº‹åŠ¡æäº¤ç­–ç•¥ï¼ˆUSBç¡¬ç›˜ä¼˜åŒ–ï¼‰
                            if batch_count >= commit_batches:
                                conn.commit()
                                batch_count = 0
                                
                                # æ‰¹é‡å†™å…¥è¿›åº¦ï¼ˆå‡å°‘æ–‡ä»¶ I/Oï¼‰
                                if pending_done:
                                    for fname in pending_done:
                                        tracker.mark_completed(fname)
                                    pending_done.clear()
                            
                            total_inserted += inserted
                            break  # æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        
                        except psycopg2.DatabaseError as e:
                            logger.warning(f"æ’å…¥é”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}")
                            conn.rollback()
                            batch_count = 0
                            
                            if attempt < max_retries - 1:
                                # é‡è¿æ•°æ®åº“
                                try:
                                    if cursor:
                                        cursor.close()
                                    if conn:
                                        conn.close()
                                    conn = psycopg2.connect(**db_config)
                                    conn.autocommit = False
                                    cursor = conn.cursor()
                                    cursor.execute("SET synchronous_commit = OFF")
                                    time.sleep(0.5)  # çŸ­æš‚ç­‰å¾…
                                except Exception:
                                    pass
                            else:
                                # æœ€åä¸€æ¬¡ä¹Ÿå¤±è´¥ï¼Œè®°å½•å¹¶è·³è¿‡
                                logger.error(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰ï¼Œè·³è¿‡")
                                break
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯2ç§’ï¼Œæ›´é¢‘ç¹çš„åé¦ˆï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 2:
                        elapsed = current_time - start_time
                        rate = total_inserted / elapsed if elapsed > 0 else 0
                        progress_pct = (completed_files / total_files * 100) if total_files > 0 else 0
                        
                        print(f"\rğŸ“Š [{completed_files}/{total_files}] {progress_pct:.1f}% | "
                              f"{total_inserted:,}æ¡ | {rate:,.0f}æ¡/ç§’    ", end='', flush=True)
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    pending_done.append(file_name)
                    completed_files += 1
                
                elif item_type == 'error':
                    _, file_name, error = item
                    failed_logger.log_failed(file_name, error)
                    completed_files += 1
            
            except Empty:
                continue
            except Exception as e:
                logger.error(f"å¤„ç†é˜Ÿåˆ—é¡¹é”™è¯¯: {e}")
                if conn:
                    conn.rollback()
                continue
        
        # æœ€ç»ˆæäº¤
        if batch_count > 0:
            conn.commit()
        
        # å†™å…¥å‰©ä½™è¿›åº¦
        if pending_done:
            for fname in pending_done:
                tracker.mark_completed(fname)
        
        stats_dict['inserted'] = total_inserted
        
    except Exception as e:
        logger.error(f"æ’å…¥è¿›ç¨‹é”™è¯¯: {e}")
    finally:
        # å®‰å…¨æ¸…ç†
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def batch_insert_corpusids(cursor, batch: list, buffer: StringIO = None) -> int:
    """æ‰¹é‡æ’å…¥ corpusidï¼ˆCOPY æ–¹å¼ï¼Œå¤ç”¨ StringIOï¼‰"""
    if not batch:
        return 0
    
    try:
        # å¤ç”¨ StringIO å¯¹è±¡ï¼Œé¿å…é¢‘ç¹åˆ›å»ºé”€æ¯
        if buffer is None:
            buffer = StringIO()
        else:
            buffer.seek(0)
            buffer.truncate(0)
        
        # ä¼˜åŒ–ï¼šç›´æ¥å†™å…¥ï¼Œé¿å… join çš„é¢å¤–å†…å­˜åˆ†é…
        for cid in batch:
            buffer.write(str(cid))
            buffer.write('\n')
        
        buffer.seek(0)
        
        # COPY æ˜¯ PostgreSQL æœ€å¿«çš„æ‰¹é‡æ’å…¥æ–¹å¼
        cursor.copy_expert(
            f"COPY {MAPPING_TABLE} (corpusid) FROM STDIN",
            buffer
        )
        return len(batch)
    
    except Exception as e:
        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
        raise


def process_gz_folder_to_mapping(folder_path: str, dataset_type: str, 
                                 num_extractors: int = NUM_EXTRACTORS,
                                 resume: bool = True, reset_progress: bool = False):
    """å¤„ç† GZ æ–‡ä»¶å¤¹ï¼Œæå– corpusid"""
    if dataset_type not in SUPPORTED_TABLES:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_type}")
    
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    progress_file, failed_file = get_log_files(dataset_type)
    tracker = ProgressTracker(progress_file)
    failed_logger = FailedFilesLogger(failed_file)
    
    if reset_progress:
        tracker.reset()
        failed_logger.reset()
    
    completed_files = tracker.load_completed() if resume else set()
    failed_files = failed_logger.load_failed() if resume else set()
    
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ° .gz æ–‡ä»¶: {folder_path}")
        return
    
    excluded_files = completed_files | failed_files
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in excluded_files]
    
    logger.info(f"\nâ–¶ [{dataset_type}] æ€»è®¡: {len(gz_files)}, å·²å®Œæˆ: {len(completed_files)}, å¾…å¤„ç†: {len(pending_files)}")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ\n")
        return
    
    overall_start = time.time()
    
    try:
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        manager = Manager()
        stats_dict = manager.dict()
        
        for task in pending_files:
            file_queue.put(task)
        
        for _ in range(num_extractors):
            file_queue.put(None)
        
        inserter = Process(
            target=inserter_worker,
            args=(data_queue, dataset_type, stats_dict, tracker, failed_logger, COMMIT_BATCHES, len(pending_files)),
            name='Inserter'
        )
        inserter.start()
        
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, BATCH_SIZE),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        for p in extractors:
            p.join()
        
        data_queue.put(('stop', None, None))
        inserter.join()
        
        elapsed = time.time() - overall_start
        total_inserted = stats_dict.get('inserted', 0)
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"\nâœ… [{dataset_type}] å®Œæˆ: {len(pending_files)}æ–‡ä»¶, {total_inserted:,}æ¡, {elapsed/60:.1f}åˆ†é’Ÿ, {avg_rate:.0f}æ¡/ç§’\n")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='æå– corpusid åˆ°å¤§æ•°æ®é›†è¡¨ï¼ˆå†™å…¥æœ¬æœº Machine1 æ•°æ®åº“ï¼‰')
    parser.add_argument('--dir', type=str, required=True, help='GZ æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True, choices=list(SUPPORTED_TABLES), help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS, help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--resume', action='store_true', help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true', help='é‡ç½®è¿›åº¦')
    parser.set_defaults(resume=True)
    
    args = parser.parse_args()
    
    process_gz_folder_to_mapping(
        folder_path=args.dir,
        dataset_type=args.dataset,
        num_extractors=args.extractors,
        resume=args.resume,
        reset_progress=args.reset
    )


if __name__ == '__main__':
    main()
