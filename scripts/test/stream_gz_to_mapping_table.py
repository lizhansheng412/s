#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½»é‡çº§æ˜ å°„è¡¨æ’å…¥è„šæœ¬ï¼ˆæµ‹è¯•ç”¨ï¼‰
åªæå– corpusid å’Œ filenameï¼Œæ’å…¥åˆ° corpus_filename_mapping è¡¨

ä¸“é—¨é’ˆå¯¹å¤§æ•°æ®é›†ï¼šembeddings_specter_v1, embeddings_specter_v2, s2orc, s2orc_v2
ä¸æ’å…¥çœŸæ­£çš„æ•°æ®ï¼Œåªå»ºç«‹ corpusid â†’ filename çš„ç´¢å¼•æ˜ å°„

æ€§èƒ½ä¼˜åŒ–ï¼ˆæè‡´ç‰ˆï¼‰ï¼š
  âœ… ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼š4ä¸ªè§£å‹è¿›ç¨‹ + 1ä¸ªæ’å…¥è¿›ç¨‹å¹¶è¡Œ
  âœ… æ­£åˆ™å¿«é€Ÿæå–corpusidï¼šé¢„ç¼–è¯‘æ­£åˆ™ï¼Œé¿å…å®Œæ•´JSONè§£æ
  âœ… è¶…å¤§æ‰¹æ¬¡ï¼š200000æ¡/æ‰¹æ¬¡ï¼Œ200ä¸‡æ¡/äº‹åŠ¡
  âœ… å¤§é˜Ÿåˆ—ç¼“å†²ï¼ˆ40ä¸ªæ‰¹æ¬¡ï¼‰ï¼šæŒç»­ä¾›åº”æ•°æ®ï¼Œæ— ç©ºé—²ç­‰å¾…
  âœ… 16MBè§£å‹ç¼“å†²åŒºï¼šå‡å°‘ç£ç›˜I/Oæ¬¡æ•°
  âœ… æ‰¹é‡å­—ç¬¦ä¸²æ„å»ºï¼šå‡å°‘StringIOå†™å…¥æ¬¡æ•°
  âœ… æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­æ¢å¤ï¼Œè‡ªåŠ¨è¿‡æ»¤å·²æ’å…¥æ•°æ®
  
ç›®æ ‡ï¼š30000-60000æ¡/ç§’ï¼ˆæ— é¢å¤–ç´¢å¼•ï¼Œæœ€å¤§åŒ–é€Ÿåº¦ï¼Œæ¯”å®Œæ•´æ•°æ®å¿«30å€+ï¼‰
"""

import gzip
import re
import sys
import time
import logging
from pathlib import Path
from typing import Set
from multiprocessing import Process, Queue, Manager
from queue import Empty

import psycopg2

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from database.config.db_config_v2 import DB_CONFIG

# =============================================================================
# é…ç½®
# =============================================================================

# è½»é‡çº§é…ç½®ï¼šåªæœ‰ä¸¤ä¸ªå­—æ®µï¼Œæ’å…¥æå¿«ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
BATCH_SIZE = 200000  # 20ä¸‡æ¡/æ‰¹æ¬¡ï¼ˆcorpusid + filename å¾ˆå°ï¼Œæé€Ÿæ’å…¥ï¼‰
COMMIT_BATCHES = 10  # æ¯10ä¸ªæ‰¹æ¬¡commitä¸€æ¬¡ï¼ˆ200ä¸‡æ¡/äº‹åŠ¡ï¼Œçº¦60MBï¼‰
NUM_EXTRACTORS = 4  # 4ä¸ªè§£å‹è¿›ç¨‹ï¼ˆå……åˆ†åˆ©ç”¨å¤šæ ¸CPUï¼‰
QUEUE_SIZE = 40  # å¢å¤§é˜Ÿåˆ—ç¼“å†²ï¼ˆä¿è¯æ•°æ®æŒç»­ä¾›åº”ï¼‰

# æ˜ å°„è¡¨å
MAPPING_TABLE = 'corpus_filename_mapping'

# æ”¯æŒçš„æ•°æ®é›†ç±»å‹ï¼ˆåªé’ˆå¯¹å¤§æ•°æ®é›†ï¼‰
SUPPORTED_TABLES = {'embeddings_specter_v1', 'embeddings_specter_v2', 's2orc', 's2orc_v2'}

# æ—¥å¿—æ–‡ä»¶è·¯å¾„
PROGRESS_DIR = 'logs/progress_mapping'
FAILED_DIR = 'logs/failed_mapping'

# æ­£åˆ™è¡¨è¾¾å¼ï¼šå¿«é€Ÿæå–corpusid
CORPUSID_PATTERN = re.compile(r'"corpusid"\s*:\s*(\d+)', re.IGNORECASE)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.ERROR,
    format='%(message)s'
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


# =============================================================================
# æ–­ç‚¹ç»­ä¼ 
# =============================================================================

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, progress_file: str):
        self.progress_file = Path(progress_file)
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_completed(self) -> Set[str]:
        if not self.progress_file.exists():
            return set()
        
        completed = set()
        with open(self.progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    completed.add(line)
        
        return completed
    
    def mark_completed(self, file_name: str):
        with open(self.progress_file, 'a', encoding='utf-8') as f:
            f.write(f"{file_name}\n")
            f.flush()
    
    def reset(self):
        if self.progress_file.exists():
            self.progress_file.unlink()


class FailedFilesLogger:
    """å¤±è´¥æ–‡ä»¶è®°å½•å™¨"""
    
    def __init__(self, failed_file: str):
        self.failed_file = Path(failed_file)
        self.failed_file.parent.mkdir(parents=True, exist_ok=True)
    
    def load_failed(self) -> Set[str]:
        if not self.failed_file.exists():
            return set()
        
        failed = set()
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('|')
                    if len(parts) >= 2:
                        failed.add(parts[1].strip())
        
        return failed
    
    def log_failed(self, file_name: str, error: str):
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(self.failed_file, 'a', encoding='utf-8') as f:
            f.write(f"{timestamp} | {file_name} | {error}\n")
            f.flush()
    
    def reset(self):
        if self.failed_file.exists():
            self.failed_file.unlink()


def get_log_files(dataset_type: str):
    """
    æ ¹æ®æ•°æ®é›†ç±»å‹è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„
    
    Args:
        dataset_type: æ•°æ®é›†ç±»å‹ (embeddings_specter_v1/v2, s2orc/s2orc_v2)
    
    Returns:
        (progress_file, failed_file) å…ƒç»„
    """
    progress_dir = Path(PROGRESS_DIR)
    failed_dir = Path(FAILED_DIR)
    
    progress_dir.mkdir(parents=True, exist_ok=True)
    failed_dir.mkdir(parents=True, exist_ok=True)
    
    progress_file = progress_dir / f"{dataset_type}_mapping_progress.txt"
    failed_file = failed_dir / f"{dataset_type}_mapping_failed.txt"
    
    return str(progress_file), str(failed_file)


# =============================================================================
# ç”Ÿäº§è€…ï¼šè§£å‹è¿›ç¨‹
# =============================================================================

def extractor_worker(
    file_queue: Queue,
    data_queue: Queue,
    stats_dict: dict,
    batch_size: int = BATCH_SIZE
):
    """
    è§£å‹å·¥ä½œè¿›ç¨‹ï¼ˆç”Ÿäº§è€…ï¼‰
    åªæå– corpusid å’Œ filename
    """
    # ç¦ç”¨æ­¤è¿›ç¨‹çš„æ—¥å¿—è¾“å‡º
    import logging
    logging.getLogger().setLevel(logging.CRITICAL)
    
    while True:
        try:
            task = file_queue.get(timeout=1)
            if task is None:  # ç»“æŸä¿¡å·
                break
            
            gz_file_path, file_name = task
            
            try:
                batch = []
                valid_count = 0
                
                # æµå¼è§£å‹ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼šæ›´å¤§ç¼“å†²åŒºï¼‰
                try:
                    import io
                    with gzip.open(gz_file_path, 'rb') as f_binary:
                        # 16MBç¼“å†²åŒºï¼ˆ2å€æå‡ï¼Œå‡å°‘I/Oæ¬¡æ•°ï¼‰
                        f = io.TextIOWrapper(io.BufferedReader(f_binary, buffer_size=16*1024*1024), 
                                            encoding='utf-8', errors='ignore')
                        for line in f:
                            line = line.strip()
                            if not line:
                                continue
                            
                            # æ­£åˆ™å¿«é€Ÿæå–corpusidï¼ˆé¢„ç¼–è¯‘çš„æ­£åˆ™ï¼Œæé€Ÿï¼‰
                            match = CORPUSID_PATTERN.search(line)
                            if not match:
                                continue
                            
                            corpusid = int(match.group(1))
                            valid_count += 1
                            
                            # åªå­˜å‚¨ (corpusid, filename)
                            batch.append((corpusid, file_name))
                            
                            # æ‰¹æ¬¡æ»¡äº†ï¼Œå‘é€åˆ°é˜Ÿåˆ—
                            if len(batch) >= batch_size:
                                data_queue.put(('data', file_name, batch))
                                batch = []
                    
                    # å‘é€å‰©ä½™æ•°æ®
                    if batch:
                        data_queue.put(('data', file_name, batch))
                    
                    # å‘é€æ–‡ä»¶å®Œæˆä¿¡å·
                    data_queue.put(('done', file_name, valid_count))
                    
                    # æ›´æ–°ç»Ÿè®¡
                    stats_dict['extracted'] = stats_dict.get('extracted', 0) + valid_count
                    
                except (OSError, EOFError, ValueError, gzip.BadGzipFile) as gz_error:
                    # GZIPæ–‡ä»¶æŸåæˆ–è¯»å–é”™è¯¯ï¼Œç›´æ¥è·³è¿‡
                    data_queue.put(('error', file_name, f"Corrupted or unreadable"))
                    continue
                except MemoryError:
                    # å†…å­˜ä¸è¶³ï¼Œå°è¯•æ¸…ç†å¹¶è·³è¿‡
                    import gc
                    gc.collect()
                    data_queue.put(('error', file_name, f"Memory error"))
                    continue
                
            except Exception as e:
                data_queue.put(('error', file_name, str(e)))
        
        except Empty:
            continue
        except Exception:
            break


# =============================================================================
# æ¶ˆè´¹è€…ï¼šæ’å…¥è¿›ç¨‹
# =============================================================================

def inserter_worker(
    data_queue: Queue,
    dataset_type: str,
    stats_dict: dict,
    tracker: ProgressTracker,
    failed_logger: FailedFilesLogger,
    commit_batches: int = COMMIT_BATCHES,
    total_files: int = 0
):
    """
    æ’å…¥å·¥ä½œè¿›ç¨‹ï¼ˆæ¶ˆè´¹è€…ï¼‰
    åªæ’å…¥ (corpusid, filename) åˆ°æ˜ å°„è¡¨
    """
    try:
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        conn = psycopg2.connect(**DB_CONFIG)
        conn.autocommit = False
        cursor = conn.cursor()
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®ï¼ˆè½»é‡çº§ç´¢å¼•ä¸“ç”¨ - æè‡´ä¼˜åŒ–ï¼‰
        try:
            cursor.execute("SET synchronous_commit = OFF")  # å¼‚æ­¥æäº¤ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
            cursor.execute("SET commit_delay = 100000")  # å»¶è¿Ÿæäº¤100ms
            cursor.execute("SET work_mem = '1GB'")  # æ›´å¤§çš„å·¥ä½œå†…å­˜ï¼ˆåŠ é€Ÿæ’åºå’ŒæŸ¥è¯¢ï¼‰
            cursor.execute("SET maintenance_work_mem = '2GB'")  # ç»´æŠ¤å†…å­˜
            cursor.execute("SET effective_cache_size = '16GB'")  # ç¼“å­˜å¤§å°
            cursor.execute("SET temp_buffers = '512MB'")  # ä¸´æ—¶ç¼“å†²åŒº
            cursor.execute("SET max_parallel_workers_per_gather = 0")  # å…³é—­å¹¶è¡Œï¼ˆå•è¿›ç¨‹æ›´å¿«ï¼‰
            # ä¸´æ—¶è¡¨ç©ºé—´ä½¿ç”¨Dç›˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                cursor.execute("SET temp_tablespaces = 'd1_temp'")
            except:
                pass
        except Exception as e:
            conn.rollback()
            logger.warning(f"éƒ¨åˆ†æ€§èƒ½é…ç½®å¤±è´¥ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
        
        total_inserted = 0
        file_stats = {}
        completed_files = 0
        last_log_time = time.time()
        start_time = time.time()
        batch_count = 0
        
        while True:
            try:
                item = data_queue.get(timeout=5)
                item_type = item[0]
                
                if item_type == 'stop':
                    break
                
                elif item_type == 'data':
                    _, file_name, batch = item
                    
                    try:
                        # æ‰¹é‡æ’å…¥æ˜ å°„è¡¨
                        inserted = batch_insert_mapping(cursor, batch)
                        batch_count += 1
                        
                        # æ¯Nä¸ªæ‰¹æ¬¡commitä¸€æ¬¡
                        if batch_count >= commit_batches:
                            conn.commit()
                            batch_count = 0
                        
                        total_inserted += inserted
                        file_stats[file_name] = file_stats.get(file_name, 0) + inserted
                    
                    except psycopg2.DatabaseError as db_error:
                        # æ•°æ®åº“é”™è¯¯ï¼Œå›æ»šå¹¶é‡è¯•
                        conn.rollback()
                        batch_count = 0
                        logger.error(f"æ•°æ®åº“é”™è¯¯ï¼ˆå·²å›æ»šï¼‰: {db_error}")
                        # å°è¯•é‡è¿ï¼ˆå¯èƒ½æ˜¯è¿æ¥è¶…æ—¶ï¼‰
                        try:
                            conn.close()
                            conn = psycopg2.connect(**DB_CONFIG)
                            conn.autocommit = False
                            cursor = conn.cursor()
                        except:
                            pass
                        continue
                    except Exception as insert_error:
                        conn.rollback()
                        batch_count = 0
                        logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼ˆå·²å›æ»šï¼‰: {insert_error}")
                        continue
                    
                    # å®šæœŸè¾“å‡ºè¿›åº¦ï¼ˆæ¯3ç§’ï¼‰
                    current_time = time.time()
                    if current_time - last_log_time >= 3:
                        elapsed = current_time - start_time
                        rate = total_inserted / elapsed if elapsed > 0 else 0
                        progress_pct = (completed_files / total_files * 100) if total_files > 0 else 0
                        
                        if completed_files > 0:
                            avg_time_per_file = elapsed / completed_files
                            remaining_files = total_files - completed_files
                            eta_seconds = remaining_files * avg_time_per_file
                            eta_hours = int(eta_seconds / 3600)
                            eta_mins = int((eta_seconds % 3600) / 60)
                            eta_str = f"{eta_hours}å°æ—¶{eta_mins}åˆ†" if eta_hours > 0 else f"{eta_mins}åˆ†"
                        else:
                            eta_str = "è®¡ç®—ä¸­..."
                        
                        print(f"\rğŸ“Š [{completed_files}/{total_files}] {progress_pct:.1f}% | "
                              f"{total_inserted:,}æ¡ | {rate:.0f}æ¡/ç§’ | "
                              f"å‰©ä½™: {eta_str}    ", end='', flush=True)
                        last_log_time = current_time
                
                elif item_type == 'done':
                    _, file_name, _ = item
                    tracker.mark_completed(file_name)
                    completed_files += 1
                
                elif item_type == 'error':
                    _, file_name, error = item
                    failed_logger.log_failed(file_name, error)
                    completed_files += 1
            
            except Empty:
                continue
            except Exception as e:
                logger.error(f"[Inserter] å¤„ç†æ•°æ®å¼‚å¸¸: {e}")
                conn.rollback()
                continue
        
        # æäº¤å‰©ä½™æ•°æ®
        if batch_count > 0:
            conn.commit()
        
        cursor.close()
        conn.close()
        
        stats_dict['inserted'] = total_inserted
        
    except Exception as e:
        logger.error(f"[Inserter] ä¸¥é‡é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def batch_insert_mapping(cursor, batch: list) -> int:
    """
    æ‰¹é‡æ’å…¥æ˜ å°„è¡¨ï¼ˆåªæœ‰ä¸¤ä¸ªå­—æ®µï¼šcorpusid, filenameï¼‰
    
    Args:
        cursor: æ•°æ®åº“æ¸¸æ ‡
        batch: æ•°æ®æ‰¹æ¬¡ [(corpusid, filename), ...]
    
    Returns:
        æ’å…¥çš„è®°å½•æ•°
    """
    if not batch:
        return 0
    
    from io import StringIO
    import psycopg2.errors
    
    try:
        # ä½¿ç”¨COPYæ‰¹é‡æ’å…¥ï¼ˆæœ€å¿«æ–¹æ³• - ä¼˜åŒ–ç‰ˆï¼šé¢„åˆ†é…ç¼“å†²åŒºï¼‰
        buffer = StringIO()
        # ä¼˜åŒ–ï¼šä½¿ç”¨åˆ—è¡¨æ‰¹é‡æ„å»ºï¼Œç„¶åä¸€æ¬¡æ€§å†™å…¥ï¼ˆå‡å°‘å†™å…¥æ¬¡æ•°ï¼‰
        lines = []
        for corpusid, filename in batch:
            # corpusidæ˜¯æ•°å­—ï¼Œfilenameéœ€è¦è½¬ä¹‰ï¼ˆgzæ–‡ä»¶åé€šå¸¸ä¸éœ€è¦å¤æ‚è½¬ä¹‰ï¼‰
            escaped_filename = filename.replace('\\', '\\\\').replace('\t', '\\t').replace('\n', '\\n')
            lines.append(f"{corpusid}\t{escaped_filename}\n")
        buffer.write(''.join(lines))
        buffer.seek(0)
        
        try:
            cursor.copy_expert(
                f"""
                COPY {MAPPING_TABLE} (corpusid, filename)
                FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t')
                """,
                buffer
            )
            return len(batch)
        except psycopg2.errors.UniqueViolation:
            # æœ‰é‡å¤corpusidï¼ˆæ–­ç‚¹ç»­ä¼ åœºæ™¯ï¼‰
            cursor.connection.rollback()
            
            # æå–æ‰€æœ‰corpusid
            batch_ids = [corpusid for corpusid, _ in batch]
            if not batch_ids:
                return 0
            
            placeholders = ','.join(['%s'] * len(batch_ids))
            
            # æŸ¥è¯¢å·²å­˜åœ¨çš„corpusid
            cursor.execute(f"""
                SELECT corpusid FROM {MAPPING_TABLE} 
                WHERE corpusid IN ({placeholders})
            """, batch_ids)
            
            existing_ids = set(row[0] for row in cursor.fetchall())
            
            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„è®°å½•
            new_batch = [(cid, fn) for cid, fn in batch if cid not in existing_ids]
            
            if not new_batch:
                return 0  # å…¨éƒ¨å·²å­˜åœ¨
            
            # COPYæ’å…¥æ–°æ•°æ®ï¼ˆä¼˜åŒ–ï¼šæ‰¹é‡æ„å»ºï¼‰
            buffer = StringIO()
            lines = []
            for corpusid, filename in new_batch:
                escaped_filename = filename.replace('\\', '\\\\').replace('\t', '\\t').replace('\n', '\\n')
                lines.append(f"{corpusid}\t{escaped_filename}\n")
            buffer.write(''.join(lines))
            buffer.seek(0)
            
            try:
                cursor.copy_expert(
                    f"""
                    COPY {MAPPING_TABLE} (corpusid, filename)
                    FROM STDIN WITH (FORMAT TEXT, DELIMITER E'\\t')
                    """,
                    buffer
                )
                return len(new_batch)
            except Exception:
                cursor.connection.rollback()
                return 0
    
    except Exception as e:
        logger.error(f"æ‰¹é‡æ’å…¥æ˜ å°„è¡¨å¤±è´¥: {e}")
        raise


# =============================================================================
# ä¸»åè°ƒå™¨
# =============================================================================

def process_gz_folder_to_mapping(
    folder_path: str,
    dataset_type: str,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True,
    reset_progress: bool = False,
    retry_failed: bool = False
):
    """
    å¤„ç†GZæ–‡ä»¶å¤¹ï¼Œæå–corpusidå’Œfilenameåˆ°æ˜ å°„è¡¨
    
    Args:
        folder_path: GZæ–‡ä»¶å¤¹è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹ (embeddings_specter_v1/v2, s2orc/s2orc_v2)
        num_extractors: è§£å‹è¿›ç¨‹æ•°
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
        reset_progress: æ˜¯å¦é‡ç½®è¿›åº¦
        retry_failed: æ˜¯å¦é‡è¯•å¤±è´¥çš„æ–‡ä»¶
    """
    # éªŒè¯æ•°æ®é›†ç±»å‹
    if dataset_type not in SUPPORTED_TABLES:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}. æ”¯æŒçš„ç±»å‹: {SUPPORTED_TABLES}")
    
    folder = Path(folder_path)
    if not folder.exists():
        raise ValueError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
    
    # è·å–æ—¥å¿—æ–‡ä»¶è·¯å¾„
    progress_file, failed_file = get_log_files(dataset_type)
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
    tracker = ProgressTracker(progress_file)
    failed_logger = FailedFilesLogger(failed_file)
    
    if reset_progress:
        tracker.reset()
        failed_logger.reset()
    
    # åŠ è½½å·²å®Œæˆå’Œå¤±è´¥çš„æ–‡ä»¶
    completed_files = tracker.load_completed() if resume else set()
    failed_files = failed_logger.load_failed() if (resume and not retry_failed) else set()
    
    # æ‰«æGZæ–‡ä»¶
    gz_files = sorted(folder.glob("*.gz"))
    if not gz_files:
        logger.warning(f"æœªæ‰¾åˆ°.gzæ–‡ä»¶: {folder_path}")
        return
    
    # è¿‡æ»¤ï¼šæ’é™¤å·²å®Œæˆå’Œå¤±è´¥çš„æ–‡ä»¶
    excluded_files = completed_files | failed_files
    pending_files = [(str(f), f.name) for f in gz_files if f.name not in excluded_files]
    
    # è¾“å‡ºä¿¡æ¯
    failed_info = f", å¤±è´¥: {len(failed_files)}" if failed_files else ""
    logger.info(f"\nâ–¶ [{dataset_type}] æ˜ å°„è¡¨æ¨¡å¼ - æ€»è®¡: {len(gz_files)}, å·²å®Œæˆ: {len(completed_files)}{failed_info}, å¾…å¤„ç†: {len(pending_files)}")
    
    if not pending_files:
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆï¼\n")
        return
    
    overall_start = time.time()
    
    try:
        # åˆ›å»ºé˜Ÿåˆ—
        file_queue = Queue()
        data_queue = Queue(maxsize=QUEUE_SIZE)
        
        # åˆ›å»ºå…±äº«ç»Ÿè®¡å­—å…¸
        manager = Manager()
        stats_dict = manager.dict()
        
        # æ·»åŠ æ–‡ä»¶ä»»åŠ¡
        for task in pending_files:
            file_queue.put(task)
        
        # æ·»åŠ ç»“æŸä¿¡å·
        for _ in range(num_extractors):
            file_queue.put(None)
        
        # å¯åŠ¨æ’å…¥è¿›ç¨‹
        inserter = Process(
            target=inserter_worker,
            args=(data_queue, dataset_type, stats_dict, tracker, failed_logger, COMMIT_BATCHES, len(pending_files)),
            name='Inserter'
        )
        inserter.start()
        
        # å¯åŠ¨è§£å‹è¿›ç¨‹
        extractors = []
        for i in range(num_extractors):
            p = Process(
                target=extractor_worker,
                args=(file_queue, data_queue, stats_dict, BATCH_SIZE),
                name=f'Extractor-{i+1}'
            )
            p.start()
            extractors.append(p)
        
        # ç­‰å¾…æ‰€æœ‰è§£å‹è¿›ç¨‹å®Œæˆ
        for p in extractors:
            p.join()
        
        # å‘é€åœæ­¢ä¿¡å·
        data_queue.put(('stop', None, None))
        
        # ç­‰å¾…æ’å…¥è¿›ç¨‹å®Œæˆ
        inserter.join()
        
        elapsed = time.time() - overall_start
        total_inserted = stats_dict.get('inserted', 0)
        avg_rate = total_inserted / elapsed if elapsed > 0 else 0
        
        logger.info(f"âœ… [{dataset_type}] å®Œæˆ: {len(pending_files)}ä¸ªæ–‡ä»¶, {total_inserted:,}æ¡æ˜ å°„, {elapsed/60:.1f}åˆ†é’Ÿ, {avg_rate:.0f}æ¡/ç§’\n")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='è½»é‡çº§æ˜ å°„è¡¨æ’å…¥è„šæœ¬ï¼ˆæµ‹è¯•ç”¨ï¼‰- åªæå– corpusid å’Œ filename',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
è¯´æ˜ï¼š
  æœ¬è„šæœ¬ä¸“é—¨ç”¨äºå¤§æ•°æ®é›†ï¼ˆembeddings_specter_v1/v2, s2orc/s2orc_v2ï¼‰
  ä¸æ’å…¥å®Œæ•´æ•°æ®ï¼Œåªå»ºç«‹ corpusid â†’ filename çš„ç´¢å¼•æ˜ å°„
  
  æ€§èƒ½ä¼˜åŒ–ï¼š
    - 4ä¸ªè§£å‹è¿›ç¨‹å¹¶è¡Œï¼ˆå……åˆ†åˆ©ç”¨CPUï¼‰
    - 20ä¸‡æ¡/æ‰¹æ¬¡ï¼Œ200ä¸‡æ¡/äº‹åŠ¡ï¼ˆæé€Ÿæ’å…¥ï¼‰
    - 16MBè§£å‹ç¼“å†²åŒºï¼ˆå‡å°‘I/Oï¼‰
    - æ— é¢å¤–ç´¢å¼•ï¼ˆåªæœ‰ä¸»é”®ï¼Œæœ€å¤§åŒ–æ’å…¥é€Ÿåº¦ï¼‰
  
  ç›®æ ‡é€Ÿåº¦ï¼š30000-60000æ¡/ç§’ï¼ˆæ— ç´¢å¼•å¼€é”€ï¼Œæ¯”å®Œæ•´æ•°æ®å¿«30å€+ï¼‰

ä½¿ç”¨å‰æï¼š
  å¿…é¡»å…ˆæ‰§è¡Œ init_mapping_table.py åˆ›å»ºæ˜ å°„è¡¨

ç¤ºä¾‹ï¼š
  # å¤„ç† embeddings-specter_v1 æ–‡ä»¶å¤¹
  python scripts/test/stream_gz_to_mapping_table.py --dir "E:\\2025-09-30\\embeddings-specter_v1" --dataset embeddings_specter_v1
  
  # å¤„ç† s2orc æ–‡ä»¶å¤¹
  python scripts/test/stream_gz_to_mapping_table.py --dir "E:\\2025-09-30\\s2orc" --dataset s2orc
  
  # ä¸­æ–­åç»§ç»­
  python scripts/test/stream_gz_to_mapping_table.py --dir "E:\\2025-09-30\\s2orc" --dataset s2orc --resume
        """
    )
    
    parser.add_argument('--dir', type=str, required=True,
                       help='GZæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--dataset', type=str, required=True,
                       choices=list(SUPPORTED_TABLES),
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--resume', action='store_true',
                       help='å¯ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--reset', action='store_true',
                       help='é‡ç½®è¿›åº¦')
    parser.add_argument('--retry-failed', action='store_true',
                       help='é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡ä»¶')
    
    parser.set_defaults(resume=True)
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¤„ç†
    process_gz_folder_to_mapping(
        folder_path=args.dir,
        dataset_type=args.dataset,
        num_extractors=args.extractors,
        resume=args.resume,
        reset_progress=args.reset,
        retry_failed=args.retry_failed
    )


if __name__ == '__main__':
    main()

