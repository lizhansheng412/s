#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†è„šæœ¬ï¼ˆæ˜ å°„è¡¨æµ‹è¯•ç‰ˆï¼‰
åªå¤„ç† embeddings_specter_v1/v2 å’Œ s2orc/s2orc_v2
åªæå– corpusid å’Œ filename åˆ°æ˜ å°„è¡¨ï¼Œä¸æ’å…¥çœŸæ­£çš„æ•°æ®

âš ï¸  æµ‹è¯•è„šæœ¬ï¼Œä¸ä¼šå½±å“ç°æœ‰çš„çœŸå®æ•°æ®æ’å…¥æµç¨‹
"""

import sys
import time
import logging
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from machine_config import get_machine_config
from scripts.test.stream_gz_to_mapping_table import process_gz_folder_to_mapping, NUM_EXTRACTORS

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def batch_process_machine_mapping(
    machine_id: str,
    base_dir: str,
    num_extractors: int = NUM_EXTRACTORS,
    resume: bool = True
):
    """
    æ‰¹é‡å¤„ç†è¯¥æœºå™¨åˆ†é…çš„æ–‡ä»¶å¤¹ï¼ˆæ˜ å°„è¡¨æ¨¡å¼ï¼‰
    åªé’ˆå¯¹ machine1 å’Œ machine2
    
    Args:
        machine_id: æœºå™¨ID ('machine1', 'machine2')
        base_dir: S2ORCæ•°æ®æ ¹ç›®å½•
        num_extractors: è§£å‹è¿›ç¨‹æ•°
        resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
    """
    # åªæ”¯æŒ machine1 å’Œ machine2
    if machine_id not in ['machine1', 'machine2']:
        logger.error(f"âŒ æ˜ å°„è¡¨æ¨¡å¼åªæ”¯æŒ machine1 å’Œ machine2ï¼ˆå¤§æ•°æ®é›†ï¼‰")
        logger.error(f"   å½“å‰æœºå™¨: {machine_id}")
        sys.exit(1)
    
    # è·å–æœºå™¨é…ç½®
    config = get_machine_config(machine_id)
    folders = config['folders']
    tables = config['tables']
    
    logger.info("="*80)
    logger.info(f"ğŸš€ æ‰¹é‡å¤„ç†å¯åŠ¨ï¼ˆæ˜ å°„è¡¨æµ‹è¯•æ¨¡å¼ - æé€Ÿç‰ˆï¼‰")
    logger.info("="*80)
    logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šåªæå– corpusid + filenameï¼Œä¸æ’å…¥çœŸå®æ•°æ®")
    logger.info(f"æœºå™¨ID: {machine_id}")
    logger.info(f"é…ç½®: {config['description']}")
    logger.info(f"æ•°æ®æ ¹ç›®å½•: {base_dir}")
    logger.info(f"å¾…å¤„ç†æ–‡ä»¶å¤¹: {len(folders)}")
    for folder, table in zip(folders, tables):
        logger.info(f"  - {folder} â†’ corpus_filename_mapping (dataset={table})")
    logger.info(f"")
    logger.info(f"âš¡ æ€§èƒ½é…ç½®ï¼š")
    logger.info(f"  - è§£å‹è¿›ç¨‹æ•°: {num_extractors}")
    logger.info(f"  - æ‰¹é‡å¤§å°: 200000 æ¡/æ‰¹æ¬¡")
    logger.info(f"  - äº‹åŠ¡å¤§å°: 2000000 æ¡/äº‹åŠ¡")
    logger.info(f"  - ç¼“å†²åŒº: 16MB")
    logger.info(f"  - æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
    logger.info(f"  - æ— é¢å¤–ç´¢å¼•: åªæœ‰ä¸»é”®ï¼ˆæœ€å¤§åŒ–æ’å…¥é€Ÿåº¦ï¼‰")
    logger.info(f"  - ç›®æ ‡é€Ÿåº¦: 30000-60000 æ¡/ç§’")
    logger.info("="*80)
    logger.info("")
    
    base_path = Path(base_dir)
    if not base_path.exists():
        logger.error(f"âŒ æ•°æ®æ ¹ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        sys.exit(1)
    
    overall_start = time.time()
    success_count = 0
    failed_folders = []
    
    for i, (folder_name, dataset_type) in enumerate(zip(folders, tables), 1):
        folder_path = base_path / folder_name
        
        logger.info("")
        logger.info("="*80)
        logger.info(f"ğŸ“ [{i}/{len(folders)}] å¤„ç†æ–‡ä»¶å¤¹: {folder_name}")
        logger.info(f"æ•°æ®é›†ç±»å‹: {dataset_type}")
        logger.info(f"ç›®æ ‡: corpus_filename_mapping è¡¨")
        logger.info("="*80)
        logger.info("")
        
        if not folder_path.exists():
            logger.warning(f"âš ï¸  æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè·³è¿‡: {folder_path}")
            failed_folders.append(f"{folder_name} (ä¸å­˜åœ¨)")
            continue
        
        try:
            # å¤„ç†è¯¥æ–‡ä»¶å¤¹ï¼ˆæ˜ å°„è¡¨æ¨¡å¼ï¼‰
            process_gz_folder_to_mapping(
                folder_path=str(folder_path),
                dataset_type=dataset_type,
                num_extractors=num_extractors,
                resume=resume,
                reset_progress=False
            )
            
            success_count += 1
            logger.info(f"âœ… [{i}/{len(folders)}] {folder_name} å¤„ç†å®Œæˆ\n")
            
        except KeyboardInterrupt:
            logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼ˆè¿›åº¦å·²ä¿å­˜ï¼‰")
            logger.info(f"å·²å®Œæˆ: {success_count}/{len(folders)}")
            sys.exit(1)
        except Exception as e:
            logger.error(f"âŒ [{i}/{len(folders)}] {folder_name} å¤„ç†å¤±è´¥: {e}")
            failed_folders.append(f"{folder_name} ({str(e)})")
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶å¤¹
            continue
    
    # æ€»ç»“
    elapsed = time.time() - overall_start
    
    logger.info("")
    logger.info("="*80)
    logger.info("ğŸ æ‰¹é‡å¤„ç†å®Œæˆï¼ˆæ˜ å°„è¡¨æµ‹è¯•æ¨¡å¼ï¼‰")
    logger.info("="*80)
    logger.info(f"æœºå™¨ID: {machine_id}")
    logger.info(f"æ€»æ–‡ä»¶å¤¹æ•°: {len(folders)}")
    logger.info(f"æˆåŠŸ: {success_count}")
    logger.info(f"å¤±è´¥: {len(failed_folders)}")
    logger.info(f"æ€»è€—æ—¶: {elapsed/3600:.2f} å°æ—¶")
    
    if failed_folders:
        logger.warning("")
        logger.warning("âš ï¸  ä»¥ä¸‹æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥:")
        for folder in failed_folders:
            logger.warning(f"  - {folder}")
    
    logger.info("="*80)
    logger.info("")
    
    if success_count == len(folders):
        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å¤¹å¤„ç†æˆåŠŸï¼")
        logger.info("ğŸ“Š corpus_filename_mapping è¡¨å·²å»ºç«‹ç´¢å¼•")
        logger.info("")
        logger.info("ğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
        logger.info("   1. æŸ¥è¯¢æ˜ å°„è¡¨ï¼šSELECT filename FROM corpus_filename_mapping WHERE corpusid = ?")
        logger.info("   2. æ ¹æ®æ–‡ä»¶åä» gz æ–‡ä»¶ä¸­è¯»å–å®Œæ•´æ•°æ®")
        logger.info("   3. å¯¹æ¯”å®Œæ•´æ•°æ®æ’å…¥æ–¹å¼ï¼Œè¯„ä¼°æ€§èƒ½æå‡")
        logger.info("")
        logger.info("âš¡ æ€§èƒ½å¯¹æ¯”ï¼ˆæ— é¢å¤–ç´¢å¼•ä¼˜åŒ–ï¼‰ï¼š")
        logger.info("   - æ˜ å°„è¡¨æ’å…¥ï¼š30000-60000 æ¡/ç§’")
        logger.info("   - å®Œæ•´æ•°æ®æ’å…¥ï¼š1000-3000 æ¡/ç§’")
        logger.info("   - é€Ÿåº¦æå‡ï¼š15-60 å€")
        logger.info("   - ç©ºé—´èŠ‚çœï¼šæ— ç´¢å¼•é¢å¤–å¼€é”€ï¼ˆèŠ‚çœ30-50%ï¼‰")
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description='æ‰¹é‡å¤„ç†è„šæœ¬ï¼ˆæ˜ å°„è¡¨æµ‹è¯•ç‰ˆï¼‰- åªæå–ç´¢å¼•ï¼Œä¸æ’å…¥çœŸå®æ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ˜ å°„è¡¨æµ‹è¯•æ¨¡å¼è¯´æ˜ï¼š
  - åªæ”¯æŒ machine1 å’Œ machine2ï¼ˆå¤§æ•°æ®é›†ï¼‰
  - åªæå– corpusid + filenameï¼Œä¸æ’å…¥å®Œæ•´æ•°æ®
  - ç›®æ ‡è¡¨ï¼šcorpus_filename_mapping
  - ä¸å½±å“ç°æœ‰çš„çœŸå®æ•°æ®æ’å…¥æµç¨‹
  
ä½¿ç”¨å‰æï¼š
  å¿…é¡»å…ˆæ‰§è¡Œ init_mapping_table.py åˆ›å»ºæ˜ å°„è¡¨

æœºå™¨é…ç½®ï¼š
  machine1: embeddings-specter_v1, s2orc
  machine2: embeddings-specter_v2, s2orc_v2

ç¤ºä¾‹ï¼š
  # Machine 1ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
  python scripts/test/batch_process_machine_mapping_test.py --machine machine1 --base-dir "E:\\2025-09-30"
  
  # Machine 2ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
  python scripts/test/batch_process_machine_mapping_test.py --machine machine2 --base-dir "E:\\2025-09-30"
  
  # è‡ªå®šä¹‰è§£å‹è¿›ç¨‹æ•°
  python scripts/test/batch_process_machine_mapping_test.py --machine machine1 --base-dir "E:\\data" --extractors 4
        """
    )
    
    parser.add_argument('--machine', type=str, required=True,
                       choices=['machine1', 'machine2'],
                       help='æœºå™¨IDï¼ˆåªæ”¯æŒ machine1 å’Œ machine2ï¼‰')
    parser.add_argument('--base-dir', type=str, required=True,
                       help='S2ORCæ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--extractors', type=int, default=NUM_EXTRACTORS,
                       help=f'è§£å‹è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: {NUM_EXTRACTORS}ï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                       help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆä»å¤´å¼€å§‹ï¼‰')
    
    args = parser.parse_args()
    
    batch_process_machine_mapping(
        machine_id=args.machine,
        base_dir=args.base_dir,
        num_extractors=args.extractors,
        resume=not args.no_resume
    )


if __name__ == '__main__':
    main()

