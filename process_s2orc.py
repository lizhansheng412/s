#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
S2ORC æ•°æ®é›†æ‰¹é‡å¢å¼ºå¤„ç†è„šæœ¬
ä» GZ æ–‡ä»¶ä¸­æå–è®ºæ–‡æ•°æ®ï¼Œè°ƒç”¨ Semantic Scholar API è·å–è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶ä¿å­˜å¢å¼ºåçš„æ•°æ®
"""

import gzip
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Tuple
import requests
from tqdm import tqdm

# å¯¼å…¥ç”¨æˆ·é…ç½®
try:
    from config import (
        INPUT_DIR as CONFIG_INPUT_DIR,
        API_KEY,
        OUTPUT_PREFIX,
        OUTPUT_EXTENSION,
        LOG_FILE,
        LOG_LEVEL
    )
except ImportError:
    # å¦‚æœæ²¡æœ‰é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼
    CONFIG_INPUT_DIR = r"E:\machine_win01\2025-09-22"
    API_KEY = None
    OUTPUT_PREFIX = "enhanced_"
    OUTPUT_EXTENSION = ".jsonl"
    LOG_FILE = "s2orc_processing.log"
    LOG_LEVEL = "INFO"


# ==================== API é…ç½®ï¼ˆå›ºå®šé…ç½®ï¼Œæ— éœ€ä¿®æ”¹ï¼‰====================

# Semantic Scholar API ç«¯ç‚¹
API_ENDPOINT = "https://api.semanticscholar.org/graph/v1/paper/batch"

# æ‰¹é‡æŸ¥è¯¢å¤§å°ï¼ˆAPI é™åˆ¶æœ€å¤§ 500ï¼‰
BATCH_SIZE = 500

# API è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
API_TIMEOUT = 60

# æ‰¹æ¬¡ä¹‹é—´çš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé¿å… API é™æµ
BATCH_DELAY = 3  # å¢åŠ åˆ° 3 ç§’ä»¥é¿å…é€Ÿç‡é™åˆ¶

# API é‡è¯•é…ç½®
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 10  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

# éœ€è¦ä» API è·å–çš„å­—æ®µåˆ—è¡¨
API_FIELDS = (
    "url,title,abstract,venue,publicationVenue,year,referenceCount,"
    "citationCount,influentialCitationCount,isOpenAccess,openAccessPdf,"
    "fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,"
    "journal,citationStyles,authors,citations,references,embedding,tldr"
)

# API å“åº”å¤§å°é™åˆ¶
MAX_RESPONSE_SIZE = 10 * 1024 * 1024  # 10 MB

# ==================== æ—¥å¿—é…ç½® ====================

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class S2ORCProcessor:
    """S2ORC æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, input_dir: str):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            input_dir: åŒ…å« GZ æ–‡ä»¶çš„ç›®å½•è·¯å¾„
        """
        self.input_dir = Path(input_dir)
        if not self.input_dir.exists():
            raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        
        logger.info(f"åˆå§‹åŒ– S2ORC å¤„ç†å™¨ï¼Œè¾“å…¥ç›®å½•: {self.input_dir}")
    
    def scan_gz_files(self) -> List[Path]:
        """
        æ‰«æç›®å½•ä¸­çš„æ‰€æœ‰ GZ æ–‡ä»¶
        
        Returns:
            GZ æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        gz_files = list(self.input_dir.glob("*.gz"))
        logger.info(f"æ‰¾åˆ° {len(gz_files)} ä¸ª GZ æ–‡ä»¶")
        return sorted(gz_files)
    
    def read_gz_file(self, gz_path: Path) -> List[Dict[str, Any]]:
        """
        è¯»å–å¹¶è§£å‹ GZ æ–‡ä»¶ï¼Œè¿”å›è®ºæ–‡æ•°æ®åˆ—è¡¨
        
        Args:
            gz_path: GZ æ–‡ä»¶è·¯å¾„
            
        Returns:
            è®ºæ–‡æ•°æ®åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼‰
        """
        decompress_start = time.time()
        papers = []
        try:
            with gzip.open(gz_path, 'rt', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        paper = json.loads(line)
                        papers.append(paper)
                    except json.JSONDecodeError as e:
                        logger.error(f"æ–‡ä»¶ {gz_path.name} ç¬¬ {line_num} è¡Œ JSON è§£æé”™è¯¯: {e}")
                        raise
            
            decompress_time = time.time() - decompress_start
            logger.info(f"æˆåŠŸè¯»å– {gz_path.name}ï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡")
            logger.info(f"è§£å‹è€—æ—¶: {decompress_time:.2f} ç§’")
            return papers
        
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶ {gz_path.name} å¤±è´¥: {e}")
            raise
    
    def extract_corpus_ids(self, papers: List[Dict[str, Any]]) -> List[str]:
        """
        ä»è®ºæ–‡æ•°æ®ä¸­æå– corpusId å¹¶æ ¼å¼åŒ–
        
        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„ corpusId åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šCorpusId:<æ•°å­—>ï¼‰
        """
        corpus_ids = []
        for i, paper in enumerate(papers):
            if 'corpusId' in paper and paper['corpusId']:
                corpus_id = paper['corpusId']
                # ç¡®ä¿æ ¼å¼ä¸º CorpusId:<æ•°å­—>
                formatted_id = f"CorpusId:{corpus_id}"
                corpus_ids.append(formatted_id)
            else:
                logger.warning(f"è®ºæ–‡ç´¢å¼• {i} ç¼ºå°‘ corpusId å­—æ®µ")
                corpus_ids.append(None)
        
        valid_count = sum(1 for cid in corpus_ids if cid is not None)
        logger.info(f"æå–åˆ° {valid_count} ä¸ªæœ‰æ•ˆçš„ corpusId")
        return corpus_ids
    
    def fetch_paper_details_batch(
        self, 
        corpus_ids: List[str],
        batch_size: int = BATCH_SIZE
    ) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è°ƒç”¨ Semantic Scholar API è·å–è®ºæ–‡è¯¦ç»†ä¿¡æ¯
        
        Args:
            corpus_ids: corpusId åˆ—è¡¨
            batch_size: æ¯æ‰¹æ¬¡çš„å¤§å°
            
        Returns:
            å­—å…¸ï¼Œkey ä¸º corpusIdï¼ˆæ•°å­—éƒ¨åˆ†ï¼‰ï¼Œvalue ä¸ºè®ºæ–‡è¯¦ç»†ä¿¡æ¯
        """
        # è¿‡æ»¤æ‰ None å€¼
        valid_ids = [cid for cid in corpus_ids if cid is not None]
        
        if not valid_ids:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ corpusId éœ€è¦æŸ¥è¯¢")
            return {}
        
        all_details = {}
        total_batches = (len(valid_ids) + batch_size - 1) // batch_size
        
        logger.info(f"å¼€å§‹æ‰¹é‡æŸ¥è¯¢ï¼Œå…± {len(valid_ids)} ä¸ª IDï¼Œåˆ† {total_batches} æ‰¹æ¬¡")
        
        with tqdm(total=len(valid_ids), desc="API æŸ¥è¯¢è¿›åº¦", unit="ç¯‡") as pbar:
            for batch_idx in range(0, len(valid_ids), batch_size):
                batch_ids = valid_ids[batch_idx:batch_idx + batch_size]
                current_batch = (batch_idx // batch_size) + 1
                
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {current_batch}/{total_batches}ï¼ŒåŒ…å« {len(batch_ids)} ä¸ª ID")
                
                try:
                    details = self._fetch_single_batch(batch_ids)
                    all_details.update(details)
                    pbar.update(len(batch_ids))
                    
                    # é¿å… API é™æµï¼Œæ·»åŠ çŸ­æš‚å»¶è¿Ÿ
                    time.sleep(BATCH_DELAY)
                    
                except Exception as e:
                    logger.error(f"æ‰¹æ¬¡ {current_batch} API è°ƒç”¨å¤±è´¥: {e}")
                    logger.error(f"å¤±è´¥çš„æ‰¹æ¬¡ ID: {batch_ids[:5]}... (æ˜¾ç¤ºå‰5ä¸ª)")
                    raise
        
        logger.info(f"API æŸ¥è¯¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(all_details)} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯")
        return all_details
    
    def _fetch_single_batch(self, corpus_ids: List[str]) -> Dict[str, Dict[str, Any]]:
        """
        è°ƒç”¨ä¸€æ¬¡ API è·å–å•æ‰¹æ¬¡è®ºæ–‡è¯¦ç»†ä¿¡æ¯ï¼ˆæ”¯æŒé‡è¯•ï¼‰
        
        Args:
            corpus_ids: corpusId åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šCorpusId:<æ•°å­—>ï¼‰
            
        Returns:
            å­—å…¸ï¼Œkey ä¸º corpusIdï¼ˆæ•°å­—éƒ¨åˆ†ï¼‰ï¼Œvalue ä¸ºè®ºæ–‡è¯¦ç»†ä¿¡æ¯
        """
        headers = {
            'Content-Type': 'application/json'
        }
        
        # å¦‚æœæœ‰ API Keyï¼Œæ·»åŠ åˆ°è¯·æ±‚å¤´
        if API_KEY:
            headers['x-api-key'] = API_KEY
        
        payload = {
            'ids': corpus_ids
        }
        
        params = {
            'fields': API_FIELDS
        }
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(MAX_RETRIES):
            try:
                response = requests.post(
                    API_ENDPOINT,
                    headers=headers,
                    json=payload,
                    params=params,
                    timeout=API_TIMEOUT
                )
            
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 429:
                    # é€Ÿç‡é™åˆ¶é”™è¯¯
                    if attempt < MAX_RETRIES - 1:
                        wait_time = RETRY_DELAY * (attempt + 1)
                        logger.warning(f"é‡åˆ°é€Ÿç‡é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{MAX_RETRIES})")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"API é€Ÿç‡é™åˆ¶ (429)ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                        logger.error(f"å»ºè®®: 1) ç”³è¯· API Key: https://www.semanticscholar.org/product/api#api-key-form")
                        logger.error(f"      2) ç­‰å¾…ä¸€æ®µæ—¶é—´åå†è¯•")
                        raise Exception(f"API é€Ÿç‡é™åˆ¶ï¼Œè¯·ç”³è¯· API Key æˆ–ç¨åé‡è¯•")
                
                elif response.status_code != 200:
                    logger.error(f"API è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                    logger.error(f"å“åº”å†…å®¹: {response.text}")
                    raise Exception(f"API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, æ¶ˆæ¯: {response.text}")
            
                # æ£€æŸ¥å“åº”å¤§å°
                response_size = len(response.content)
                if response_size > MAX_RESPONSE_SIZE:
                    logger.warning(f"å“åº”å¤§å° ({response_size} bytes) è¶…è¿‡ 10 MB é™åˆ¶")
                
                # è§£æå“åº”
                papers_data = response.json()
                
                # æ„å»ºç»“æœå­—å…¸ï¼Œä½¿ç”¨ corpusId ä½œä¸º key
                result = {}
                for paper in papers_data:
                    if paper and 'corpusId' in paper and paper['corpusId']:
                        corpus_id = str(paper['corpusId'])
                        result[corpus_id] = paper
                
                return result
            
            except requests.exceptions.RequestException as e:
                if attempt < MAX_RETRIES - 1:
                    logger.warning(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}ï¼Œ{RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(RETRY_DELAY)
                    continue
                else:
                    logger.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
                    raise
            except json.JSONDecodeError as e:
                logger.error(f"API å“åº” JSON è§£æé”™è¯¯: {e}")
                raise
    
    def merge_paper_data(
        self, 
        original_papers: List[Dict[str, Any]], 
        api_details: Dict[str, Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        åˆå¹¶åŸå§‹è®ºæ–‡æ•°æ®å’Œ API è¿”å›çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            original_papers: åŸå§‹è®ºæ–‡æ•°æ®åˆ—è¡¨
            api_details: API è¿”å›çš„è¯¦ç»†ä¿¡æ¯å­—å…¸
            
        Returns:
            åˆå¹¶åçš„è®ºæ–‡æ•°æ®åˆ—è¡¨
        """
        enhanced_papers = []
        
        for paper in tqdm(original_papers, desc="åˆå¹¶æ•°æ®", unit="ç¯‡"):
            enhanced_paper = paper.copy()
            
            # è·å–å¯¹åº”çš„ API è¯¦ç»†ä¿¡æ¯
            if 'corpusId' in paper and paper['corpusId']:
                corpus_id = str(paper['corpusId'])
                
                if corpus_id in api_details:
                    api_data = api_details[corpus_id]
                    
                    # å°† API è¿”å›çš„å­—æ®µæ·»åŠ åˆ°åŸå§‹æ•°æ®ä¸­ï¼ˆä¸è¦†ç›–å·²å­˜åœ¨çš„å­—æ®µï¼‰
                    for key, value in api_data.items():
                        if key not in enhanced_paper:
                            enhanced_paper[key] = value
                        elif enhanced_paper[key] is None and value is not None:
                            # å¦‚æœåŸå§‹å­—æ®µä¸º Noneï¼Œç”¨ API æ•°æ®å¡«å……
                            enhanced_paper[key] = value
                else:
                    logger.warning(f"corpusId {corpus_id} æœªåœ¨ API å“åº”ä¸­æ‰¾åˆ°")
            
            enhanced_papers.append(enhanced_paper)
        
        logger.info(f"æ•°æ®åˆå¹¶å®Œæˆï¼Œå…± {len(enhanced_papers)} ç¯‡è®ºæ–‡")
        return enhanced_papers
    
    def save_enhanced_jsonl(
        self, 
        papers: List[Dict[str, Any]], 
        output_path: Path
    ) -> None:
        """
        ä¿å­˜å¢å¼ºåçš„è®ºæ–‡æ•°æ®ä¸º JSONL æ ¼å¼
        
        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                for paper in papers:
                    json_line = json.dumps(paper, ensure_ascii=False)
                    f.write(json_line + '\n')
            
            logger.info(f"æˆåŠŸä¿å­˜å¢å¼ºæ•°æ®åˆ°: {output_path}")
            logger.info(f"æ–‡ä»¶å¤§å°: {output_path.stat().st_size / (1024*1024):.2f} MB")
        
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def process_single_file(self, gz_path: Path, file_index: int, total_files: int) -> bool:
        """
        å¤„ç†å•ä¸ª GZ æ–‡ä»¶
        
        Args:
            gz_path: GZ æ–‡ä»¶è·¯å¾„
            file_index: å½“å‰æ–‡ä»¶ç´¢å¼•ï¼ˆä» 1 å¼€å§‹ï¼‰
            total_files: æ€»æ–‡ä»¶æ•°
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        start_time = time.time()
        logger.info(f"\n{'='*60}")
        logger.info(f"å¤„ç†æ–‡ä»¶ {file_index}/{total_files}: {gz_path.name}")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"{'='*60}")
        
        try:
            # 1. è¯»å–å¹¶è§£å‹ GZ æ–‡ä»¶
            logger.info("æ­¥éª¤ 1/5: è§£å‹ GZ æ–‡ä»¶...")
            papers = self.read_gz_file(gz_path)
            
            if not papers:
                logger.warning(f"æ–‡ä»¶ {gz_path.name} ä¸­æ²¡æœ‰è®ºæ–‡æ•°æ®ï¼Œè·³è¿‡å¤„ç†")
                return False
        
        except Exception as e:
            logger.error(f"âš  è§£å‹æ–‡ä»¶å¤±è´¥: {e}")
            logger.warning(f"âš  è·³è¿‡æŸåçš„æ–‡ä»¶: {gz_path.name}")
            return False
        
        # 2. æå– corpusId
        logger.info("æ­¥éª¤ 2/5: æå– corpusId...")
        corpus_ids = self.extract_corpus_ids(papers)
        
        # 3. æ‰¹é‡æŸ¥è¯¢ API
        logger.info("æ­¥éª¤ 3/5: è°ƒç”¨ Semantic Scholar API...")
        api_details = self.fetch_paper_details_batch(corpus_ids)
        
        # 4. åˆå¹¶æ•°æ®
        logger.info("æ­¥éª¤ 4/5: åˆå¹¶è®ºæ–‡æ•°æ®...")
        enhanced_papers = self.merge_paper_data(papers, api_details)
        
        # 5. ä¿å­˜ç»“æœ
        logger.info("æ­¥éª¤ 5/5: ä¿å­˜å¢å¼ºæ•°æ®...")
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼šenhanced_<åŸæ–‡ä»¶å>.jsonl
        original_name = gz_path.stem  # å»é™¤ .gz åç¼€
        output_filename = f"{OUTPUT_PREFIX}{original_name}{OUTPUT_EXTENSION}"
        output_path = gz_path.parent / output_filename
        
        self.save_enhanced_jsonl(enhanced_papers, output_path)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        elapsed_time = time.time() - start_time
        logger.info(f"âœ“ æ–‡ä»¶ {gz_path.name} å¤„ç†å®Œæˆï¼")
        logger.info(f"è€—æ—¶: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é’Ÿ)")
        return True
    
    def process_all_files(self) -> None:
        """
        å¤„ç†æ‰€æœ‰ GZ æ–‡ä»¶
        """
        # æ‰«ææ‰€æœ‰ GZ æ–‡ä»¶
        gz_files = self.scan_gz_files()
        
        if not gz_files:
            logger.warning("æœªæ‰¾åˆ° GZ æ–‡ä»¶ï¼Œé€€å‡ºå¤„ç†")
            return
        
        total_files = len(gz_files)
        total_start_time = time.time()
        logger.info(f"\nå¼€å§‹å¤„ç† {total_files} ä¸ª GZ æ–‡ä»¶...")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ç»Ÿè®¡å¤„ç†ç»“æœ
        success_count = 0
        failed_count = 0
        skipped_files = []
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶
        for idx, gz_path in enumerate(gz_files, 1):
            try:
                success = self.process_single_file(gz_path, idx, total_files)
                if success:
                    success_count += 1
                else:
                    failed_count += 1
                    skipped_files.append(gz_path.name)
            except Exception as e:
                # API é”™è¯¯ç­‰ä¸¥é‡é”™è¯¯æ‰åœæ­¢
                if "API" in str(e) or "é€Ÿç‡é™åˆ¶" in str(e):
                    logger.error(f"é‡åˆ° API é”™è¯¯: {e}")
                    logger.error("åœæ­¢å¤„ç†ä»¥é¿å…è¿›ä¸€æ­¥çš„ API é™åˆ¶")
                    raise
                else:
                    # å…¶ä»–é”™è¯¯è·³è¿‡æ–‡ä»¶
                    logger.error(f"âš  å¤„ç†æ–‡ä»¶ {gz_path.name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    logger.warning(f"âš  è·³è¿‡è¯¥æ–‡ä»¶ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª")
                    failed_count += 1
                    skipped_files.append(gz_path.name)
        
        # è®¡ç®—æ€»è€—æ—¶
        total_elapsed = time.time() - total_start_time
        logger.info(f"\n{'='*60}")
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼")
        logger.info(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ€»è€—æ—¶: {total_elapsed:.2f} ç§’ ({total_elapsed/60:.2f} åˆ†é’Ÿ)")
        logger.info(f"{'='*60}")
        logger.info(f"å¤„ç†ç»Ÿè®¡:")
        logger.info(f"  âœ“ æˆåŠŸå¤„ç†: {success_count} ä¸ªæ–‡ä»¶")
        logger.info(f"  âœ— è·³è¿‡å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        logger.info(f"  ğŸ“Š æ€»æ–‡ä»¶æ•°: {total_files} ä¸ª")
        
        if success_count > 0:
            avg_time = total_elapsed / total_files
            logger.info(f"  â± å¹³å‡æ¯ä¸ªæ–‡ä»¶: {avg_time:.2f} ç§’")
        
        if skipped_files:
            logger.warning(f"\nè·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨:")
            for filename in skipped_files:
                logger.warning(f"  - {filename}")
        
        logger.info(f"{'='*60}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥ API Key é…ç½®
        if API_KEY:
            logger.info("âœ“ ä½¿ç”¨ API Key è¿›è¡Œè¯·æ±‚ï¼ˆæ›´é«˜é€Ÿç‡é™åˆ¶ï¼‰")
        else:
            logger.warning("âš  æœªé…ç½® API Keyï¼Œä½¿ç”¨å…¬å…±è®¿é—®ï¼ˆé€Ÿç‡é™åˆ¶è¾ƒä½ï¼‰")
        
        # åˆ›å»ºå¤„ç†å™¨ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç›®å½•ï¼‰
        processor = S2ORCProcessor(CONFIG_INPUT_DIR)
        
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶
        processor.process_all_files()
        
        logger.info("\nâœ“ å¤„ç†æˆåŠŸå®Œæˆï¼")
        
    except KeyboardInterrupt:
        logger.warning("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâœ— å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

